[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spatial Data Science",
    "section": "",
    "text": "Book found at: https://r-spatial.org/book/\nAnswers to most of the exercises"
  },
  {
    "objectID": "01.html",
    "href": "01.html",
    "title": "1  Intro",
    "section": "",
    "text": "List five differences between raster and vector data.\n\nraster data have values for pixels, vector data for points, lines of polygons\nspatial locations of raster pixels are constrained to a grid, vector data coordinates can have arbitrary locations (only limited by floating point representation of coordinates)\nraster data lend themselves well to represent spatially continuously observed variables (such as imagery) or spatially continuously varying variables (such as elevation or temperature); vector data lend themselves well to represent spatially discrete features such as houses and roads, or administrative regions.\nraster data cover their spatial extent completely: every point is part of a single pixel; vector data may contain holes, or have intersecting geometries where points belong to multiple polygons.\nthe operations on raster data are often simple mathematical (raster algebra) operations that include spatial operations; such simple operations are usually not available for vector data.\nraster data has trivial topology: it is clear which 4 or 8 pixels are the neighbours of every pixel; for vector data spatial there are more types of relationships, and these relationships are more complicated to detect.\n\nThe answer “Raster data is continuous data while vector data is discrete data.” is not complete: a raster of land use type represens a discrete (type) variable, a polygon map with population density represents a continuous variable. The difference lies in spatially continuous variables like elevation or temperature which are more easily represented by raster data, and spatially discrete features, such as houses and roads, which are easier represented by vector data."
  },
  {
    "objectID": "01.html#ex-1.2.",
    "href": "01.html#ex-1.2.",
    "title": "1  Intro",
    "section": "1.2 ex 1.2.",
    "text": "1.2 ex 1.2.\nIn addition to those listed below figure 1.1, list five further graphical components that are often found on a map.\n\nscale bar\ndata source\nwell defined title, subtitle\norientation indicator, north arrow\nfurther reference elements: seas, land mass, rivers"
  },
  {
    "objectID": "01.html#ex-1.3.",
    "href": "01.html#ex-1.3.",
    "title": "1  Intro",
    "section": "1.3 ex 1.3.",
    "text": "1.3 ex 1.3.\nWhy the numeric information shown in figure 1.4 misleading (or meaningless):\nThe values shown in figure 1.4 are population total associated with their respective counties. Without the county boundaries the meaning disappears: raster pixels do not contain population totals per pixel, population totals over larger regions or populations densities can no longer be derived based on this raster map alone."
  },
  {
    "objectID": "01.html#ex-1.4.",
    "href": "01.html#ex-1.4.",
    "title": "1  Intro",
    "section": "1.4 ex 1.4.",
    "text": "1.4 ex 1.4.\nUnder which conditions would you expect strong differences when doing geometrical operations on \\(S^2\\), compared to doing them on \\(R^2\\)\n\nwhen computing distances between two points at large distance from each other\nwhen determining what the shortest line is between two points, in particular near to the poles, or when the antimeridian crosses this line"
  },
  {
    "objectID": "02.html",
    "href": "02.html",
    "title": "2  Coordinates",
    "section": "",
    "text": "List three geographic measures that do not have a natural zero origin\n\nlongitude: the zero meridian is arbitrary, 100 years ago there were many other zero meridians fashionable\nlatitude: the equator may feel like a natural zero, but one could equally use the North Pole as zero, or choose entirely different origins and orientation for longitude and latitude.\naltitude (measured w.r.t. mean sea level, geoid, or ellispoid)"
  },
  {
    "objectID": "02.html#ex-2.2---2.4",
    "href": "02.html#ex-2.2---2.4",
    "title": "2  Coordinates",
    "section": "2.2 ex 2.2 - 2.4",
    "text": "2.2 ex 2.2 - 2.4\n(thanks to Jonas Hurst)\nConvert the (x, y) point s (10, 2), (-10, -1), (10, -2) and (0, 10) to polar cooridnates\n\ncart2polar = function(x, y){\n  r = sqrt(x*x + y*y)  # compute r (distance from origin)\n  phi = atan2(y, x)  # compute phi (angle between point and positive x axis in rad)\n  phi_deg = phi * 180 / pi  #  compute angle in deg\n  result = c(r, phi_deg)\n  return(result)\n}\n\ncart2polar(10, 2)\n# [1] 10.19804 11.30993\ncart2polar(-10, -1)\n# [1]   10.04988 -174.28941\ncart2polar(10, -2)\n# [1]  10.19804 -11.30993\ncart2polar(0, 10)\n# [1] 10 90"
  },
  {
    "objectID": "02.html#convert-from-polar-to-cartesian",
    "href": "02.html#convert-from-polar-to-cartesian",
    "title": "2  Coordinates",
    "section": "2.3 Convert from Polar to Cartesian",
    "text": "2.3 Convert from Polar to Cartesian\nConvert the polar (r, phi) points (10, 45°), (0, 100°) and (5, 259°) to Cartesian coordinates\n\ndeg2rad = function(angle_degree) {\n  angle_degree * pi / 180\n}\n\npolar2cart = function(r, phi_deg){\n  # phi must be in degrees\n  phi_rad = deg2rad(phi_deg)  # convert phi in degrees to radians\n  x = r * cos(phi_rad)\n  y = r * sin(phi_rad)\n  c(x, y) # return value\n}\n\npolar2cart(10, 45)\n# [1] 7.071068 7.071068\npolar2cart(0, 100)\n# [1] 0 0\npolar2cart(5, 259)\n# [1] -0.954045 -4.908136\n\nassuming the Earth is a sphere with a radius of 6371 km, compute for (lambda, phi) points the great circle distance between (10, 10) and (11, 10), between (10, 80) >and (11, 80), between (10, 10) and (10, 11) and between (10, 80) and (10, 81).\n\ndistOnSphere = function(l1, phi1, l2, phi2, radius) {\n  l1_rad = deg2rad(l1)\n  l2_rad = deg2rad(l2)\n  phi1_rad = deg2rad(phi1)\n  phi2_rad = deg2rad(phi2)\n\n  theta = acos(\n    sin(phi1_rad) * sin(phi2_rad) +\n    cos(phi1_rad) * cos(phi2_rad) * cos(abs(l1_rad - l2_rad))\n  )\n  radius * theta # return value\n}\n\nradius = 3671\ndistOnSphere(10, 10, 11, 10, radius)\n# [1] 63.09763\ndistOnSphere(10, 80, 11, 80, radius)\n# [1] 11.12568\ndistOnSphere(10, 10, 10, 11, radius)\n# [1] 64.07104\ndistOnSphere(10, 80, 10, 81, radius)\n# [1] 64.07104\n\nUnit of all results are kilometers."
  },
  {
    "objectID": "03.html",
    "href": "03.html",
    "title": "3  Geometries",
    "section": "",
    "text": "(thanks to Jannis Fröhlking)"
  },
  {
    "objectID": "03.html#ex-3.1",
    "href": "03.html#ex-3.1",
    "title": "3  Geometries",
    "section": "3.1 ex 3.1",
    "text": "3.1 ex 3.1\nGive two examples of geometries in 2-D (flat) space that are not simple feature geometries, and create a plot of them.\n\nlibrary(sf)\n# Linking to GEOS 3.10.2, GDAL 3.4.3, PROJ 8.2.1; sf_use_s2() is TRUE\nx1 <- st_linestring(rbind(c(0,0),c(2,2),c(0,2),c(2,0)))\nx2 <- st_polygon(list(rbind(c(3,0),c(5,2),c(3,2),c(5,0),c(3,0))))\nplot(c(x1,x2), col = 2:3)\n\n\n\nst_is_simple(x1)\n# [1] FALSE\nst_is_simple(x2)\n# [1] FALSE"
  },
  {
    "objectID": "03.html#ex-3.2",
    "href": "03.html#ex-3.2",
    "title": "3  Geometries",
    "section": "3.2 ex 3.2",
    "text": "3.2 ex 3.2\nRecompute the coordinates 10.542, 0.01, 45321.789 using precision values 1, 1e3, 1e6, and 1e-2.\n\nfor(i in c(1,1e3,1e6,1e-2)) \n  print(round(i * c(10.542, 0.01, 45321.789))/i)\n# [1]    11     0 45322\n# [1]    10.542     0.010 45321.789\n# [1]    10.542     0.010 45321.789\n# [1]     0     0 45300"
  },
  {
    "objectID": "03.html#ex-3.3",
    "href": "03.html#ex-3.3",
    "title": "3  Geometries",
    "section": "3.3 ex 3.3",
    "text": "3.3 ex 3.3\nDescribe a practical problem for which an n-ary intersection would be needed.\n\nfor a long-term set of polygons with fire extents, find the polygons that underwent 0, 1, 2, 3, … fires\nfor a set of extents of n individual plant species, find polygons with 0, 1, …, n species, or find the polygon(s) that contain a particular subset of plant species."
  },
  {
    "objectID": "03.html#ex-3.4",
    "href": "03.html#ex-3.4",
    "title": "3  Geometries",
    "section": "3.4 ex 3.4",
    "text": "3.4 ex 3.4\nHow can you create a Voronoi diagram (figure 3.3) that has closed polygons for every point?\nVoronoi diagrams have “open polygons”, areas that extend into infinity, for boundary points. These cannot be represented by simple feature geometries. st_voronoi chooses a default (square) polygon to limit the extent, which can be enlarged. Alternatively, the extent can be limited using st_intersection on its result:\n\nlibrary(sf)\npar(mfrow = c(2,2))\nset.seed(133331)\nmp = st_multipoint(matrix(runif(20), 10))\nplot(st_voronoi(mp), col = NA, border = 'black')\nplot(mp, add = TRUE)\ntitle(\"default extent\")\ne2 = st_polygon(list(rbind(c(-5,-5), c(5, -5), c(5,5), c(-5, 5), c(-5,-5))))\nplot(st_voronoi(mp, envelope = e2), col = NA, border = 'black')\nplot(mp, add = TRUE)\ntitle(\"enlarged envelope\")\ne3 = st_polygon(list(rbind(c(0,0), c(1, 0), c(1, 1), c(0, 1), c(0, 0))))\nv = st_voronoi(mp) %>% st_collection_extract() # pulls POLYGONs out of GC\nplot(st_intersection(v, e3), col = NA, border = 'black', axes=TRUE)\nplot(mp, add = TRUE)\ntitle(\"smaller, intersected envelope\")"
  },
  {
    "objectID": "03.html#ex-3.5",
    "href": "03.html#ex-3.5",
    "title": "3  Geometries",
    "section": "3.5 ex 3.5",
    "text": "3.5 ex 3.5\nGive the unary measure dimension for geometries POINT Z (0 1 1), LINESTRING Z (0 0 1,1 1 2), and POLYGON Z ((0 0 0,1 0 0,1 1 0,0 0 0))\n\nst_dimension(st_point(c(0,1,1)))\n# [1] 0\nst_dimension(st_linestring(rbind(c(0,1,1),c(1,1,2))))\n# [1] 1\nst_dimension(st_polygon(list(rbind(c(0,0,0),c(1,0,0),c(1,1,0),c(0,0,0)))))\n# [1] 2\n\n(these are all zero-dimensional geometries because they are points, irrespective the number of dimensions they’re defined in)"
  },
  {
    "objectID": "03.html#ex-3.6",
    "href": "03.html#ex-3.6",
    "title": "3  Geometries",
    "section": "3.6 ex 3.6",
    "text": "3.6 ex 3.6\nGive the DE-9IM relation between LINESTRING(0 0,1 0) and LINESTRING(0.5 0,0.5 1); explain the individual characters.\n\nline_1 = st_linestring(rbind(c(0,0),c(1,0)))\nline_2 = st_linestring(rbind(c(.5,0),c(.5,1)))\nplot(line_1,col = \"green\")\nplot(line_2,col = \"red\", add = TRUE)\n\n\n\nst_relate(line_1, line_2)\n#      [,1]       \n# [1,] \"F01FF0102\"\n\nThe DE-9IM relation is F01FF0102\n\nF Intersection of green lines interior and red lines interior is empty\n0 Intersection of green lines interior and red lines boundary results in one point in the middle of the green line\n1 Intersection of green lines interior and red lines exterior results in a line covering most parts of the green line\nF Intersection of green lines boundary and red lines interior is empty\nF Intersection of green lines boundary and red lines boundary is empty\n0 Intersection of green lines boundary and red lines exterior results in the two boundary points of the green line\n1 Intersection of green lines exterior and red lines interior results in a line covering most parts of the red line\n0 Intersection of green lines exterior and red lines boundary results in the upper boundary point of the red line\n2 Intersection of green lines exterior and red lines results in a polygonal geometry covering everything except the two lines\n\n(the boundary of a LINESTRING is formed by its two end points)"
  },
  {
    "objectID": "03.html#ex-3.7",
    "href": "03.html#ex-3.7",
    "title": "3  Geometries",
    "section": "3.7 ex 3.7",
    "text": "3.7 ex 3.7\nCan a set of simple feature polygons form a coverage? If so, under which constraints? Yes, but I would say that the set may just contain one polygon, because simple features provide no way of assigning points on the boundary of two adjacent polygons to a single polygon."
  },
  {
    "objectID": "03.html#ex-3.8",
    "href": "03.html#ex-3.8",
    "title": "3  Geometries",
    "section": "3.8 ex 3.8",
    "text": "3.8 ex 3.8\nFor the nc counties in the dataset that comes with R package sf, find the points touched by four counties.\n\n# read data\nnc <- st_read(system.file(\"shape/nc.shp\", package=\"sf\"))\n# Reading layer `nc' from data source \n#   `/home/edzer/R/x86_64-pc-linux-gnu-library/4.0/sf/shape/nc.shp' \n#   using driver `ESRI Shapefile'\n# Simple feature collection with 100 features and 14 fields\n# Geometry type: MULTIPOLYGON\n# Dimension:     XY\n# Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\n# Geodetic CRS:  NAD27\n# get intersections\n(nc_geom = st_geometry(nc))\n# Geometry set for 100 features \n# Geometry type: MULTIPOLYGON\n# Dimension:     XY\n# Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\n# Geodetic CRS:  NAD27\n# First 5 geometries:\n# MULTIPOLYGON (((-81.47276 36.23436, -81.54084 3...\n# MULTIPOLYGON (((-81.23989 36.36536, -81.24069 3...\n# MULTIPOLYGON (((-80.45634 36.24256, -80.47639 3...\n# MULTIPOLYGON (((-76.00897 36.3196, -76.01735 36...\n# MULTIPOLYGON (((-77.21767 36.24098, -77.23461 3...\nnc_ints = st_intersection(nc_geom)\n# although coordinates are longitude/latitude, st_intersection\n# assumes that they are planar\nplot(nc_ints, main = \"All intersections\")\n\n\n\n# Function to check class of intersection objects\nget_points = function(x){\n  if(class(x)[2]==\"POINT\")  return(x)\n}\n# get points\npoints = lapply(nc_ints, get_points)\npoints[sapply(points,is.null)] <- NULL\nsf_points = st_sfc(points)\nst_crs(sf_points) = st_crs(nc)\n# get points with four neighbouring geometries (=states)\ntouch = st_touches(sf_points, nc_geom)\nfour_n = sapply(touch, function(y) which(length(y)==4))\nnames(four_n) = seq_along(four_n)\npoint_no = array(as.numeric(names(unlist(four_n))))\nresult = st_sfc(points[point_no])\nplot(nc_geom, main = \"Points touched by four counties\")\nplot(result, add = TRUE, col = \"red\", pch = 10, cex = 2)\n\n\n\n\nA more compact way might be to search for points where counties touch another county only in a point, which can be found using st_relate using a pattern:\n\n(pts = nc %>% st_relate(pattern = \"****0****\"))\n# although coordinates are longitude/latitude, st_relate_pattern\n# assumes that they are planar\n# Sparse geometry binary predicate list of length 100, where\n# the predicate was `relate_pattern'\n# first 10 elements:\n#  1: (empty)\n#  2: (empty)\n#  3: (empty)\n#  4: (empty)\n#  5: (empty)\n#  6: (empty)\n#  7: (empty)\n#  8: (empty)\n#  9: 31\n#  10: 26\nnc %>% st_relate(pattern = \"****0****\") %>% lengths() %>% sum()\n# although coordinates are longitude/latitude, st_relate_pattern\n# assumes that they are planar\n# [1] 28\n\nwhich is, as expected, four times the number of points shown in the plot above.\nHow can we find these points? See here:\n\nnc = st_geometry(nc)\ns2 = sf_use_s2(FALSE) # use GEOM geometry\n# Spherical geometry (s2) switched off\npts = st_intersection(nc, nc)\n# although coordinates are longitude/latitude, st_intersection\n# assumes that they are planar\npts = pts[st_dimension(pts) == 0]\nplot(st_geometry(nc))\nplot(st_geometry(pts), add = TRUE, col = \"red\", pch = 10, cex = 2)\n\n\n\nsf_use_s2(s2) # set back\n# Spherical geometry (s2) switched on"
  },
  {
    "objectID": "03.html#ex-3.9",
    "href": "03.html#ex-3.9",
    "title": "3  Geometries",
    "section": "3.9 ex 3.9",
    "text": "3.9 ex 3.9\nHow would figure 3.6 look like if delta for the y-coordinate was positive? Only cells that were fully crossed by the red line would be grey:\n\nlibrary(stars)\n# Loading required package: abind\nls = st_sf(a = 2, st_sfc(st_linestring(rbind(c(0.1, 0), c(1, .9)))))\ngrd = st_as_stars(st_bbox(ls), nx = 10, ny = 10, xlim = c(0, 1.0), ylim = c(0, 1),\n   values = -1)\nattr(grd, \"dimensions\")$y$delta = .1\nattr(grd, \"dimensions\")$y$offset = 0 \nr = st_rasterize(ls, grd, options = \"ALL_TOUCHED=TRUE\")\nr[r == -1] = NA\nplot(st_geometry(st_as_sf(grd)), border = 'orange', col = NA, \n     reset = FALSE, key.pos=NULL)\nplot(r, axes = TRUE, add = TRUE, breaks = \"equal\") # ALL_TOUCHED=FALSE;\nplot(ls, add = TRUE, col = \"red\", lwd = 2)\n\n\n\n\nThe reason is that in this case, lower left corners of grid cells are part of the cell, rather than upper left corners."
  },
  {
    "objectID": "04.html",
    "href": "04.html",
    "title": "4  Spherical geometry",
    "section": "",
    "text": "Straight GeoJSON lines\nHow does the GeoJSON format define “straight” lines between ellipsoidal coordinates (section 3.1.1)? Using this definition of straight, how would LINESTRING(0 85,180 85) look like in a polar projection? How could this geometry be modified to have it cross the North Pole?\nGeoJSON defines straight lines between pairs of ellipsoidal coordinates as the straight line in Cartesian space formed by longitude and latitude. This means e.g. that all parallels are straight lines.\nUsing this definition of straight, how would LINESTRING(0 85,180 85) look like in a polar projection?\nLike a half circle:\n\nlibrary(sf)\n# Linking to GEOS 3.10.2, GDAL 3.4.3, PROJ 8.2.1; sf_use_s2() is TRUE\nl <- st_as_sfc(\"LINESTRING(0 85,180 85)\") %>%\n    st_segmentize(1) %>%\n    st_set_crs('EPSG:4326')\nplot(st_transform(l, 'EPSG:3995'), col = 'red', lwd = 2,\n     graticule = TRUE, axes = TRUE, reset = FALSE)\n\n\n\n\nHow could this geometry be modified to have it cross the North Pole?\nOne would have to let it pass through (0 90) and (180, 90):\n\nlibrary(sf)\nl <- st_as_sfc(\"LINESTRING(0 85,0 90,180 90,180 85)\") %>%\n    st_segmentize(1) %>%\n    st_set_crs('EPSG:4326')\nplot(st_transform(l, 'EPSG:3995'), col = 'red', lwd = 2,\n     graticule = TRUE, axes = TRUE, reset = FALSE)"
  },
  {
    "objectID": "04.html#ex-4.2",
    "href": "04.html#ex-4.2",
    "title": "4  Spherical geometry",
    "section": "4.2 ex 4.2",
    "text": "4.2 ex 4.2\nFor a typical polygon on \\(S^2\\), how can you find out ring direction?\nRing direction (clock-wise CW, counter clock-wise CCW) is unambiguous on \\(R^2\\) but not on \\(S^2\\): on \\(S^2\\) every polygon divides the sphere’s surface in two parts. When the inside of the polygon is taken as the area to the left when traversing the polygons’s points then for a small polygon, then ring direction is CCW if the area of the polygon is smaller than half of the area of the sphere. For polygons dividing the sphere in two equal parts (great circles such as the equator or meridians) ring direction is ambiguous."
  },
  {
    "objectID": "04.html#ex-4.3",
    "href": "04.html#ex-4.3",
    "title": "4  Spherical geometry",
    "section": "4.3 ex 4.3",
    "text": "4.3 ex 4.3\nAre there advantages of using bounding caps over using bounding boxes? If so, list them.\nBounding caps may be more compact (have a smaller area compared to the bounding box corresponding to the same geometries), they need fewer parameters, and they are invariant under rotation of (the origins of) longitude and latitude.\nFor areas covering one of the poles, a bounding box will always need to have a longitude range that spans from -180 to 180, irrespective whether the geometry is centered around the pole."
  },
  {
    "objectID": "04.html#ex-4.4",
    "href": "04.html#ex-4.4",
    "title": "4  Spherical geometry",
    "section": "4.4 ex 4.4",
    "text": "4.4 ex 4.4\nWhy is, for small areas, the orthographic projection centered at the area a good approximation of the geometry as handled on \\(S^2\\)\nBecause that is the closest approximation of the geometry on \\(R^2\\)."
  },
  {
    "objectID": "04.html#ex-4.5-fiji-in-rnaturalearth",
    "href": "04.html#ex-4.5-fiji-in-rnaturalearth",
    "title": "4  Spherical geometry",
    "section": "4.5 ex 4.5 Fiji in rnaturalearth",
    "text": "4.5 ex 4.5 Fiji in rnaturalearth\nFor rnaturalearth::ne_countries(country = \"Fiji\", returnclass=\"sf\"), check whether the geometry is valid on \\(R^2\\), on an orthographic projection centered on the country, and on \\(S^2\\). How can the geometry be made valid on S^2? Plot the resulting geometry back on \\(R^2\\). Compare the centroid of the country, as computed on \\(R^2\\) and on \\(S^2\\), and the distance between the two.\nValid on \\(R^2\\):\n\nfi = rnaturalearth::ne_countries(country = \"Fiji\", returnclass=\"sf\") %>%\n        st_geometry()\ns2 = sf_use_s2(FALSE)\n# Spherical geometry (s2) switched off\nst_is_valid(fi)\n# [1] TRUE\n\nValid on orthographic projection:\n\northo = \"+proj=ortho +lon_0=178.6 +lat_0=-17.3\"\nst_transform(fi, ortho) %>% st_is_valid()\n# [1] FALSE\nplot(st_transform(fi, ortho), border = 'red')\n\n\n\n\nThe red line following the antimeridian makes the geometry invalid in this projection, and also on \\(S^2\\):\n\nsf_use_s2(TRUE)\n# Spherical geometry (s2) switched on\nst_is_valid(fi)\n# [1] FALSE\n\nMake valid on \\(S^2\\), and plot:\n\nfi.s2 = st_make_valid(fi)\nst_is_valid(fi.s2)\n# [1] TRUE\nplot(st_transform(fi.s2, ortho), border = 'red')\ntitle(\"valid\")\n\n\n\n\nwhere we see that the line at the antimeridian has disappeared. This makes plotting in \\(R^2\\) look terrible, with lines spanning the globe:\n\nplot(fi.s2, axes = TRUE)\n\n\n\n\nCompare the centroid of the country, as computed on \\(R^2\\) and on \\(S^2\\), and the distance between the two.\n\nsf_use_s2(FALSE)\n# Spherical geometry (s2) switched off\n(c1 = st_centroid(fi))\n# Warning in st_centroid.sfc(fi): st_centroid does not give correct\n# centroids for longitude/latitude data\n# Geometry set for 1 feature \n# Geometry type: POINT\n# Dimension:     XY\n# Bounding box:  xmin: 163.8532 ymin: -17.31631 xmax: 163.8532 ymax: -17.31631\n# CRS:           +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\n# POINT (163.8532 -17.31631)\nsf_use_s2(TRUE)\n# Spherical geometry (s2) switched on\n(c2 = st_centroid(fi.s2))\n# Geometry set for 1 feature \n# Geometry type: POINT\n# Dimension:     XY\n# Bounding box:  xmin: 178.5684 ymin: -17.31562 xmax: 178.5684 ymax: -17.31562\n# CRS:           +proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0\n# POINT (178.5684 -17.31562)\nst_distance(c1, c2)\n# Units: [m]\n#         [,1]\n# [1,] 1561723\nsf_use_s2(s2)"
  },
  {
    "objectID": "04.html#ex-4.6",
    "href": "04.html#ex-4.6",
    "title": "4  Spherical geometry",
    "section": "4.6 ex 4.6",
    "text": "4.6 ex 4.6\nConsider dataset gisco_countries in R package giscoR, and select the country with NAME_ENGL == \"Fiji\". Does it have a valid geometry on the sphere? If so, how was this accomplished?\n\nlibrary(giscoR)\nlibrary(tidyverse) |> suppressPackageStartupMessages()\nlibrary(sf)\ngisco_countries |> filter(NAME_ENGL == \"Fiji\") -> fiji\nst_geometry(fiji) |> plot()\n\n\n\n\nThis is not helpful.\n\nst_bbox(fiji)\n#       xmin       ymin       xmax       ymax \n# -180.00000  -20.70876  179.99999  -12.46203\n\nThis is more helpful: we see that xmax does not run until 180, but stops just before: the island crossing the antimeridian has been cut in two, and a small gap was created between the parts."
  },
  {
    "objectID": "05.html",
    "href": "05.html",
    "title": "5  Attributes",
    "section": "",
    "text": "type of State\nThe appropriate value would be constant: there is no identity relationship of State to one of the counties in nc, and the value of State is constant through each county in the state (every point in every county in the state has this value for State)."
  },
  {
    "objectID": "05.html#ex-5.2.",
    "href": "05.html#ex-5.2.",
    "title": "5  Attributes",
    "section": "5.2 ex 5.2.",
    "text": "5.2 ex 5.2.\ntype of State for the entire state\nNow, the unioned geometry is that of the state, and we can assign identity: there is only one state of North Carolina, an this geometry is its geometry."
  },
  {
    "objectID": "05.html#ex-5.3.",
    "href": "05.html#ex-5.3.",
    "title": "5  Attributes",
    "section": "5.3 ex 5.3.",
    "text": "5.3 ex 5.3.\nthe AREA variable\n\nThe nc dataset is rather old, and did not come with an extensive report how, in detail, certain variables such as AREA were derived, so some detective work is needed here. How did people do this, more than three decades ago?\nWe can now compute area by\n\nlibrary(sf)\n# Linking to GEOS 3.10.2, GDAL 3.4.3, PROJ 8.2.1; sf_use_s2() is TRUE\nnc = read_sf(system.file(\"gpkg/nc.gpkg\", package=\"sf\"))\nnc$AREA[1:10]\n#  [1] 0.114 0.061 0.143 0.070 0.153 0.097 0.062 0.091 0.118 0.124\ns2 = sf_use_s2(FALSE) # use spherical geometry:\n# Spherical geometry (s2) switched off\nnc$area = a_sph = st_area(nc)\nnc$area[1:10]\n# Units: [m^2]\n#  [1] 1137388604  611077263 1423489919  694546292 1520740530\n#  [6]  967727952  615942210  903650119 1179347051 1232769242\nsf_use_s2(TRUE) # use ellipsoidal geometry:\n# Spherical geometry (s2) switched on\nnc$area = a_ell = st_area(nc)\nnc$area[1:10]\n# Units: [m^2]\n#  [1] 1137107793  610916077 1423145355  694378925 1520366979\n#  [6]  967504822  615794941  903423919 1179065710 1232475139\nsf_use_s2(s2) # set back to original\ncor(a_ell, a_sph)\n# [1] 0.9999999\n\nand this gives the area, in square metres, computed using either ellipsoidal or spherical geometry. We see that these are not identical, but nearly perfectly linearly correlated.\nA first hypothesis might be a constant factor between the area and AREA variables. For this, we could try a power of 10:\n\nnc$area2 = units::drop_units(nc$area / 1e10)\ncor(nc$AREA, nc$area2)\n# [1] 0.9998116\nsummary(lm(area2 ~ AREA, nc))\n# \n# Call:\n# lm(formula = area2 ~ AREA, data = nc)\n# \n# Residuals:\n#        Min         1Q     Median         3Q        Max \n# -2.281e-03 -6.279e-04  6.328e-05  5.495e-04  2.746e-03 \n# \n# Coefficients:\n#               Estimate Std. Error t value Pr(>|t|)    \n# (Intercept) -0.0009781  0.0002692  -3.633 0.000448 ***\n# AREA         1.0138124  0.0019882 509.920  < 2e-16 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 0.0009733 on 98 degrees of freedom\n# Multiple R-squared:  0.9996,  Adjusted R-squared:  0.9996 \n# F-statistic: 2.6e+05 on 1 and 98 DF,  p-value: < 2.2e-16\nplot(area2 ~ AREA, nc)\nabline(0, 1)\n\n\n\n\nand we see a pretty good, close to 1:1 correspondence! But the factor 1e10 is strange: it does not convert square metres into a usual unit for area, neither for metric nor for imperial units.\nAlso, there are deviations from the 1:1 regression line. Could these be explained by the rounding of AREA to three digits? If rounding to three digits was the only cause of spread around the regression line, we would expect a residual standard error similar to the standard deviation of a uniform distribution with width .001, which is\n\nsqrt(0.001^2/12)\n# [1] 0.0002886751\n\nbut the one obtained int he regression is three times larger. Also, the units of AREA would be 1e10 \\(m^2\\), or 1e4 \\(km^2\\), which is odd and could ring some bells: one degree latitude corresponds roughly to 111 km, so one “square degree” at the equator corresponds roughly to \\(1.11^2 \\times 10^4\\), and at 35 degrees North roughly to\n\n111 ^ 2 * cos(35 / 180 * pi)\n# [1] 10092.77\n\nwhich closely corresponds to the regression slope found above.\nWe can compute “square degree” area by using the \\(R^2\\) area routines, e.g. obtained when we set the CRS to NA:\n\nnc2 = nc\nst_crs(nc2) = NA\nnc2$area = st_area(nc2) # \"square degrees\"\nplot(area ~ AREA, nc2)\nabline(0,1)\n\n\n\ncor(nc2$area, nc2$AREA)\n# [1] 0.999983\nsummary(lm(area ~ AREA, nc2))\n# \n# Call:\n# lm(formula = area ~ AREA, data = nc2)\n# \n# Residuals:\n#        Min         1Q     Median         3Q        Max \n# -5.471e-04 -2.265e-04 -9.880e-06  2.714e-04  4.594e-04 \n# \n# Coefficients:\n#              Estimate Std. Error  t value Pr(>|t|)    \n# (Intercept) 7.436e-05  7.965e-05    0.934    0.353    \n# AREA        9.996e-01  5.882e-04 1699.395   <2e-16 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 0.0002879 on 98 degrees of freedom\n# Multiple R-squared:      1,   Adjusted R-squared:      1 \n# F-statistic: 2.888e+06 on 1 and 98 DF,  p-value: < 2.2e-16\n\nWe now get a much better fit, a near perfect correlation, and a regression standard error that corresponds exactly to what one would expect after rounding AREA to three digits.\nA further “red flag” against the constant (1e10) conversion hypothesis is the spatial pattern of the regression residuals obtained by the first approach:\n\nnc$resid = residuals(lm(area2 ~ AREA, nc))\nplot(nc[\"resid\"])\n\n\n\n\nthese residuals clearly show a North-South trend, corresponding to the effect that the Earth’s curvature has been ignored during the computation of AREA (ellipsoidal coordinates were treated as if they were Cartesian). “Square degrees” become smaller when going north.\nThe “unit” of the AREA variable is hence “square degree”. This is a meaningless unit for area on the sphere, because a unit square degree does not have a constant area."
  },
  {
    "objectID": "05.html#ex-5.4",
    "href": "05.html#ex-5.4",
    "title": "5  Attributes",
    "section": "5.4 ex 5.4",
    "text": "5.4 ex 5.4\ntype of area\n“area” is of type aggregate: it is a property of a polygon as a whole, not of each individual point in the polygon. It is extensive: if we cut a polygon in two parts, the total area is distributed over the parts."
  },
  {
    "objectID": "05.html#ex-5.5",
    "href": "05.html#ex-5.5",
    "title": "5  Attributes",
    "section": "5.5 ex 5.5",
    "text": "5.5 ex 5.5\narea-weighted interpolation\nFrom the on-line version of the book we get the code that created the plot:\n\ng = st_make_grid(st_bbox(st_as_sfc(\"LINESTRING(0 0,1 1)\")), n = c(2,2))\npar(mar = rep(0,4))\nplot(g)\nplot(g[1] * diag(c(3/4, 1)) + c(0.25, 0.125), add = TRUE, lty = 2)\ntext(c(.2, .8, .2, .8), c(.2, .2, .8, .8), c(1,2,4,8), col = 'red')\n\n\n\n\nA question is how we can make g into an sf object with the right attribute values associated with the right geometries. We try values 1:4:\nsf = st_sf(x = 1:4, geom = g)\nplot(sf)\nand see the order of the geometries: row-wise, bottom row first, so\n\nsf = st_sf(x = c(1,2,4,8), geom = g)\nplot(sf)\n\n\n\n\ngives us the source object. We create target geometries by\n\ndashed = g[1] * diag(c(3/4, 1)) + c(0.25, 0.125)\nbox = st_union(g)\nc(dashed, box)\n# Geometry set for 2 features \n# Geometry type: POLYGON\n# Dimension:     XY\n# Bounding box:  xmin: 0 ymin: 0 xmax: 1 ymax: 1\n# CRS:           NA\n# POLYGON ((0.25 0.125, 0.625 0.125, 0.625 0.625,...\n# POLYGON ((0 0.5, 0 1, 0.5 1, 1 1, 1 0.5, 1 0, 0...\n\nand can call st_interpolate_aw to compute the area-weighted interpolations:\n\nst_interpolate_aw(sf, c(dashed, box), extensive = TRUE)\n# Warning in st_interpolate_aw.sf(sf, c(dashed, box), extensive =\n# TRUE): st_interpolate_aw assumes attributes are constant or uniform\n# over areas of x\n# Simple feature collection with 2 features and 1 field\n# Attribute-geometry relationship: 0 constant, 1 aggregate, 0 identity\n# Geometry type: POLYGON\n# Dimension:     XY\n# Bounding box:  xmin: 0 ymin: 0 xmax: 1 ymax: 1\n# CRS:           NA\n#       x                       geometry\n# 1  1.75 POLYGON ((0.25 0.125, 0.625...\n# 2 15.00 POLYGON ((0 0.5, 0 1, 0.5 1...\nst_interpolate_aw(sf, c(dashed, box), extensive = FALSE)\n# Warning in st_interpolate_aw.sf(sf, c(dashed, box), extensive =\n# FALSE): st_interpolate_aw assumes attributes are constant or uniform\n# over areas of x\n# Simple feature collection with 2 features and 1 field\n# Attribute-geometry relationship: 0 constant, 1 aggregate, 0 identity\n# Geometry type: POLYGON\n# Dimension:     XY\n# Bounding box:  xmin: 0 ymin: 0 xmax: 1 ymax: 1\n# CRS:           NA\n#          x                       geometry\n# 1 2.333333 POLYGON ((0.25 0.125, 0.625...\n# 2 3.750000 POLYGON ((0 0.5, 0 1, 0.5 1...\n\nThis generates a warning, which we can get rid of by setting the agr to constant:\n\nst_agr(sf) = \"constant\"\nst_interpolate_aw(sf, c(dashed, box), FALSE)\n# Simple feature collection with 2 features and 1 field\n# Attribute-geometry relationship: 0 constant, 1 aggregate, 0 identity\n# Geometry type: POLYGON\n# Dimension:     XY\n# Bounding box:  xmin: 0 ymin: 0 xmax: 1 ymax: 1\n# CRS:           NA\n#          x                       geometry\n# 1 2.333333 POLYGON ((0.25 0.125, 0.625...\n# 2 3.750000 POLYGON ((0 0.5, 0 1, 0.5 1..."
  },
  {
    "objectID": "06.html",
    "href": "06.html",
    "title": "6  Data Cubes",
    "section": "",
    "text": "Why is it difficult to represent trajectories, sequences of \\((x,y,t)\\) obtained by tracking moving objects, by data cubes as described in this chapter?\n\nrounding \\((x,y,t)\\) to the discrete set of dimension values in a data cube may cause loss of information\nif the dimensions all have a high resolution, data loss is limited but the data cube will be very sparse; this will only be effective if a system capable of storing sparse data cubes is used (e.g. SciDB, TileDB)"
  },
  {
    "objectID": "06.html#ex-6.2.",
    "href": "06.html#ex-6.2.",
    "title": "6  Data Cubes",
    "section": "6.2 ex 6.2.",
    "text": "6.2 ex 6.2.\nIn a socio-economic vector data cube with variables population, life expectancy, and gross domestic product ordered by dimensions country and year, which variables have block support for the spatial dimension, and which have block support for the temporal dimension?\n\npopulation has spatial block support (total over an area), typically not temporal block support (but the population e.g. on a particular day of the year)\nlife expectancy is calculated over the total population of the country, and as such has spatial block support; it has temporal block support as the number of deaths over a particular period are counted, it is not clear whether this always corresponds to a single year or a longer period.\nGDP has both spatial and temporal block support: it is a total over an area and a time period."
  },
  {
    "objectID": "06.html#ex-6.3.",
    "href": "06.html#ex-6.3.",
    "title": "6  Data Cubes",
    "section": "6.3 ex 6.3.",
    "text": "6.3 ex 6.3.\nThe Sentinel-2 satellites collect images in 12 spectral bands; list advantages and disadvantages to represent them as (i) different data cubes, (ii) a data cube with 12 attributes, one for each band, and (iii) a single attribute data cube with a spectral dimension.\n\nas (i): it would be easy to cope with the differences in cell sizes;\nas (ii): one would have to cope with differences in cell sizes (10, 20, 60m), and it would not be easy to consider the spectral reflectance curve of individual pixels\nas (iii): as (ii) but it would be easier to consider (analyse, classify, reduce) spectral reflectance curves, as they are now organized in a dimension"
  },
  {
    "objectID": "06.html#ex-6.4.",
    "href": "06.html#ex-6.4.",
    "title": "6  Data Cubes",
    "section": "6.4 ex 6.4.",
    "text": "6.4 ex 6.4.\nExplain why a curvilinear raster as shown in figure 1.5 can be considered a special case of a data cube.\n\nCurvilinear grids do not have a simple relationship between dimension index (row/col, i/j) to coordinate values (lon/lat, x/y): one needs both row and col to find the coordinate pair, and from a coordinate pair a rather complex look-up to find the corresponding row and column."
  },
  {
    "objectID": "06.html#ex-6.5.",
    "href": "06.html#ex-6.5.",
    "title": "6  Data Cubes",
    "section": "6.5 ex 6.5.",
    "text": "6.5 ex 6.5.\nExplain how the following problems can be solved with data cube operations filter, apply, reduce and/or aggregate, and in which order. Also mention for each which function is applied, and what the dimensionality of the resulting data cube is (if any):"
  },
  {
    "objectID": "06.html#ex-6.5.1",
    "href": "06.html#ex-6.5.1",
    "title": "6  Data Cubes",
    "section": "6.6 ex 6.5.1",
    "text": "6.6 ex 6.5.1\nfrom hourly \\(PM_{10}\\) measurements for a set of air quality monitoring stations, compute per station the amount of days per year that the average daily \\(PM_{10}\\) value exceeds 50 \\(\\mu g/m^3\\)\n\nconvert measured hourly values into daily averages: aggregate (from hourly to daily, function: mean)\nconvert daily averages into TRUE/FALSE whether the daily average exceeds 50: apply (function: larger-than)\ncompute the number of days: reduce time (function: sum)\n\nThis gives a one-dimensional data cube, with dimension “station”"
  },
  {
    "objectID": "06.html#ex-6.5.2",
    "href": "06.html#ex-6.5.2",
    "title": "6  Data Cubes",
    "section": "6.7 ex 6.5.2",
    "text": "6.7 ex 6.5.2\nfor a sequence of aerial images of an oil spill, find the time at which the oil spill had its largest extent, and the corresponding extent\n\nfor each image, classify pixels into oil/no oil: apply (function: classify)\nfor each image, compute size (extent) of oil spill: reduce space (function: sum)\nfor the extent time series, find time of maximum: reduce time (function: which.max, then look up time)\n\nThis gives a zero-dimensional data cube (a scalar)."
  },
  {
    "objectID": "06.html#ex-6.5.3",
    "href": "06.html#ex-6.5.3",
    "title": "6  Data Cubes",
    "section": "6.8 ex 6.5.3",
    "text": "6.8 ex 6.5.3\nfrom a 10-year period with global daily sea surface temperature (SST) raster maps, find the area with the 10% largest and 10% smallest temporal trends in SST values.\n\nfrom daily SST to trend values per pixel: reduce time (function: trend function, lm)\nfrom trend raster, find 10- and 90-percentile: reduce space (function: quantile)\nusing percentiles, threshold the trend raster: apply (function: less than / more than)\n\nThis gives a two-dimensional data cube (or raster layer: the reclassified trend raster)."
  },
  {
    "objectID": "07.html",
    "href": "07.html",
    "title": "7  sf, stars",
    "section": "",
    "text": "Find the names of the nc counties that intersect LINESTRING(-84 35,-78 35); use [ for this, and use st_join() for this.\n\nlibrary(sf)\n# Linking to GEOS 3.10.2, GDAL 3.4.3, PROJ 8.2.1; sf_use_s2() is TRUE\nlibrary(stars)\n# Loading required package: abind\n(file = system.file(\"gpkg/nc.gpkg\", package=\"sf\"))\n# [1] \"/home/edzer/R/x86_64-pc-linux-gnu-library/4.0/sf/gpkg/nc.gpkg\"\nnc = st_read(file)\n# Reading layer `nc.gpkg' from data source \n#   `/home/edzer/R/x86_64-pc-linux-gnu-library/4.0/sf/gpkg/nc.gpkg' \n#   using driver `GPKG'\n# Simple feature collection with 100 features and 14 fields\n# Geometry type: MULTIPOLYGON\n# Dimension:     XY\n# Bounding box:  xmin: -84.32385 ymin: 33.88199 xmax: -75.45698 ymax: 36.58965\n# Geodetic CRS:  NAD27\nline = st_as_sfc(\"LINESTRING(-84 35,-78 35)\", crs = st_crs(nc))\nnc[line,]$NAME\n#  [1] \"Jackson\"     \"Mecklenburg\" \"Macon\"       \"Sampson\"    \n#  [5] \"Cherokee\"    \"Cumberland\"  \"Union\"       \"Anson\"      \n#  [9] \"Hoke\"        \"Duplin\"      \"Richmond\"    \"Clay\"       \n# [13] \"Scotland\"\nst_join(st_sf(line), nc)$NAME # left join: `line` should be first argument\n#  [1] \"Jackson\"     \"Mecklenburg\" \"Macon\"       \"Sampson\"    \n#  [5] \"Cherokee\"    \"Cumberland\"  \"Union\"       \"Anson\"      \n#  [9] \"Hoke\"        \"Duplin\"      \"Richmond\"    \"Clay\"       \n# [13] \"Scotland\""
  },
  {
    "objectID": "07.html#ex.-7.2",
    "href": "07.html#ex.-7.2",
    "title": "7  sf, stars",
    "section": "7.2 ex. 7.2",
    "text": "7.2 ex. 7.2\nRepeat this after setting sf_use_s2(FALSE), and compute the difference (hint: use setdiff()), and color the counties of the difference using color ‘#00880088’.\n\n# save names first:\nsf_use_s2(TRUE)\nnames_with_s2 = nc[line,]$NAME\nsf_use_s2(FALSE)\n# Spherical geometry (s2) switched off\nnc[line,]$NAME\n# although coordinates are longitude/latitude, st_intersects assumes\n# that they are planar\n#  [1] \"Macon\"      \"Sampson\"    \"Cherokee\"   \"Cumberland\"\n#  [5] \"Union\"      \"Anson\"      \"Hoke\"       \"Duplin\"    \n#  [9] \"Richmond\"   \"Clay\"       \"Scotland\"\n(diff = setdiff(names_with_s2, nc[line,]$NAME))\n# although coordinates are longitude/latitude, st_intersects assumes\n# that they are planar\n# [1] \"Jackson\"     \"Mecklenburg\"\nplot(st_geometry(nc))\nplot(st_geometry(nc)[nc$NAME %in% diff], col = \"#00880088\", add = TRUE)"
  },
  {
    "objectID": "07.html#ex.-7.3",
    "href": "07.html#ex.-7.3",
    "title": "7  sf, stars",
    "section": "7.3 ex. 7.3",
    "text": "7.3 ex. 7.3\nPlot the two different lines in a single plot; note that R will plot a straight line always straight in the projection currently used; st_segmentize can be used to add points on straight line, or on a great circle for ellipsoidal coordinates.\n\nplot(st_geometry(nc))\nplot(st_geometry(nc)[nc$NAME %in% diff], col = \"#00880088\", add = TRUE)\nplot(line, add = TRUE)\nplot(st_segmentize(line, units::set_units(10, km)), add = TRUE, col = 'red')\n\n\n\n\nTo show that the red line is curved, but only curved in plate carree, and not e.g. in an orthographic projection centered at this region, we can also plot it in an orthographic projection:\n\nl.gc = st_segmentize(line, units::set_units(10, km))\nl.pc = st_segmentize(st_set_crs(line, NA), 0.1) %>% st_set_crs(st_crs(l.gc))\no = st_crs(\"+proj=ortho +lon_0=-80 +lat_0=35\")\nplot(st_transform(st_geometry(nc), o), axes = TRUE)\nplot(st_transform(st_geometry(nc), o)[nc$NAME %in% diff],\n     col = \"#00880088\", add = TRUE)\nplot(st_transform(l.gc, o), col = 'red', add = TRUE)\nplot(st_transform(l.pc, o), col = 'black', add = TRUE)\nplot(st_transform(line, o), col = 'green', add = TRUE)\n\n\n\n\nThe fact that the unsegmented line line is straight (R plotted it as straight, it contains only the two endpoints) and that it covers the red line supports that in this plot, the great circle line (red) is plotted straight, and the “straight in plate carree” line is not."
  },
  {
    "objectID": "07.html#ex.-7.4",
    "href": "07.html#ex.-7.4",
    "title": "7  sf, stars",
    "section": "7.4 ex. 7.4",
    "text": "7.4 ex. 7.4\nNDVI, normalized differenced vegetation index, is computed as (NIR-R)/(NIR+R), with NIR the near infrared and R the red band. Read the L7_ETMs.tif file into object x, and distribute the band dimensions over attributes by split(x, \"band\"). Then, add attribute NDVI to this object by using an expression that uses the NIR (band 4) and R (band 3) attributes directly.\n\nlibrary(stars)\n(x = read_stars(system.file(\"tif/L7_ETMs.tif\", package = \"stars\")))\n# stars object with 3 dimensions and 1 attribute\n# attribute(s):\n#              Min. 1st Qu. Median     Mean 3rd Qu. Max.\n# L7_ETMs.tif     1      54     69 68.91242      86  255\n# dimension(s):\n#      from  to  offset delta            refsys point x/y\n# x       1 349  288776  28.5 SIRGAS 2000 / ... FALSE [x]\n# y       1 352 9120761 -28.5 SIRGAS 2000 / ... FALSE [y]\n# band    1   6      NA    NA                NA    NA\n(x.spl = split(x))\n# stars object with 2 dimensions and 6 attributes\n# attribute(s):\n#     Min. 1st Qu. Median     Mean 3rd Qu. Max.\n# X1    47      67     78 79.14772      89  255\n# X2    32      55     66 67.57465      79  255\n# X3    21      49     63 64.35886      77  255\n# X4     9      52     63 59.23541      75  255\n# X5     1      63     89 83.18266     112  255\n# X6     1      32     60 59.97521      88  255\n# dimension(s):\n#   from  to  offset delta            refsys point x/y\n# x    1 349  288776  28.5 SIRGAS 2000 / ... FALSE [x]\n# y    1 352 9120761 -28.5 SIRGAS 2000 / ... FALSE [y]\nx.spl$NDVI = (x.spl$X4 - x.spl$X3)/(x.spl$X4 + x.spl$X3)\nplot(x.spl[\"NDVI\"])"
  },
  {
    "objectID": "07.html#ex.-7.5",
    "href": "07.html#ex.-7.5",
    "title": "7  sf, stars",
    "section": "7.5 ex. 7.5",
    "text": "7.5 ex. 7.5\nCompute NDVI for the L7_ETMs.tif image by reducing the band dimension, using st_apply and an a function ndvi = function(x) { (x[4]-x[3])/(x[4]+x[3]) }. Plot the result, and write the result to a GeoTIFF.\n\nndvi_fn = function(x) { (x[4]-x[3])/(x[4]+x[3]) }\nndvi = st_apply(x, 1:2, ndvi_fn)\nplot(ndvi)\n\n\n\nwrite_stars(ndvi, \"ndvi.tif\")\n\nan alternative function is\n\nndvi_fn = function(x1,x2,x3,x4,x5,x6) { (x4-x3)/(x4+x3) }\nndvi2 = st_apply(x, 1:2, ndvi_fn)\nall.equal(ndvi, ndvi2)\n# [1] TRUE\n\nThis latter function can be much faster, as it is called for chunks of data rather than for individual pixels."
  },
  {
    "objectID": "07.html#ex.-7.6",
    "href": "07.html#ex.-7.6",
    "title": "7  sf, stars",
    "section": "7.6 ex. 7.6",
    "text": "7.6 ex. 7.6\nUse st_transform to transform the stars object read from L7_ETMs.tif to EPSG:4326. Print the object. Is this a regular grid? Plot the first band using arguments axes=TRUE and explain why this takes such a long time.\n\n(x_t = st_transform(x, 'EPSG:4326'))\nplot(x_t[,,,1], axes = TRUE)\n\nthe printed summary shows that this is a curvilinear grid. Plotting takes so long because for curvilinear grids, each cell is converted to a small polygon and then plotted."
  },
  {
    "objectID": "07.html#ex.-7.7",
    "href": "07.html#ex.-7.7",
    "title": "7  sf, stars",
    "section": "7.7 ex. 7.7",
    "text": "7.7 ex. 7.7\nUse st_warp to warp the L7_ETMs.tif object to EPSG:4326, and plot the resulting object with axes=TRUE. Why is the plot created much faster than after st_transform?\n\nx_w = st_warp(x, crs = 'EPSG:4326')\nplot(x_w[,,,1], reset = FALSE)\nplot(st_as_sfc(st_bbox(x_w)), col = NA, border = 'red', add = TRUE)\n\n\n\n\nPlotting is faster now because we created a new regular grid. Note that the grid border does not align perfectly with the square formed by the bounding box (using straight lines in an equidistant rectangular projection): white grid cells indicate the misalignment due to warping/transforming."
  },
  {
    "objectID": "07.html#ex.-7.8",
    "href": "07.html#ex.-7.8",
    "title": "7  sf, stars",
    "section": "7.8 ex. 7.8",
    "text": "7.8 ex. 7.8\nUsing a vector representation of the raster L7_ETMs, plot the intersection with a circular area around POINT(293716 9113692) with radius 75 m, and compute the area-weighted mean pixel values for this circle. Compare the area-weighted values with those obtained by aggregate using the vector data, and by aggregate using the raster data, using exact=FALSE (default) and exact=FALSE. Explain the differences.\n\nl7 = st_as_sf(x)\nst_agr(l7) = \"constant\"\na = st_as_sfc(\"POINT(293716 9113692)\", crs = st_crs(l7)) %>%\n    st_buffer(units::set_units(74, m))\nplot(st_intersection(l7, a))\n\n\n\n(aw = st_interpolate_aw(l7, a, mean, extensive = FALSE))\n# Simple feature collection with 1 feature and 6 fields\n# Attribute-geometry relationship: 0 constant, 6 aggregate, 0 identity\n# Geometry type: POLYGON\n# Dimension:     XY\n# Bounding box:  xmin: 293642 ymin: 9113618 xmax: 293790 ymax: 9113766\n# Projected CRS: SIRGAS 2000 / UTM zone 25S\n#   L7_ETMs.tif.V1 L7_ETMs.tif.V2 L7_ETMs.tif.V3 L7_ETMs.tif.V4\n# 1       88.54362       73.50115       80.69708       49.98497\n#   L7_ETMs.tif.V5 L7_ETMs.tif.V6                       geometry\n# 1       111.8357       97.56707 POLYGON ((293790 9113692, 2...\n(ag_vector  = aggregate(l7, a, mean))\n# Simple feature collection with 1 feature and 6 fields\n# Geometry type: POLYGON\n# Dimension:     XY\n# Bounding box:  xmin: 293642 ymin: 9113618 xmax: 293790 ymax: 9113766\n# Projected CRS: SIRGAS 2000 / UTM zone 25S\n#   L7_ETMs.tif.V1 L7_ETMs.tif.V2 L7_ETMs.tif.V3 L7_ETMs.tif.V4\n# 1        88.0303       72.84848       79.30303       50.33333\n#   L7_ETMs.tif.V5 L7_ETMs.tif.V6                       geometry\n# 1       111.9697       96.72727 POLYGON ((293790 9113692, 2...\n(ag_rasterF = st_as_sf(aggregate(x, a, mean)))\n# Simple feature collection with 1 feature and 6 fields\n# Geometry type: POLYGON\n# Dimension:     XY\n# Bounding box:  xmin: 293642 ymin: 9113618 xmax: 293790 ymax: 9113766\n# Projected CRS: SIRGAS 2000 / UTM zone 25S\n#   L7_ETMs.tif.V1 L7_ETMs.tif.V2 L7_ETMs.tif.V3 L7_ETMs.tif.V4\n# 1           88.6          73.55           81.1          49.55\n#   L7_ETMs.tif.V5 L7_ETMs.tif.V6                       geometry\n# 1          112.1           98.4 POLYGON ((293790 9113692, 2...\n(ag_rasterT = st_as_sf(aggregate(x, a, mean, exact = TRUE)))\n# Simple feature collection with 1 feature and 6 fields\n# Geometry type: POLYGON\n# Dimension:     XY\n# Bounding box:  xmin: 293642 ymin: 9113618 xmax: 293790 ymax: 9113766\n# Projected CRS: SIRGAS 2000 / UTM zone 25S\n#   L7_ETMs.tif.V1 L7_ETMs.tif.V2 L7_ETMs.tif.V3 L7_ETMs.tif.V4\n# 1       88.54362       73.50115       80.69708       49.98497\n#   L7_ETMs.tif.V5 L7_ETMs.tif.V6                       geometry\n# 1       111.8357       97.56707 POLYGON ((293790 9113692, 2...\nrbind(st_drop_geometry(aw),\n      st_drop_geometry(ag_vector), \n      st_drop_geometry(ag_rasterF), \n      st_drop_geometry(ag_rasterT))\n#    L7_ETMs.tif.V1 L7_ETMs.tif.V2 L7_ETMs.tif.V3 L7_ETMs.tif.V4\n# 1        88.54362       73.50115       80.69708       49.98497\n# 11       88.03030       72.84848       79.30303       50.33333\n# 12       88.60000       73.55000       81.10000       49.55000\n# 13       88.54362       73.50115       80.69708       49.98497\n#    L7_ETMs.tif.V5 L7_ETMs.tif.V6\n# 1        111.8357       97.56707\n# 11       111.9697       96.72727\n# 12       112.1000       98.40000\n# 13       111.8357       97.56707\n\nArea-weighted interpolation computes the area-weighted mean of the areas shown in the plot; aggregate on the vector values computes the unweighted mean over all polygonized pixels that intersect with the circle (black lines); aggregate on the raster values only averages (unweighted) the cells with pixel centers intersecting with the circle (light red):\n\nplot(st_geometry(l7)[a])\nplot(a, add = TRUE, col = NA, border = 'red')\nplot(st_as_sf(L7_ETMs[a])[1], add = TRUE, col = '#ff000066')\nplot(st_as_sf(L7_ETMs[a], as_points = TRUE)[1], add = TRUE, pch = 3, col = 1)"
  },
  {
    "objectID": "08.html",
    "href": "08.html",
    "title": "8  Plotting",
    "section": "",
    "text": "(t.b.d.)"
  },
  {
    "objectID": "09.html",
    "href": "09.html",
    "title": "9  Large datasets",
    "section": "",
    "text": "For the S2 image (above), find out in which order the bands are using st_get_dimension_values(), and try to find out (e.g. by internet search) which spectral bands / colors they correspond to.\n\nf = \"sentinel/S2A_MSIL1C_20180220T105051_N0206_R051_T32ULE_20180221T134037.zip\"\n  granule = system.file(file = f, package = \"starsdata\")\nfile.size(granule)\n# [1] 769461997\nbase_name = strsplit(basename(granule), \".zip\")[[1]]\ns2 = paste0(\"SENTINEL2_L1C:/vsizip/\", granule, \"/\", base_name, \n    \".SAFE/MTD_MSIL1C.xml:10m:EPSG_32632\")\nlibrary(stars)\n# Loading required package: abind\n# Loading required package: sf\n# Linking to GEOS 3.10.2, GDAL 3.4.3, PROJ 8.2.1; sf_use_s2() is TRUE\n(p = read_stars(s2, proxy = TRUE))\n# stars_proxy object with 1 attribute in 1 file(s):\n# $EPSG_32632\n# [1] \"[...]/MTD_MSIL1C.xml:10m:EPSG_32632\"\n# \n# dimension(s):\n#      from    to offset delta            refsys    values x/y\n# x       1 10980  3e+05    10 WGS 84 / UTM z...      NULL [x]\n# y       1 10980  6e+06   -10 WGS 84 / UTM z...      NULL [y]\n# band    1     4     NA    NA                NA B4,...,B8\nst_get_dimension_values(p, \"band\")\n# [1] \"B4\" \"B3\" \"B2\" \"B8\""
  },
  {
    "objectID": "09.html#ex-9.2",
    "href": "09.html#ex-9.2",
    "title": "9  Large datasets",
    "section": "9.2 ex 9.2",
    "text": "9.2 ex 9.2\nCompute NDVI for the S2 image, using st_apply and an an appropriate ndvi function. Plot the result to screen, and then write the result to a GeoTIFF. Explain the difference in runtime between plotting and writing.\n\nndvi_fn = function(r, g, b, nir) (nir-r)/(nir+r)\nndvi = st_apply(p, 1:2, ndvi_fn)\nplot(ndvi)\n# downsample set to 18\n\n\n\n\nAlternatively, one could use\n\nndvi_fn = function(r, g, b, nir) (nir-r)/(nir+r)\n\nbut that is much less efficient. Write to a tiff:\n\nsystem.time(write_stars(ndvi, \"ndvi.tif\"))\n# ====================================================================\n#    user  system elapsed \n# 229.269   9.189  65.025\n\nThe runtime difference is caused by the fact that plot downsamples, so computes a very small fraction of the available pixels, where write_stars computes all pixels, and then writes them."
  },
  {
    "objectID": "09.html#ex-9.3",
    "href": "09.html#ex-9.3",
    "title": "9  Large datasets",
    "section": "9.3 ex 9.3",
    "text": "9.3 ex 9.3\nPlot an RGB composite of the S2 image, using the rgb argument to plot(), and then by using st_rgb() first.\n\nplot(p, rgb = 1:3)\n# downsample set to 18\n\n\n\n# plot(st_rgb(p[,,,1:3], maxColorValue=13600)) # FIXME: fails"
  },
  {
    "objectID": "09.html#ex-9.4",
    "href": "09.html#ex-9.4",
    "title": "9  Large datasets",
    "section": "9.4 ex 9.4",
    "text": "9.4 ex 9.4\nselect five random points from the bounding box of S2, and extract the band values at these points. What is the class of the object returned? Convert the object returned to an sf object.\n\npts =  p %>% st_bbox() %>% st_as_sfc() %>% st_sample(5)\n(p5 = st_extract(p, pts))\n# stars object with 2 dimensions and 1 attribute\n# attribute(s):\n#             Min. 1st Qu. Median   Mean 3rd Qu. Max.\n# EPSG_32632  1071  2212.5   2636 2535.2    2942 3620\n# dimension(s):\n#          from to            refsys point\n# geometry    1  5 WGS 84 / UTM z...  TRUE\n# band        1  4                NA    NA\n#                                           values\n# geometry POINT (382373....,...,POINT (310818....\n# band                                   B4,...,B8\nclass(p5)\n# [1] \"stars\"\nst_as_sf(p5)\n# Simple feature collection with 5 features and 4 fields\n# Geometry type: POINT\n# Dimension:     XY\n# Bounding box:  xmin: 300852.3 ymin: 5894652 xmax: 382373.8 ymax: 5954153\n# Projected CRS: WGS 84 / UTM zone 32N\n#     B4   B3   B2   B8                 geometry\n# 1 1071 1155 1482 1679 POINT (382373.8 5931760)\n# 2 2714 2848 3489 2848 POINT (300852.3 5954153)\n# 3 2218 2287 2659 2818 POINT (352995.3 5894652)\n# 4 3457 3224 3620 3538 POINT (321390.3 5921864)\n# 5 2404 2384 2613 2196 POINT (310818.6 5926701)"
  },
  {
    "objectID": "09.html#ex-9.5",
    "href": "09.html#ex-9.5",
    "title": "9  Large datasets",
    "section": "9.5 ex 9.5",
    "text": "9.5 ex 9.5\nFor the 10 km radius circle around POINT(390000  5940000), compute the mean pixel values of the S2 image when downsampling the images with factor 30, and on the original resolution. Compute the relative difference between the results.\n\nb = st_buffer(st_sfc(st_point(c(390000, 5940000)), crs = st_crs(p)), \n    units::set_units(10, km))\nplot(p[,,,1], reset = FALSE, axes = TRUE)\n# downsample set to 8\nplot(b, col = NA, border = 'green', add = TRUE)\n\n\n\np1 = st_as_stars(p, downsample = 30)\na1 = aggregate(p1, b, mean)\n\nFor the full resolution, this takes a while:\n\nsystem.time(a2 <- aggregate(p, b, mean))\n# Warning in c.stars(structure(list(EPSG_32632 =\n# structure(c(1032.86211707414, : along argument ignored; maybe you\n# wanted to use st_redimension?\n#    user  system elapsed \n#  81.734   2.319  75.323\n\nRelative differences: we will work on the array of the stars objects:\n\n(a1[[1]] - a2[[1]])/((a1[[1]]+a2[[1]])/2)\n#             [,1]         [,2]        [,3]         [,4]\n# [1,] 0.001055567 0.0009431265 0.001103254 7.781836e-05\n\nAlternatively one could convert a1 and a2 to a data.frame, using as.data.frame, and work on the third column of the data frames."
  },
  {
    "objectID": "09.html#ex-9.6",
    "href": "09.html#ex-9.6",
    "title": "9  Large datasets",
    "section": "9.6 ex 9.6",
    "text": "9.6 ex 9.6\nUse hist to compute the histogram on the downsampled S2 image. Also do this for each of the bands. Use ggplot2 to compute a single plot with all four histograms in facets.\n\nhist(p1)\n\n\n\nhist(p1[,,,1])\n\n\n\nhist(p1[,,,2])\n\n\n\nhist(p1[,,,3])\n\n\n\nhist(p1[,,,4])\n\n\n\nlibrary(ggplot2)\nggplot(as.data.frame(p1), aes(x = EPSG_32632)) +\n        geom_histogram() + facet_wrap(~band)\n# `stat_bin()` using `bins = 30`. Pick better value with\n# `binwidth`."
  },
  {
    "objectID": "09.html#ex-9.7",
    "href": "09.html#ex-9.7",
    "title": "9  Large datasets",
    "section": "9.7 ex 9.7",
    "text": "9.7 ex 9.7\nUse st_crop to crop the S2 image to the area covered by the 10 km circle. Plot the results. Explore the effect of setting argument crop = FALSE\n\nplot(st_crop(p, b))\n# downsample set to 2\n\n\n\nplot(st_crop(p, b, crop = FALSE))\n# downsample set to 18"
  },
  {
    "objectID": "09.html#ex-9.8",
    "href": "09.html#ex-9.8",
    "title": "9  Large datasets",
    "section": "9.8 ex 9.8",
    "text": "9.8 ex 9.8\nWith the downsampled image, compute the logical layer where all four bands have pixel values higher than 1000. Use a raster algebra expression on the four bands (use split first), or use st_apply for this.\n\np_spl = split(p1)\np_spl$high = p_spl$B4 > 1000 & p_spl$B3 > 1000 & p_spl$B2 > 1000 & p_spl$B8 > 1000\nplot(p_spl[\"high\"])\n\n\n\n\nalternative, using st_apply on the band dimension\n\np2 = st_apply(p1, 1:2, function(x) all(x > 1000))\nplot(p2)"
  },
  {
    "objectID": "10.html",
    "href": "10.html",
    "title": "10  Statistical models for spatial data",
    "section": "",
    "text": "following the lm example of Section 10.2 use a random forest model to predict SID values (e.g. using package randomForest), and plot the random forest predictions against observations, along with the \\(x=y\\) line.\n\nlibrary(tidyverse)\n# ── Attaching packages ─────────────────────────── tidyverse 1.3.1 ──\n# ✔ ggplot2 3.3.6     ✔ purrr   0.3.5\n# ✔ tibble  3.1.8     ✔ dplyr   1.0.9\n# ✔ tidyr   1.2.0     ✔ stringr 1.4.0\n# ✔ readr   2.1.2     ✔ forcats 0.5.1\n# ── Conflicts ────────────────────────────── tidyverse_conflicts() ──\n# ✖ dplyr::filter() masks stats::filter()\n# ✖ dplyr::lag()    masks stats::lag()\nlibrary(sf)\n# Linking to GEOS 3.10.2, GDAL 3.4.3, PROJ 8.2.1; sf_use_s2() is TRUE\nsystem.file(\"gpkg/nc.gpkg\", package=\"sf\") |>\n    read_sf() -> nc\nnc |> mutate(SID = SID74/BIR74, NWB = NWBIR74/BIR74) -> nc1\nlibrary(randomForest) |> suppressPackageStartupMessages()\nr = randomForest(SID ~ NWB, nc1)\nnc1$rf = predict(r)\nplot(rf~SID, nc1)\nabline(0, 1)"
  },
  {
    "objectID": "10.html#ex-10.2",
    "href": "10.html#ex-10.2",
    "title": "10  Statistical models for spatial data",
    "section": "10.2 ex 10.2",
    "text": "10.2 ex 10.2\nCreate a new dataset by randomly sampling 1000 points from the nc dataset, and rerun the linear regression model of section 10.2 on this dataset. What has changed?\n\npts = st_sample(nc, 1000)\nnc2 = st_intersection(nc1, pts)\n# Warning: attribute variables are assumed to be spatially constant\n# throughout all geometries\nlm(SID ~ NWB, nc1) |> summary()\n# \n# Call:\n# lm(formula = SID ~ NWB, data = nc1)\n# \n# Residuals:\n#        Min         1Q     Median         3Q        Max \n# -0.0033253 -0.0007411 -0.0000691  0.0005479  0.0062218 \n# \n# Coefficients:\n#              Estimate Std. Error t value Pr(>|t|)    \n# (Intercept) 0.0006773  0.0002327   2.910  0.00447 ** \n# NWB         0.0043785  0.0006204   7.058 2.44e-10 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 0.001288 on 98 degrees of freedom\n# Multiple R-squared:  0.337,   Adjusted R-squared:  0.3302 \n# F-statistic: 49.82 on 1 and 98 DF,  p-value: 2.438e-10\nlm(SID ~ NWB, nc2) |> summary()\n# \n# Call:\n# lm(formula = SID ~ NWB, data = nc2)\n# \n# Residuals:\n#        Min         1Q     Median         3Q        Max \n# -0.0034624 -0.0008199 -0.0000559  0.0005133  0.0060843 \n# \n# Coefficients:\n#              Estimate Std. Error t value Pr(>|t|)    \n# (Intercept) 6.852e-04  7.939e-05   8.631   <2e-16 ***\n# NWB         4.592e-03  2.040e-04  22.511   <2e-16 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 0.001315 on 998 degrees of freedom\n# Multiple R-squared:  0.3368,  Adjusted R-squared:  0.3361 \n# F-statistic: 506.7 on 1 and 998 DF,  p-value: < 2.2e-16\n\nwe see that the standard error has decreased with a factor 3 (sqrt(10)).\nFor prediction interval widths:\n\nlm(SID ~ NWB, nc1) |>\n  predict(nc1, interval = \"prediction\") -> pr1\nlm(SID ~ NWB, nc2) |>\n  predict(nc1, interval = \"prediction\") -> pr2\nmean(pr1[,\"upr\"] - pr1[,\"lwr\"])\n# [1] 0.005161177\nmean(pr2[,\"upr\"] - pr2[,\"lwr\"])\n# [1] 0.005167861\n\nno change, as this dominated by the residual variance;\nConfidence intervals for the predicted means:\n\nlm(SID ~ NWB, nc1) |>\n  predict(nc1, interval = \"confidence\") -> pr1\nlm(SID ~ NWB, nc2) |>\n  predict(nc1, interval = \"confidence\") -> pr2\nmean(pr1[,\"upr\"] - pr1[,\"lwr\"])\n# [1] 0.0007025904\nmean(pr2[,\"upr\"] - pr2[,\"lwr\"])\n# [1] 0.0002267319\n\ndrops for larger dataset, as this is dominated by the standard errors of estimated coefficients."
  },
  {
    "objectID": "10.html#ex-10.3",
    "href": "10.html#ex-10.3",
    "title": "10  Statistical models for spatial data",
    "section": "10.3 ex 10.3",
    "text": "10.3 ex 10.3\nRedo the water-land classification of section 7.4 using class::knn instead of lda.\nPreparing the dataset:\n\ntif <- system.file(\"tif/L7_ETMs.tif\", package = \"stars\")\nlibrary(stars)\n# Loading required package: abind\n(r <- read_stars(tif))\n# stars object with 3 dimensions and 1 attribute\n# attribute(s):\n#              Min. 1st Qu. Median     Mean 3rd Qu. Max.\n# L7_ETMs.tif     1      54     69 68.91242      86  255\n# dimension(s):\n#      from  to  offset delta            refsys point x/y\n# x       1 349  288776  28.5 SIRGAS 2000 / ... FALSE [x]\n# y       1 352 9120761 -28.5 SIRGAS 2000 / ... FALSE [y]\n# band    1   6      NA    NA                NA    NA\nset.seed(115517)\npts <- st_bbox(r) |> st_as_sfc() |> st_sample(20)\n(e <- st_extract(r, pts))\n# stars object with 2 dimensions and 1 attribute\n# attribute(s):\n#              Min. 1st Qu. Median     Mean 3rd Qu. Max.\n# L7_ETMs.tif    12   41.75     63 60.95833    80.5  145\n# dimension(s):\n#          from to            refsys point\n# geometry    1 20 SIRGAS 2000 / ...  TRUE\n# band        1  6                NA    NA\n#                                           values\n# geometry POINT (293002....,...,POINT (290941....\n# band                                        NULL\nplot(r[,,,1], reset = FALSE)\ncol <- rep(\"yellow\", 20)\ncol[c(8, 14, 15, 18, 19)] = \"red\"\nst_as_sf(e) |> st_coordinates() |> text(labels = 1:20, col = col)\n\n\n\nrs <- split(r)\ntrn <- st_extract(rs, pts)\ntrn$cls <- rep(\"land\", 20)\ntrn$cls[c(8, 14, 15, 18, 19)] <- \"water\"\n\nestimation and prediction happen in one command:\n\nlibrary(class)\nas.data.frame(trn) |> select(X1, X2, X3, X4, X5, X6) -> tr \nas.data.frame(rs) |> select(X1, X2, X3, X4, X5, X6) -> test\nrs$cls = knn(tr, test, trn$cl, k = 5)\nplot(rs[\"cls\"])"
  },
  {
    "objectID": "10.html#ex-10.4",
    "href": "10.html#ex-10.4",
    "title": "10  Statistical models for spatial data",
    "section": "10.4 ex 10.4",
    "text": "10.4 ex 10.4\nFor the nc data: estimation\n\nst_centroid(nc1) |> st_coordinates() -> cc\n# Warning in st_centroid.sf(nc1): st_centroid assumes attributes are\n# constant over geometries of x\nbind_cols(nc1, cc) |> transmute(X=X, Y=Y, SID=SID, NWB=NWB) -> nc2\n(lm0 <- lm(SID ~ NWB, nc1)) |> summary()\n# \n# Call:\n# lm(formula = SID ~ NWB, data = nc1)\n# \n# Residuals:\n#        Min         1Q     Median         3Q        Max \n# -0.0033253 -0.0007411 -0.0000691  0.0005479  0.0062218 \n# \n# Coefficients:\n#              Estimate Std. Error t value Pr(>|t|)    \n# (Intercept) 0.0006773  0.0002327   2.910  0.00447 ** \n# NWB         0.0043785  0.0006204   7.058 2.44e-10 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 0.001288 on 98 degrees of freedom\n# Multiple R-squared:  0.337,   Adjusted R-squared:  0.3302 \n# F-statistic: 49.82 on 1 and 98 DF,  p-value: 2.438e-10\n(lm1 <- lm(SID ~ NWB+X+Y, nc2)) |> summary()\n# \n# Call:\n# lm(formula = SID ~ NWB + X + Y, data = nc2)\n# \n# Residuals:\n#        Min         1Q     Median         3Q        Max \n# -0.0027669 -0.0007998 -0.0001568  0.0006015  0.0053235 \n# \n# Coefficients:\n#               Estimate Std. Error t value Pr(>|t|)    \n# (Intercept) -1.248e-03  1.050e-02  -0.119  0.90564    \n# NWB          5.950e-03  7.983e-04   7.454 3.99e-11 ***\n# X           -2.266e-04  7.794e-05  -2.907  0.00453 ** \n# Y           -4.655e-04  2.167e-04  -2.148  0.03424 *  \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 0.001213 on 96 degrees of freedom\n# Multiple R-squared:  0.424,   Adjusted R-squared:  0.406 \n# F-statistic: 23.56 on 3 and 96 DF,  p-value: 1.645e-11\n(lm2 <- lm(SID ~ NWB+X+Y+I(X^2)+I(Y^2)+X*Y, nc2)) |> summary()\n# \n# Call:\n# lm(formula = SID ~ NWB + X + Y + I(X^2) + I(Y^2) + X * Y, data = nc2)\n# \n# Residuals:\n#        Min         1Q     Median         3Q        Max \n# -0.0027759 -0.0007526 -0.0001441  0.0006399  0.0053102 \n# \n# Coefficients:\n#               Estimate Std. Error t value Pr(>|t|)    \n# (Intercept)  4.703e-01  7.119e-01   0.661    0.511    \n# NWB          5.978e-03  9.463e-04   6.317 9.08e-09 ***\n# X           -2.632e-04  1.016e-02  -0.026    0.979    \n# Y           -2.719e-02  2.576e-02  -1.055    0.294    \n# I(X^2)      -1.064e-05  3.387e-05  -0.314    0.754    \n# I(Y^2)       3.240e-04  3.445e-04   0.941    0.349    \n# X:Y         -4.714e-05  1.688e-04  -0.279    0.781    \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 0.001221 on 93 degrees of freedom\n# Multiple R-squared:  0.4339,  Adjusted R-squared:  0.3974 \n# F-statistic: 11.88 on 6 and 93 DF,  p-value: 7.383e-10\n\nThe first order model seems to have significant coordinate effects, for the second order model none of the coordinate effects are significant.\nPrediction:\n\nnc1$pr0 <- lm0 |> predict(nc2)\nnc1$pr1 <- lm1 |> predict(nc2)\nnc1$pr2 <- lm2 |> predict(nc2)\nnc1[c(\"pr0\", \"pr1\", \"pr2\")] |> st_as_stars() |> merge() |> plot(breaks = \"equal\")\n\n\n\n\nLargely the same pattern is shown in the predictions, some extremes get more extreme.\nFor the knn on the remote sensing data:\n\ncbind(as.data.frame(trn), st_coordinates(trn)) |> \n  select(X, Y, X1, X2, X3, X4, X5, X6) -> tr1\nas.data.frame(rs) |> transmute(X=x, Y=y, X1, X2, X3, X4, X5, X6) -> test1\nrs$cls1 = knn(tr1, test1, trn$cl, k = 5)\ncbind(as.data.frame(trn), st_coordinates(trn)) |> \n  transmute(X, Y, X2=X^2, Y2=Y^2, XY=X*Y, X1, X2, X3, X4, X5, X6) -> tr2\nas.data.frame(rs) |> \n  transmute(X=x, Y=y, X2=X^2, Y2=Y^2, XY=X*Y, X1, X2, X3, X4, X5, X6) -> test2\nrs$cls2 = knn(tr2, test2, trn$cl, k = 5)\nrs[c(\"cls\", \"cls1\", \"cls2\")] |> merge() |> plot()\n\n\n\n\nBoth models involving coordinates show much worse results!"
  },
  {
    "objectID": "11.html",
    "href": "11.html",
    "title": "11  Spatial Point Patterns",
    "section": "",
    "text": "(t.b.d.)"
  },
  {
    "objectID": "12.html",
    "href": "12.html",
    "title": "12  Spatial Interpolation",
    "section": "",
    "text": "Create a plot like the one in figure 12.13 that has the inverse distance interpolated map of figure 12.2 added on left side.\nload the .Rmd of this chapter in rstudio, and run it all the way up to the chunk where figure 12.13 is created (note that this requires that the population density csv file is present).\n\nload(\"ch12.RData\")\n\nThe chunk preceding the one that creates the plot can be modified as follows, to add inverse distance interpolations to the kriging and residual kriging:\n\nlibrary(stars)\n# Loading required package: abind\n# Loading required package: sf\n# Linking to GEOS 3.10.2, GDAL 3.4.3, PROJ 8.2.1; sf_use_s2() is TRUE\nlibrary(gstat)\nkr = krige(NO2~sqrt(pop_dens), no2.sf, grd[\"pop_dens\"], vr.m)\n# [using universal kriging]\nk$kr1 = k$var1.pred\nk$kr2 = kr$var1.pred\ni = idw(NO2~1, no2.sf, grd[\"values\"])\n# [inverse distance weighted interpolation]\nk$kr0 = i$var1.pred\nst_redimension(k[c(\"kr0\", \"kr1\", \"kr2\")], \n    along = list(what = c(\"idw\", \"kriging\", \"residual kriging\"))) %>%\n    setNames(\"NO2\") -> km\n\nNext, the plot can be created identically, as the what dimension now contains the inverse distance “layer”:\n\nlibrary(ggplot2)\ng + geom_stars(data = km, aes(fill = NO2, x = x, y = y)) + \n    geom_sf(data = st_cast(de, \"MULTILINESTRING\")) + \n    geom_sf(data = no2.sf) + facet_wrap(~what) +\n    coord_sf(lims_method = \"geometry_bbox\")\n# Coordinate system already present. Adding new coordinate system, which will replace the existing one."
  },
  {
    "objectID": "12.html#ex-12.2",
    "href": "12.html#ex-12.2",
    "title": "12  Spatial Interpolation",
    "section": "12.2 ex 12.2",
    "text": "12.2 ex 12.2\nCreate a scatter plot of the map values of the idw and kriging map, and a scatter plot of map values of idw and residual kriging.\nFor this we can e.g. convert the object \\(k\\) to a data.frame:\n\nas.data.frame(k) |> head() # NA indicates cells outside AOI\n#          x       y var1.pred var1.var kr1 kr2 kr0\n# 1 285741.3 6096239        NA       NA  NA  NA  NA\n# 2 295741.3 6096239        NA       NA  NA  NA  NA\n# 3 305741.3 6096239        NA       NA  NA  NA  NA\n# 4 315741.3 6096239        NA       NA  NA  NA  NA\n# 5 325741.3 6096239        NA       NA  NA  NA  NA\n# 6 335741.3 6096239        NA       NA  NA  NA  NA\nk.df = as.data.frame(k)\nplot(kr0 ~ kr1, k.df, xlab = \"kriging\", ylab = \"idw\")\n\n\n\nplot(kr0 ~ kr2, k.df, xlab = \"residual kriging\", ylab = \"idw\")"
  },
  {
    "objectID": "12.html#ex-12.3",
    "href": "12.html#ex-12.3",
    "title": "12  Spatial Interpolation",
    "section": "12.3 ex 12.3",
    "text": "12.3 ex 12.3\nCarry out a block kriging by setting the block argument in krige(), and do this for block sizes of 10 km (the grid cell size), 50 km and 200 km. Compare the resulting maps of estimates for these three blocks sizes with those obtained by point kriging, and do the same thing for all associated kriging standard errors.\nFor the point and block kriging values:\n\nb0 =   krige(NO2~1, no2.sf, grd[\"values\"], v.m) # points kriging\n# [using ordinary kriging]\nb10 =  krige(NO2~1, no2.sf, grd[\"values\"], v.m, block = c(1e4, 1e4))\n# [using ordinary kriging]\nb50 =  krige(NO2~1, no2.sf, grd[\"values\"], v.m, block = c(5e4, 5e4))\n# [using ordinary kriging]\nb200 = krige(NO2~1, no2.sf, grd[\"values\"], v.m, block = c(2e5, 2e5))\n# [using ordinary kriging]\nb10$points = b0$var1.pred\nb10$b10 = b10$var1.pred\nb10$b50 = b50$var1.pred\nb10$b200 = b200$var1.pred\nst_redimension(b10[c(\"points\", \"b10\", \"b50\", \"b200\")], \n    along = list(what = c(\"points\", \"blocks: 10 km\", \n                          \"blocks: 50 km\", \"blocks: 200 km\"))) %>%\n    setNames(\"NO2\") -> b\ng + geom_stars(data = b, aes(fill = NO2, x = x, y = y)) + \n    geom_sf(data = st_cast(de, \"MULTILINESTRING\")) + \n    geom_sf(data = no2.sf) + facet_wrap(~what) +\n    coord_sf(lims_method = \"geometry_bbox\")\n# Coordinate system already present. Adding new coordinate system, which will replace the existing one.\n\n\n\n\nFor the standard errors:\n\nb10$pointse = sqrt(b0$var1.var)\nb10$b10se = sqrt(b10$var1.var)\nb10$b50se = sqrt(b50$var1.var)\nb10$b200se = sqrt(b200$var1.var)\nst_redimension(b10[c(\"pointse\", \"b10se\", \"b50se\", \"b200se\")], \n    along = list(what = c(\"points\", \"blocks: 10 km\", \n                          \"blocks: 50 km\", \"blocks: 200 km\"))) %>%\n    setNames(\"NO2_SE\") -> b\ng + geom_stars(data = b, aes(fill = NO2_SE, x = x, y = y)) + \n    geom_sf(data = st_cast(de, \"MULTILINESTRING\")) + \n    geom_sf(data = no2.sf) + facet_wrap(~what) +\n    coord_sf(lims_method = \"geometry_bbox\")\n# Coordinate system already present. Adding new coordinate system, which will replace the existing one."
  },
  {
    "objectID": "12.html#ex-12.4",
    "href": "12.html#ex-12.4",
    "title": "12  Spatial Interpolation",
    "section": "12.4 ex 12.4",
    "text": "12.4 ex 12.4\nBased on the residual kriging results obtained above, compute maps of the lower and upper boundary of a 95% confidence interval, when assuming that the kriging error is normally distributed, and show them in a plot with a single (joint) legend\n\nb10$lower = b10$points - 1.96 * b10$pointse\nb10$upper = b10$points + 1.96 * b10$pointse\nst_redimension(b10[c(\"lower\", \"upper\")], \n    along = list(what = c(\"lower boundary C.I.\", \"upper boundary C.I.\"))) %>%\n    setNames(\"NO2_95CI\") -> b\ng + geom_stars(data = b, aes(fill = NO2_95CI, x = x, y = y)) + \n    geom_sf(data = st_cast(de, \"MULTILINESTRING\")) + \n    geom_sf(data = no2.sf) + facet_wrap(~what) +\n    coord_sf(lims_method = \"geometry_bbox\")\n# Coordinate system already present. Adding new coordinate system, which will replace the existing one."
  },
  {
    "objectID": "12.html#ex-12.5",
    "href": "12.html#ex-12.5",
    "title": "12  Spatial Interpolation",
    "section": "12.5 ex 12.5",
    "text": "12.5 ex 12.5\nCompute and show the map with the probability that NO2 point values exceeds the level of 15 ppm, assuming a normal distribution.\nFor this we use pnorm(), which gives the cumulative area under the curve from minus infinity up to a given value; one minus that value gives the probability of exceeding it.\n\nb10$`p(NO2 > 15)` = 1 - pnorm(15, b10$points, b10$pointse)\ng + geom_stars(data = b10, aes(fill = `p(NO2 > 15)`, x = x, y = y)) + \n    geom_sf(data = st_cast(de, \"MULTILINESTRING\")) + \n    geom_sf(data = no2.sf, col = 'orange') +\n    coord_sf(lims_method = \"geometry_bbox\")\n# Coordinate system already present. Adding new coordinate system, which will replace the existing one."
  },
  {
    "objectID": "13.html",
    "href": "13.html",
    "title": "13  Multivariate and Spatiotemporal Geostatistics",
    "section": "",
    "text": "Which fraction of the stations is removed in section @ref(preparing) when the criterion applied that a station must be 75% complete?\n\nload(\"ch13.RData\")\nsel = apply(aq, 2, function(x) mean(is.na(x)) < 0.25)\n1 - mean(sel)\n# [1] 0.01703163\n\nmeaning, 1.7 percent of the stations were removed in this step. We can use mean becasue the logical values TRUE and FALSE map to 1 and 0, respectively, when treated as numeric."
  },
  {
    "objectID": "13.html#ex-13.2",
    "href": "13.html#ex-13.2",
    "title": "13  Multivariate and Spatiotemporal Geostatistics",
    "section": "13.2 ex 13.2",
    "text": "13.2 ex 13.2\nFrom the hourly time series in no2.st, compute daily mean concentrations using aggregate, and compute the spatiotemporal variogram of this. How does it compare to the variogram of hourly values?\n\nlibrary(stars)\n# Loading required package: abind\n# Loading required package: sf\n# Linking to GEOS 3.10.2, GDAL 3.4.3, PROJ 8.2.1; sf_use_s2() is TRUE\nno2.d = aggregate(no2.st, \"1 day\", mean, na.rm = TRUE)\nlibrary(gstat)\nv.d = variogramST(NO2~1, no2.d)"
  },
  {
    "objectID": "13.html#ex-13.3",
    "href": "13.html#ex-13.3",
    "title": "13  Multivariate and Spatiotemporal Geostatistics",
    "section": "13.3 ex 13.3",
    "text": "13.3 ex 13.3\nCarry out a spatiotemporal interpolation for daily mean values for the days corresponding to those shown in @ref(fig:plotspatiotemporalpredictions), and compare the results.\n\nprodSumModel <- vgmST(\"productSum\",\n    space = vgm(50, \"Exp\", 200, 0),\n    time = vgm(20, \"Sph\", 40, 0),\n    k = 2)\nStAni = estiStAni(v.d, c(0,20000))\n(fitProdSumModel <- fit.StVariogram(v.d, prodSumModel, fit.method = 7,\n    stAni = StAni, method = \"L-BFGS-B\",\n    control = list(parscale = c(1,10,1,1,0.1,1,10)),\n    lower = rep(0.0001, 7)))\n# space component: \n#   model    psill range\n# 1   Nug  0.00010     0\n# 2   Exp 21.36956   200\n# time component: \n#   model     psill    range\n# 1   Nug 15.447236  0.00000\n# 2   Sph  2.237385 40.00188\n# k: 1e-04\nplot(v.d, fitProdSumModel, wireframe = FALSE, all = TRUE, scales = list(arrows=FALSE), zlim = c(0,50))"
  },
  {
    "objectID": "13.html#ex-13.4",
    "href": "13.html#ex-13.4",
    "title": "13  Multivariate and Spatiotemporal Geostatistics",
    "section": "13.4 ex 13.4",
    "text": "13.4 ex 13.4\nFollowing the example in the demo scripts pointed at in section @ref(cokriging), carry out a cokriging on the daily mean station data for the four days shown in @ref(fig:plotspatiotemporalpredictions). What are the differences of this approach to spatiotemporal kriging?"
  },
  {
    "objectID": "14.html",
    "href": "14.html",
    "title": "14  Proximity and Areal Data",
    "section": "",
    "text": "If dimensionality (point/line/polygon) varies in the data set, geometries must be reduced to the lowest dimension present (usually points). If all the observations are polygonal (polygon or multipolygon), contiguities (shared boundaries) are a sparse and robust neighbour representation (spdep::poly2nb()). Polygons may also be reduced to points by taking for example centroids, but neighbours found by triangulating points may not be the same as contiguity neighbours for the polygons being represented by these centroids (spdep::tri2nb()). If the geometries are multipoint, they must also be reduced to a single point. If the geometries have point rather than areal support, for example real estate transaction data, k-nearest neighbour (spdep::knn2nb(spdep::knearneigh())), graph-based (spdep::graph2nb() applied to the output of spdep::soi.graph(), spdep::relativeneigh() or spdep::gabrielneigh()) and distance-based methods (spdep::dnearneigh()`) may be used."
  },
  {
    "objectID": "14.html#ex-14.2",
    "href": "14.html#ex-14.2",
    "title": "14  Proximity and Areal Data",
    "section": "14.2 ex 14.2",
    "text": "14.2 ex 14.2\nGraph-based functions for creating neighbour objects (spdep::tri2nb(), spdep::soi.graph(), spdep::relativeneigh() and spdep::gabrielneigh()) may not be used if the support of the observations is not that of points on the plane. All other functions may be used with both planar and spherical/elliptical geometries, but the neighbours generated may differ if a non-planar data set is treated as planar."
  },
  {
    "objectID": "14.html#ex-14.3",
    "href": "14.html#ex-14.3",
    "title": "14  Proximity and Areal Data",
    "section": "14.3 ex 14.3",
    "text": "14.3 ex 14.3\nA chessboard is an \\(8 \\times 8\\) grid:\n\nxy <- data.frame(expand.grid(1:8, 1:8), col=rep(c(rep(c(\"black\", \"white\"), 4), rep(c(\"white\", \"black\"), 4)), 4))\nlibrary(stars)\n# Loading required package: abind\n# Loading required package: sf\n# Linking to GEOS 3.10.2, GDAL 3.4.3, PROJ 8.2.1; sf_use_s2() is TRUE\nlibrary(sf)\n(xy %>% st_as_stars() %>% st_as_sf() -> grd)\n# Simple feature collection with 64 features and 1 field\n# Geometry type: POLYGON\n# Dimension:     XY\n# Bounding box:  xmin: 0.5 ymin: 0.5 xmax: 8.5 ymax: 8.5\n# CRS:           NA\n# First 10 features:\n#      col                       geometry\n# 1  white POLYGON ((0.5 8.5, 1.5 8.5,...\n# 2  black POLYGON ((1.5 8.5, 2.5 8.5,...\n# 3  white POLYGON ((2.5 8.5, 3.5 8.5,...\n# 4  black POLYGON ((3.5 8.5, 4.5 8.5,...\n# 5  white POLYGON ((4.5 8.5, 5.5 8.5,...\n# 6  black POLYGON ((5.5 8.5, 6.5 8.5,...\n# 7  white POLYGON ((6.5 8.5, 7.5 8.5,...\n# 8  black POLYGON ((7.5 8.5, 8.5 8.5,...\n# 9  black POLYGON ((0.5 7.5, 1.5 7.5,...\n# 10 white POLYGON ((1.5 7.5, 2.5 7.5,...\n\n\nlibrary(spdep)\n# Loading required package: sp\n# Loading required package: spData\n(rook <- poly2nb(grd, queen=FALSE))\n# Neighbour list object:\n# Number of regions: 64 \n# Number of nonzero links: 224 \n# Percentage nonzero weights: 5.46875 \n# Average number of links: 3.5\n\nThe rook neighbours also form a grid, where the neighbours share a grid edge:\n\nplot(st_geometry(grd), col=grd$col) \nplot(rook, xy, add=TRUE, col=\"grey\")\n\n\n\n\n\n(queen <- poly2nb(grd, queen=TRUE))\n# Neighbour list object:\n# Number of regions: 64 \n# Number of nonzero links: 420 \n# Percentage nonzero weights: 10.25391 \n# Average number of links: 6.5625\n\nThe queen neighbours add neighbours sharing only one corner point:\n\nplot(st_geometry(grd), col=grd$col) \nplot(queen, xy, add=TRUE, col=\"grey\")\n\n\n\n\nand the difference yields neighbours sharing not more than one boundary point:\n\nplot(st_geometry(grd), col=grd$col) \nplot(diffnb(queen, rook), xy, add=TRUE, col=\"grey\")"
  },
  {
    "objectID": "14.html#ex-14.4",
    "href": "14.html#ex-14.4",
    "title": "14  Proximity and Areal Data",
    "section": "14.4 ex 14.4",
    "text": "14.4 ex 14.4\nWe can access cardinalities using card(), and tabulate their frequencies for the chessboard rook case:\n\n((rook %>% card() -> rc) %>% table() -> t)\n# .\n#  2  3  4 \n#  4 24 36\n\nTaking the counts found, we can construct the weights corresponding to those neighbour counts:\n\n1/rev(as.numeric(names(t)))\n# [1] 0.2500000 0.3333333 0.5000000\n\nPlotting the row-standardized weights, we see that they up-weight the neighbours of observations with few neighbours, and down-weight the neighbours of observations with more neighbours:\n\ngrd$rc <- as.factor(1/rc)\nplot(grd[, \"rc\"], main=\"rook row-standardized weights\", key.width = lcm(2.5))\n\n\n\n\nWe can also use the cardinality frequency table to find counts of neighbours with (increasing) weights:\n\nunname(rev(t))*rev(as.numeric(names(t)))\n# [1] 144  72   8\n\nThis can be confirmed by tabulating the frequencies of weights yielded by nb2listw():\n\ntable(unlist(nb2listw(rook, style=\"W\")$weights))\n# \n#              0.25 0.333333333333333               0.5 \n#               144                72                 8\n\nRepeating for the queen case again shows how row-standardization can engender edge effects:\n\n((queen %>% card() -> rc) %>% table() -> t)\n# .\n#  3  5  8 \n#  4 24 36\n\n\n1/rev(as.numeric(names(t)))\n# [1] 0.1250000 0.2000000 0.3333333\n\n\ngrd$rc <- as.factor(1/rc)\nplot(grd[, \"rc\"], main = \"rook row-standardised weights\", key.width = lcm(2.5))\n\n\n\n\n\nunname(rev(t))*rev(as.numeric(names(t)))\n# [1] 288 120  12\n\n\ntable(unlist(nb2listw(queen, style=\"W\")$weights))\n# \n#             0.125               0.2 0.333333333333333 \n#               288               120                12\n\n\nsave(list = ls(), file = \"ch14.RData\")"
  },
  {
    "objectID": "15.html",
    "href": "15.html",
    "title": "15  Measures of spatial autocorrelation",
    "section": "",
    "text": "load(\"ch14.RData\")\nlibrary(sf)\n# Linking to GEOS 3.10.2, GDAL 3.4.3, PROJ 8.2.1; sf_use_s2() is TRUE\nlibrary(spdep)\n# Loading required package: sp\n# Loading required package: spData"
  },
  {
    "objectID": "15.html#ex-15.1",
    "href": "15.html#ex-15.1",
    "title": "15  Measures of spatial autocorrelation",
    "section": "15.1 ex 15.1",
    "text": "15.1 ex 15.1\nRe-using the objects from exercise 14.3, we have:\n\n(grd$col |> factor() -> COL) |> table()\n# \n# black white \n#    32    32\n\nIn the rook case, no black:black or white:white neighbours are found, differing greatly from the expected values, which are based on non-free sampling from the proportions of colours in the data. Highly significant spatial autocorrelation is detected:\n\n(jc_r <- joincount.multi(COL, listw=nb2listw(rook, style=\"B\")))\n#             Joincount Expected Variance z-value\n# black:black     0.000   27.556    8.325 -9.5503\n# white:white     0.000   27.556    8.325 -9.5503\n# white:black   112.000   56.889   27.205 10.5662\n# Jtot          112.000   56.889   27.205 10.5662\n\nIn the queen neighbour case, no spatial autocorrelation is found, despite a chessboard looking spatially structured:\n\njoincount.multi(COL, listw=nb2listw(queen, style=\"B\"))\n#             Joincount Expected Variance z-value\n# black:black    49.000   51.667   23.616 -0.5487\n# white:white    49.000   51.667   23.616 -0.5487\n# white:black   112.000  106.667   47.796  0.7714\n# Jtot          112.000  106.667   47.796  0.7714\n\nThis is because we have chosen to see all eight neighbour grid cells as neighbours (away from the edges of the board), so the two categories occur equally often as neighbour values, as expected."
  },
  {
    "objectID": "15.html#ex-15.2",
    "href": "15.html#ex-15.2",
    "title": "15  Measures of spatial autocorrelation",
    "section": "15.2 ex 15.2",
    "text": "15.2 ex 15.2\nFirst, create an uncorrelated variable, and confirm that it is uncorrelated:\n\nset.seed(1)\nx <- rnorm(nrow(grd))\nmoran.test(x, nb2listw(queen, style=\"W\"), randomisation=FALSE, alternative=\"two.sided\")\n# \n#   Moran I test under normality\n# \n# data:  x  \n# weights: nb2listw(queen, style = \"W\")    \n# \n# Moran I statistic standard deviate = 0.27024, p-value =\n# 0.787\n# alternative hypothesis: two.sided\n# sample estimates:\n# Moran I statistic       Expectation          Variance \n#       0.002292497      -0.015873016       0.004518556\n\nNext inject patterning into the variable by adding a linear trend:\n\nx_t <- x + (0.15 * xy$Var1)\nmoran.test(x_t, nb2listw(queen, style=\"W\"), randomisation=FALSE, alternative=\"two.sided\")\n# \n#   Moran I test under normality\n# \n# data:  x_t  \n# weights: nb2listw(queen, style = \"W\")    \n# \n# Moran I statistic standard deviate = 3.0064, p-value =\n# 0.002644\n# alternative hypothesis: two.sided\n# sample estimates:\n# Moran I statistic       Expectation          Variance \n#       0.186218071      -0.015873016       0.004518556\n\nTest again having taken the residuals from a linear model removing the injected trend:\n\nlm.morantest(lm(x_t ~ xy$Var1), nb2listw(queen, style=\"W\"), alternative=\"two.sided\")\n# \n#   Global Moran I for regression residuals\n# \n# data:  \n# model: lm(formula = x_t ~ xy$Var1)\n# weights: nb2listw(queen, style = \"W\")\n# \n# Moran I statistic standard deviate = 0.28036, p-value =\n# 0.7792\n# alternative hypothesis: two.sided\n# sample estimates:\n# Observed Moran I      Expectation         Variance \n#     -0.012449399     -0.030600358      0.004191544\n\nThis is important to understand because the spatial patterning in a variable of interest, and picked up by a global measure of spatial autocorrelation, may be driven by an omitted variable. If we cannot add that variable, a latent variable or mixed effects model may be a good choice."
  },
  {
    "objectID": "15.html#ex-15.3",
    "href": "15.html#ex-15.3",
    "title": "15  Measures of spatial autocorrelation",
    "section": "15.3 ex 15.3",
    "text": "15.3 ex 15.3\nFalse discovery rate adjustment is required when conducting repeated tests on the same data set. Usually, local measures of spatial autocorrelation are calculated for all the observations in a data set, and so constitute repeated tests. When repeated tests are conducted, the usual reading of confidence intervals and probability values must be adjusted to take the repeated use of the data into account."
  },
  {
    "objectID": "15.html#ex-15.4",
    "href": "15.html#ex-15.4",
    "title": "15  Measures of spatial autocorrelation",
    "section": "15.4 ex 15.4",
    "text": "15.4 ex 15.4\nIf we start with the standard local Moran’s \\(I_i\\) for the random values with a slight 1D trend, upgraded to analytical conditional standard deviates, but with only the standard intercept-only mean model, we have a starting point; a fair number of the values exceed 2:\n\nlocm <- localmoran(x_t, nb2listw(queen, style=\"W\"))\n\n\nplot(density(locm[, 4]))\nabline(v=c(-2, 2))\n\n\n\n\n\ngrd$locm_sd <- locm[, 4]\nplot(grd[, \"locm_sd\"]) \n\n\n\n\n\nsum(p.adjust(locm[, 5], method=\"none\") < 0.05)\n# [1] 6\n\nIf we apply false discovery rate adjustment, we have just one significant measure:\n\nsum(p.adjust(locm[, 5], method=\"fdr\") < 0.05)\n# [1] 1\n\nIn the first Saddlepoint approximation also for the random values with a slight 1D trend, the distribution of standard deviates shifts leftward, with both positive and negative values beyond abs(2):\n\nlm_null <- lm(x_t ~ 1)\nlocm_null <- summary(localmoran.sad(lm_null, nb=queen, style=\"W\"))\n\n\nplot(density(locm_null[, \"Saddlepoint\"]))\nabline(v=c(-2, 2))\n\n\n\n\n\ngrd$locm_null_sd <- locm_null[, \"Saddlepoint\"]\nplot(grd[, \"locm_null_sd\"]) \n\n\n\n\n\nsum(p.adjust(locm_null[, \"Pr. (Sad)\"], method=\"none\") < 0.05)\n# [1] 8\n\nIf we apply false discovery rate adjustment, we also have just one significant measure:\n\nsum(p.adjust(locm_null[, \"Pr. (Sad)\"], method=\"fdr\") < 0.05)\n# [1] 1\n\nOnce we analyse a model including the 1D trend, most of the distribution of standard deviate values is between -2 and 2:\n\nlm_trend <- lm(x_t ~ xy$Var1)\nlocm_tr <- summary(localmoran.sad(lm_trend, nb=queen, style=\"W\"))\n\n\nplot(density(locm_tr[, \"Saddlepoint\"]))\nabline(v=c(-2, 2))\n\n\n\n\n\ngrd$locm_tr_sd <- locm_tr[, \"Saddlepoint\"]\nplot(grd[, \"locm_tr_sd\"]) \n\n\n\n\n\nsum(p.adjust(locm_tr[, \"Pr. (Sad)\"], method=\"none\") < 0.05)\n# [1] 2\n\nIf we apply false discovery rate adjustment, we now have no significant measures, as expected:\n\nsum(p.adjust(locm_tr[, \"Pr. (Sad)\"], method=\"fdr\") < 0.05)\n# [1] 0\n\nlocalmoran.sad() or localmoran.exact() provide both richer mean models, and estimates of the standard deviates built on the underlying spatial relationships for each observation, rather than analytical or permutation assumptions for the whole data set. This is achieved at the cost of longer compute times and larger memory use, especially when the Omega= argument to localmoran.sad() or localmoran.exact.alt() is used, because this is a dense \\(n \\times n\\) matrix."
  },
  {
    "objectID": "16.html",
    "href": "16.html",
    "title": "16  Spatial Regression",
    "section": "",
    "text": "The archived HSAR package includes an upper level polygon support municipality department data set, ans a lower level property data set. Both are \"sf\" objects, in the same projected CRS, provided locally.\n\nlibrary(sf)\n# Linking to GEOS 3.10.2, GDAL 3.4.3, PROJ 8.2.1; sf_use_s2() is TRUE\n#library(HSAR)\n#data(depmunic)\n#data(properties)\nif (packageVersion(\"spData\") > \"2.2.0\") {\n  data(depmunic, package=\"spData\")\n  data(properties, package=\"spData\")\n} else {\n  unzip(\"data/PropertiesAthens.zip\", files=c(\"depmunic.RData\",\n    \"properties.RData\"), exdir=\"data\")\n  load(\"data/depmunic.RData\")\n  load(\"data/properties.RData\")\n}\ndepmunic$popdens <- depmunic$population/ (10000*depmunic$area)\ndepmunic$foreigners <- 100 * depmunic$pop_rest/ depmunic$population\ndepmunic$prgreensp <- depmunic$greensp/ (10000*depmunic$area)\n\nIn the vignette, two upper-level variables are added to the six already present, and we change the green space variable scaling to avoid numerical issues in calculating coefficient standard errors.\n\nsummary(depmunic)\n#     num_dep        airbnb          museums         population    \n#  Min.   :1.0   Min.   : 144.0   Min.   : 0.000   Min.   : 45168  \n#  1st Qu.:2.5   1st Qu.: 377.5   1st Qu.: 0.000   1st Qu.: 78753  \n#  Median :4.0   Median : 565.0   Median : 0.000   Median : 98283  \n#  Mean   :4.0   Mean   : 712.3   Mean   : 2.571   Mean   : 93702  \n#  3rd Qu.:5.5   3rd Qu.: 675.5   3rd Qu.: 0.500   3rd Qu.:112677  \n#  Max.   :7.0   Max.   :2171.0   Max.   :17.000   Max.   :129603  \n#     pop_rest        greensp            area                geometry\n#  Min.   : 2735   Min.   : 40656   Min.   :3.987   POLYGON      :7  \n#  1st Qu.: 4588   1st Qu.: 42340   1st Qu.:4.264   epsg:2100    :0  \n#  Median : 5099   Median : 93715   Median :4.836   +proj=tmer...:0  \n#  Mean   : 7109   Mean   :183796   Mean   :5.420                    \n#  3rd Qu.: 8110   3rd Qu.:294286   3rd Qu.:6.412                    \n#  Max.   :16531   Max.   :478951   Max.   :7.764                    \n#     popdens         foreigners       prgreensp     \n#  Min.   :0.8039   Min.   : 4.890   Min.   :0.7708  \n#  1st Qu.:1.2979   1st Qu.: 5.058   1st Qu.:0.9653  \n#  Median :1.8763   Median : 6.055   Median :1.2070  \n#  Mean   :1.8697   Mean   : 7.369   Mean   :3.3883  \n#  3rd Qu.:2.2807   3rd Qu.: 8.882   3rd Qu.:4.9527  \n#  Max.   :3.2508   Max.   :12.755   Max.   :9.9040\n\nThe properties data set has only four variables, but with price per square metre already added:\n\nsummary(properties)\n#       id                 size             price        \n#  Length:1000        Min.   :  21.00   Min.   :   8000  \n#  Class :character   1st Qu.:  55.00   1st Qu.:  44750  \n#  Mode  :character   Median :  75.00   Median :  80000  \n#                     Mean   :  83.17   Mean   : 123677  \n#                     3rd Qu.:  98.00   3rd Qu.: 147000  \n#                     Max.   :1250.00   Max.   :5500000  \n#      prpsqm            age          dist_metro      \n#  Min.   : 207.5   Min.   : 0.00   Min.   :   4.423  \n#  1st Qu.: 631.1   1st Qu.:11.00   1st Qu.: 338.523  \n#  Median :1098.8   Median :42.00   Median : 537.250  \n#  Mean   :1316.9   Mean   :34.16   Mean   : 610.772  \n#  3rd Qu.:1814.2   3rd Qu.:48.00   3rd Qu.: 819.929  \n#  Max.   :9166.7   Max.   :67.00   Max.   :1888.856  \n#           geometry   \n#  POINT        :1000  \n#  epsg:2100    :   0  \n#  +proj=tmer...:   0  \n#                      \n#                      \n# \n\nThe values of the variables in depmunic get copied to each of the properties falling within the boundaries of the municipality departments:\n\nproperties_in_dd <- st_join(properties, depmunic, join = st_within)"
  },
  {
    "objectID": "16.html#ex-16.2",
    "href": "16.html#ex-16.2",
    "title": "16  Spatial Regression",
    "section": "16.2 ex 16.2",
    "text": "16.2 ex 16.2\nFor polygon support, we prefer contiguous neighbours:\n\n(mun_nb <- spdep::poly2nb(depmunic, row.names=as.character(depmunic$num_dep)))\n# Neighbour list object:\n# Number of regions: 7 \n# Number of nonzero links: 20 \n# Percentage nonzero weights: 40.81633 \n# Average number of links: 2.857143\n\nGlobal spatial autocorrelation is marginally detected for the green space variable:\n\nspdep::moran.test(depmunic$prgreensp, spdep::nb2listw(mun_nb))\n# \n#   Moran I test under randomisation\n# \n# data:  depmunic$prgreensp  \n# weights: spdep::nb2listw(mun_nb)    \n# \n# Moran I statistic standard deviate = 1.825, p-value = 0.034\n# alternative hypothesis: greater\n# sample estimates:\n# Moran I statistic       Expectation          Variance \n#        0.22384238       -0.16666667        0.04578844\n\nUnlike the vignette, which uses distance neighbours up to 1300 m and creates a very dense representation, we choose k=4 k-nearest neighbours, then convert to symmetry (note that some point locations are duplicated, preventing the use of spatial indexing):\n\n(pr_nb_k4s <- spdep::knn2nb(spdep::knearneigh(properties, k=4), sym=TRUE, row.names=properties$id))\n# Warning in spdep::knearneigh(properties, k = 4): knearneigh:\n# identical points found\n# Warning in spdep::knearneigh(properties, k = 4): knearneigh: kd_tree\n# not available for identical points\n# Neighbour list object:\n# Number of regions: 1000 \n# Number of nonzero links: 5874 \n# Percentage nonzero weights: 0.5874 \n# Average number of links: 5.874\n\nCopying out has led to the introduction of very powerful positive spatial autocorrelation in this and other variables copied out:\n\nspdep::moran.test(properties_in_dd$prgreensp, spdep::nb2listw(pr_nb_k4s))\n# \n#   Moran I test under randomisation\n# \n# data:  properties_in_dd$prgreensp  \n# weights: spdep::nb2listw(pr_nb_k4s)    \n# \n# Moran I statistic standard deviate = 51.666, p-value <\n# 2.2e-16\n# alternative hypothesis: greater\n# sample estimates:\n# Moran I statistic       Expectation          Variance \n#      0.9757099996     -0.0010010010      0.0003573715"
  },
  {
    "objectID": "16.html#ex-16.3",
    "href": "16.html#ex-16.3",
    "title": "16  Spatial Regression",
    "section": "16.3 ex 16.3",
    "text": "16.3 ex 16.3\nThe vignette proposes the full property level and municipal department level set of variables straight away. Here we choose the property level ones first, and update for the copied out municipal department level ones next:\n\nf_pr <- prpsqm ~ size + age + dist_metro\nf_pr_md <- update(f_pr, . ~ . + foreigners + prgreensp + popdens + museums + airbnb)\n\nAdding in the copied out upper level variables appears to account for more of the variability of the response than leaving them out:\n\nlibrary(mgcv)\n# Loading required package: nlme\n# This is mgcv 1.8-40. For overview type 'help(\"mgcv-package\")'.\npr_base <- gam(f_pr, data=properties_in_dd)\npr_2lev <- gam(f_pr_md, data=properties_in_dd)\nanova(pr_base, pr_2lev, test=\"Chisq\")\n# Analysis of Deviance Table\n# \n# Model 1: prpsqm ~ size + age + dist_metro\n# Model 2: prpsqm ~ size + age + dist_metro + foreigners + prgreensp + popdens + \n#     museums + airbnb\n#   Resid. Df Resid. Dev Df  Deviance  Pr(>Chi)    \n# 1       996  625149227                           \n# 2       991  524571512  5 100577716 < 2.2e-16 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nsummary(pr_base)\n# \n# Family: gaussian \n# Link function: identity \n# \n# Formula:\n# prpsqm ~ size + age + dist_metro\n# \n# Parametric coefficients:\n#               Estimate Std. Error t value Pr(>|t|)    \n# (Intercept) 1702.61693   78.59752   21.66  < 2e-16 ***\n# size           5.18681    0.46510   11.15  < 2e-16 ***\n# age          -18.11938    1.33737  -13.55  < 2e-16 ***\n# dist_metro    -0.32446    0.07115   -4.56 5.75e-06 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# \n# R-sq.(adj) =  0.234   Deviance explained = 23.7%\n# GCV = 6.3018e+05  Scale est. = 6.2766e+05  n = 1000\n\n\nsummary(pr_2lev)\n# \n# Family: gaussian \n# Link function: identity \n# \n# Formula:\n# prpsqm ~ size + age + dist_metro + foreigners + prgreensp + popdens + \n#     museums + airbnb\n# \n# Parametric coefficients:\n#               Estimate Std. Error t value Pr(>|t|)    \n# (Intercept) 1648.17176  189.84951   8.681   <2e-16 ***\n# size           4.29551    0.43386   9.901   <2e-16 ***\n# age          -20.18585    1.27009 -15.893   <2e-16 ***\n# dist_metro    -0.15180    0.07159  -2.120   0.0342 *  \n# foreigners   -38.13473   22.54311  -1.692   0.0910 .  \n# prgreensp     23.90080   16.33291   1.463   0.1437    \n# popdens      -51.82590  103.65578  -0.500   0.6172    \n# museums      -19.06125   21.27904  -0.896   0.3706    \n# airbnb         0.63284    0.32887   1.924   0.0546 .  \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# \n# R-sq.(adj) =  0.354   Deviance explained =   36%\n# GCV = 5.3414e+05  Scale est. = 5.2934e+05  n = 1000\n\nAdding an upper level IID random effect to the base formula also improves the fit of the model substantially:\n\npr_base_iid <- gam(update(f_pr, . ~ . + s(num_dep, bs=\"re\")), data=properties_in_dd)\nanova(pr_base, pr_base_iid, test=\"Chisq\")\n# Analysis of Deviance Table\n# \n# Model 1: prpsqm ~ size + age + dist_metro\n# Model 2: prpsqm ~ size + age + dist_metro + s(num_dep, bs = \"re\")\n#   Resid. Df Resid. Dev      Df Deviance  Pr(>Chi)    \n# 1       996  625149227                               \n# 2       995  557913777 0.99993 67235450 < 2.2e-16 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nsummary(pr_base_iid)\n# \n# Family: gaussian \n# Link function: identity \n# \n# Formula:\n# prpsqm ~ size + age + dist_metro + s(num_dep, bs = \"re\")\n# \n# Parametric coefficients:\n#               Estimate Std. Error t value Pr(>|t|)    \n# (Intercept) 2244.07579   89.35422  25.114  < 2e-16 ***\n# size           4.63594    0.44249  10.477  < 2e-16 ***\n# age          -19.12405    1.26739 -15.089  < 2e-16 ***\n# dist_metro    -0.18383    0.06848  -2.685  0.00738 ** \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Approximate significance of smooth terms:\n#               edf Ref.df     F p-value    \n# s(num_dep) 0.9916      1 118.9  <2e-16 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# R-sq.(adj) =  0.316   Deviance explained = 31.9%\n# GCV = 5.6353e+05  Scale est. = 5.6071e+05  n = 1000\n\nThis improvement is much more moderate when both the upper level variables and IID random effect are present:\n\npr_2lev_iid <- gam(update(f_pr_md, . ~ . + s(num_dep, bs=\"re\")), data=properties_in_dd)\nanova(pr_2lev, pr_2lev_iid, test=\"Chisq\")\n# Analysis of Deviance Table\n# \n# Model 1: prpsqm ~ size + age + dist_metro + foreigners + prgreensp + popdens + \n#     museums + airbnb\n# Model 2: prpsqm ~ size + age + dist_metro + foreigners + prgreensp + popdens + \n#     museums + airbnb + s(num_dep, bs = \"re\")\n#   Resid. Df Resid. Dev      Df Deviance Pr(>Chi)  \n# 1    991.00  524571512                            \n# 2    990.04  522144057 0.95676  2427454  0.02981 *\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nsummary(pr_2lev_iid)\n# \n# Family: gaussian \n# Link function: identity \n# \n# Formula:\n# prpsqm ~ size + age + dist_metro + foreigners + prgreensp + popdens + \n#     museums + airbnb + s(num_dep, bs = \"re\")\n# \n# Parametric coefficients:\n#               Estimate Std. Error t value Pr(>|t|)    \n# (Intercept) 1520.74633  200.41291   7.588 7.48e-14 ***\n# size           4.29515    0.43303   9.919  < 2e-16 ***\n# age          -20.09458    1.26851 -15.841  < 2e-16 ***\n# dist_metro    -0.14574    0.07152  -2.038  0.04184 *  \n# foreigners   -83.04410   32.17854  -2.581  0.01000 *  \n# prgreensp    -68.29107   49.95934  -1.367  0.17196    \n# popdens      261.72640  191.05195   1.370  0.17102    \n# museums     -125.97470   58.73993  -2.145  0.03223 *  \n# airbnb         1.92092    0.73695   2.607  0.00928 ** \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Approximate significance of smooth terms:\n#               edf Ref.df     F p-value  \n# s(num_dep) 0.7921      1 3.811  0.0285 *\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# R-sq.(adj) =  0.357   Deviance explained = 36.3%\n# GCV = 5.3252e+05  Scale est. = 5.2731e+05  n = 1000"
  },
  {
    "objectID": "16.html#ex-16.4",
    "href": "16.html#ex-16.4",
    "title": "16  Spatial Regression",
    "section": "16.4 ex 16.4",
    "text": "16.4 ex 16.4\nThe \"mrf\" smooth term needs ID keys set so that the neighbour object is correctly matched to the observations. Once these are provided, the properties level model with a municipality department level MRF smooth may be fit:\n\nnames(mun_nb) <- attr(mun_nb, \"region.id\")\nproperties_in_dd$num_dep <- factor(properties_in_dd$num_dep)\npr_base_mrf <- gam(update(f_pr, . ~ . + s(num_dep, bs=\"mrf\", xt=list(nb=mun_nb))),\n    data=properties_in_dd)\nsummary(pr_base_mrf)\n# \n# Family: gaussian \n# Link function: identity \n# \n# Formula:\n# prpsqm ~ size + age + dist_metro + s(num_dep, bs = \"mrf\", xt = list(nb = mun_nb))\n# \n# Parametric coefficients:\n#               Estimate Std. Error t value Pr(>|t|)    \n# (Intercept) 1730.81146   76.44218  22.642   <2e-16 ***\n# size           4.32010    0.43280   9.982   <2e-16 ***\n# age          -20.00432    1.26761 -15.781   <2e-16 ***\n# dist_metro    -0.14718    0.07142  -2.061   0.0396 *  \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Approximate significance of smooth terms:\n#              edf Ref.df    F p-value    \n# s(num_dep) 5.852      6 31.9  <2e-16 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# R-sq.(adj) =  0.357   Deviance explained = 36.3%\n# GCV = 5.3255e+05  Scale est. = 5.2731e+05  n = 1000\n\nRepeating for the extended model with upper level variables present, we see that no more response variability is accounted for than in the lower level variables only MRF RE model, and none of the upper level variables are significant at conventional levels.\n\npr_2lev_mrf <- gam(update(f_pr_md, . ~ . + s(num_dep, bs=\"mrf\", xt=list(nb=mun_nb))),\n    data=properties_in_dd)\nsummary(pr_2lev_mrf)\n# \n# Family: gaussian \n# Link function: identity \n# \n# Formula:\n# prpsqm ~ size + age + dist_metro + foreigners + prgreensp + popdens + \n#     museums + airbnb + s(num_dep, bs = \"mrf\", xt = list(nb = mun_nb))\n# \n# Parametric coefficients:\n#               Estimate Std. Error t value Pr(>|t|)    \n# (Intercept) 1510.58909  425.16218   3.553 0.000399 ***\n# size           4.29515    0.43303   9.919  < 2e-16 ***\n# age          -20.09456    1.26851 -15.841  < 2e-16 ***\n# dist_metro    -0.14574    0.07152  -2.038 0.041844 *  \n# foreigners   -42.18041   58.30966  -0.723 0.469613    \n# prgreensp     15.34415   54.84749   0.280 0.779720    \n# popdens      -50.09688  259.59932  -0.193 0.847016    \n# museums      -53.40432   55.25567  -0.966 0.334033    \n# airbnb         1.01045    0.81453   1.241 0.215072    \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Approximate significance of smooth terms:\n#               edf Ref.df     F p-value  \n# s(num_dep) 0.7922      1 3.812  0.0285 *\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# R-sq.(adj) =  0.357   Deviance explained = 36.3%\n# GCV = 5.3252e+05  Scale est. = 5.2731e+05  n = 1000\n\nIt also seems that the model without upper level variables outperforms that with them included:\n\nanova(pr_base_mrf, pr_2lev_mrf, test=\"Chisq\")\n# Analysis of Deviance Table\n# \n# Model 1: prpsqm ~ size + age + dist_metro + s(num_dep, bs = \"mrf\", xt = list(nb = mun_nb))\n# Model 2: prpsqm ~ size + age + dist_metro + foreigners + prgreensp + popdens + \n#     museums + airbnb + s(num_dep, bs = \"mrf\", xt = list(nb = mun_nb))\n#   Resid. Df Resid. Dev        Df Deviance Pr(>Chi)  \n# 1    990.01  522113016                              \n# 2    990.04  522143950 -0.037064   -30934    0.054 .\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nand the MRF RE outperforms the IID RE:\n\nanova(pr_base_mrf, pr_base_iid, test=\"Chisq\")\n# Analysis of Deviance Table\n# \n# Model 1: prpsqm ~ size + age + dist_metro + s(num_dep, bs = \"mrf\", xt = list(nb = mun_nb))\n# Model 2: prpsqm ~ size + age + dist_metro + s(num_dep, bs = \"re\")\n#   Resid. Df Resid. Dev      Df  Deviance  Pr(>Chi)    \n# 1    990.01  522113016                                \n# 2    995.00  557913777 -4.9939 -35800762 2.786e-13 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nsave(list = ls(), file = \"ch16.RData\")"
  },
  {
    "objectID": "17.html",
    "href": "17.html",
    "title": "17  Spatial econometrics models",
    "section": "",
    "text": "load(\"ch16.RData\")\nlibrary(spdep)\n# Loading required package: sp\n# Loading required package: spData\n# Loading required package: sf\n# Linking to GEOS 3.10.2, GDAL 3.4.3, PROJ 8.2.1; sf_use_s2() is TRUE"
  },
  {
    "objectID": "17.html#ex-17.1",
    "href": "17.html#ex-17.1",
    "title": "17  Spatial econometrics models",
    "section": "17.1 ex 17.1",
    "text": "17.1 ex 17.1\nFirst create a spatial weights object from the k=4 symmetrized neighbour object:\n\nlibrary(spatialreg)\n# Loading required package: Matrix\n# \n# Attaching package: 'spatialreg'\n# The following objects are masked from 'package:spdep':\n# \n#     get.ClusterOption, get.coresOption, get.mcOption,\n#     get.VerboseOption, get.ZeroPolicyOption,\n#     set.ClusterOption, set.coresOption, set.mcOption,\n#     set.VerboseOption, set.ZeroPolicyOption\nlw <- spdep::nb2listw(pr_nb_k4s)\n\nFit a linear model to the lower-level data; all the included variables seem worth retaining:\n\nLM_pr <- lm(f_pr, data=properties_in_dd)\nsummary(LM_pr)\n# \n# Call:\n# lm(formula = f_pr, data = properties_in_dd)\n# \n# Residuals:\n#     Min      1Q  Median      3Q     Max \n# -5691.2  -456.7  -202.2   251.9  6872.9 \n# \n# Coefficients:\n#               Estimate Std. Error t value Pr(>|t|)    \n# (Intercept) 1702.61693   78.59752   21.66  < 2e-16 ***\n# size           5.18681    0.46510   11.15  < 2e-16 ***\n# age          -18.11938    1.33737  -13.55  < 2e-16 ***\n# dist_metro    -0.32446    0.07115   -4.56 5.75e-06 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 792.2 on 996 degrees of freedom\n# Multiple R-squared:  0.2368,  Adjusted R-squared:  0.2345 \n# F-statistic:   103 on 3 and 996 DF,  p-value: < 2.2e-16\n\nHowever, there is strong residual autocorrelation:\n\nlibrary(spdep)\nlm.morantest(LM_pr, lw)\n# \n#   Global Moran I for regression residuals\n# \n# data:  \n# model: lm(formula = f_pr, data = properties_in_dd)\n# weights: lw\n# \n# Moran I statistic standard deviate = 26.039, p-value <\n# 2.2e-16\n# alternative hypothesis: greater\n# sample estimates:\n# Observed Moran I      Expectation         Variance \n#     0.4872794374    -0.0023026155     0.0003535235\n\nRobust Lagrange multiplier tests suggest that the fitted model should include a spatial autoregressive process in the residuals, but not in the response:\n\nspdep::lm.LMtests(LM_pr, lw, test=c(\"RLMerr\", \"RLMlag\"))\n# \n#   Lagrange multiplier diagnostics for spatial dependence\n# \n# data:  \n# model: lm(formula = f_pr, data = properties_in_dd)\n# weights: lw\n# \n# RLMerr = 150.88, df = 1, p-value < 2.2e-16\n# \n# \n#   Lagrange multiplier diagnostics for spatial dependence\n# \n# data:  \n# model: lm(formula = f_pr, data = properties_in_dd)\n# weights: lw\n# \n# RLMlag = 1.2688, df = 1, p-value = 0.26\n\nAdding in the copied out municipality department level variables, we see that they do not seem to be worth retaining (unless there are good reasons for doing so); they do however improve model fit:\n\nLM_pr_md <- lm(f_pr_md, data=properties_in_dd)\nsummary(LM_pr_md)\n# \n# Call:\n# lm(formula = f_pr_md, data = properties_in_dd)\n# \n# Residuals:\n#     Min      1Q  Median      3Q     Max \n# -5175.9  -371.6  -107.1   266.7  6648.1 \n# \n# Coefficients:\n#               Estimate Std. Error t value Pr(>|t|)    \n# (Intercept) 1648.17176  189.84951   8.681   <2e-16 ***\n# size           4.29551    0.43386   9.901   <2e-16 ***\n# age          -20.18585    1.27009 -15.893   <2e-16 ***\n# dist_metro    -0.15180    0.07159  -2.120   0.0342 *  \n# foreigners   -38.13473   22.54311  -1.692   0.0910 .  \n# prgreensp     23.90080   16.33291   1.463   0.1437    \n# popdens      -51.82590  103.65578  -0.500   0.6172    \n# museums      -19.06125   21.27904  -0.896   0.3706    \n# airbnb         0.63284    0.32887   1.924   0.0546 .  \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 727.6 on 991 degrees of freedom\n# Multiple R-squared:  0.3596,  Adjusted R-squared:  0.3544 \n# F-statistic: 69.55 on 8 and 991 DF,  p-value: < 2.2e-16\n\nThe pre-test results are similar to those for the properties-only variables:\n\nlm.morantest(LM_pr_md, lw)\n# \n#   Global Moran I for regression residuals\n# \n# data:  \n# model: lm(formula = f_pr_md, data = properties_in_dd)\n# weights: lw\n# \n# Moran I statistic standard deviate = 23.707, p-value <\n# 2.2e-16\n# alternative hypothesis: greater\n# sample estimates:\n# Observed Moran I      Expectation         Variance \n#     0.4307327715    -0.0070398073     0.0003410046\n\nand the LM tests continue to indicate an omitted spatial process in the residual rather than the response:\n\nlm.LMtests(LM_pr_md, lw, test=c(\"RLMerr\", \"RLMlag\"))\n# \n#   Lagrange multiplier diagnostics for spatial dependence\n# \n# data:  \n# model: lm(formula = f_pr_md, data = properties_in_dd)\n# weights: lw\n# \n# RLMerr = 116.67, df = 1, p-value < 2.2e-16\n# \n# \n#   Lagrange multiplier diagnostics for spatial dependence\n# \n# data:  \n# model: lm(formula = f_pr_md, data = properties_in_dd)\n# weights: lw\n# \n# RLMlag = 0.0035207, df = 1, p-value = 0.9527"
  },
  {
    "objectID": "17.html#ex-17.2",
    "href": "17.html#ex-17.2",
    "title": "17  Spatial econometrics models",
    "section": "17.2 ex 17.2",
    "text": "17.2 ex 17.2\nWe may update the formula for the properties-only model to include municipality department “fixed effects”, dummy variables:\n\nLM_pr_fx <- lm(update(f_pr, . ~ . + num_dep), data=properties_in_dd)\nsummary(LM_pr_fx)\n# \n# Call:\n# lm(formula = update(f_pr, . ~ . + num_dep), data = properties_in_dd)\n# \n# Residuals:\n#     Min      1Q  Median      3Q     Max \n# -5175.3  -377.2   -97.9   272.7  6644.6 \n# \n# Coefficients:\n#               Estimate Std. Error t value Pr(>|t|)    \n# (Intercept)  2.352e+03  9.816e+01  23.964  < 2e-16 ***\n# size         4.295e+00  4.330e-01   9.919  < 2e-16 ***\n# age         -2.007e+01  1.269e+00 -15.819  < 2e-16 ***\n# dist_metro  -1.442e-01  7.154e-02  -2.015 0.044176 *  \n# num_dep2    -3.342e+02  8.695e+01  -3.844 0.000129 ***\n# num_dep3    -4.896e+02  1.290e+02  -3.794 0.000157 ***\n# num_dep4    -1.031e+03  1.193e+02  -8.641  < 2e-16 ***\n# num_dep5    -8.222e+02  8.251e+01  -9.964  < 2e-16 ***\n# num_dep6    -9.183e+02  7.861e+01 -11.681  < 2e-16 ***\n# num_dep7    -6.527e+02  8.250e+01  -7.911 6.78e-15 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 726.2 on 990 degrees of freedom\n# Multiple R-squared:  0.3627,  Adjusted R-squared:  0.3569 \n# F-statistic:  62.6 on 9 and 990 DF,  p-value: < 2.2e-16\n\nThe pre-test output is similar to that for the models considered above:\n\nspdep::lm.morantest(LM_pr_fx, lw)\n# \n#   Global Moran I for regression residuals\n# \n# data:  \n# model: lm(formula = update(f_pr, . ~ . + num_dep), data =\n# properties_in_dd)\n# weights: lw\n# \n# Moran I statistic standard deviate = 23.611, p-value <\n# 2.2e-16\n# alternative hypothesis: greater\n# sample estimates:\n# Observed Moran I      Expectation         Variance \n#     0.4268054149    -0.0079765936     0.0003390813\n\n\nspdep::lm.LMtests(LM_pr_fx, lw, test=c(\"RLMerr\", \"RLMlag\"))\n# \n#   Lagrange multiplier diagnostics for spatial dependence\n# \n# data:  \n# model: lm(formula = update(f_pr, . ~ . + num_dep), data =\n# properties_in_dd)\n# weights: lw\n# \n# RLMerr = 117.99, df = 1, p-value < 2.2e-16\n# \n# \n#   Lagrange multiplier diagnostics for spatial dependence\n# \n# data:  \n# model: lm(formula = update(f_pr, . ~ . + num_dep), data =\n# properties_in_dd)\n# weights: lw\n# \n# RLMlag = 0.030669, df = 1, p-value = 0.861\n\nWe may fit a regimes model, where separate regression coefficients are calculated for interactions between the municipality department dummies and the included variables; size and dist_metro only retian influence for municipality departments 1 and 2:\n\nLM_pr_reg <- lm(update(f_pr, . ~ num_dep/(0 + .)), data=properties_in_dd)\nsummary(LM_pr_reg)\n# \n# Call:\n# lm(formula = update(f_pr, . ~ num_dep/(0 + .)), data = properties_in_dd)\n# \n# Residuals:\n#     Min      1Q  Median      3Q     Max \n# -2055.0  -328.6   -62.8   257.2  6353.5 \n# \n# Coefficients:\n#                       Estimate Std. Error t value Pr(>|t|)    \n# num_dep1            2366.55794  206.45952  11.463  < 2e-16 ***\n# num_dep2            1114.08697  167.84304   6.638 5.29e-11 ***\n# num_dep3            2095.07546  424.99044   4.930 9.68e-07 ***\n# num_dep4            1351.29266  465.10378   2.905 0.003752 ** \n# num_dep5            1695.64352  234.29757   7.237 9.30e-13 ***\n# num_dep6            1611.35873  167.16126   9.640  < 2e-16 ***\n# num_dep7            1705.22449  176.18849   9.678  < 2e-16 ***\n# num_dep1:size          1.36164    0.51169   2.661 0.007918 ** \n# num_dep2:size         14.62370    0.99848  14.646  < 2e-16 ***\n# num_dep3:size          2.25680    4.50973   0.500 0.616886    \n# num_dep4:size          3.79360    4.56205   0.832 0.405864    \n# num_dep5:size          3.06546    1.93716   1.582 0.113872    \n# num_dep6:size          1.49313    1.33800   1.116 0.264724    \n# num_dep7:size          5.91935    1.37694   4.299 1.89e-05 ***\n# num_dep1:age          -6.83169    3.55084  -1.924 0.054651 .  \n# num_dep2:age          -9.36978    3.07762  -3.044 0.002393 ** \n# num_dep3:age         -18.07526    5.65924  -3.194 0.001449 ** \n# num_dep4:age         -22.43736    5.03824  -4.453 9.43e-06 ***\n# num_dep5:age         -27.07143    2.71096  -9.986  < 2e-16 ***\n# num_dep6:age         -23.33611    2.34894  -9.935  < 2e-16 ***\n# num_dep7:age         -24.34731    2.65955  -9.155  < 2e-16 ***\n# num_dep1:dist_metro   -0.72509    0.21429  -3.384 0.000744 ***\n# num_dep2:dist_metro   -0.61219    0.17457  -3.507 0.000474 ***\n# num_dep3:dist_metro   -0.56261    0.52693  -1.068 0.285918    \n# num_dep4:dist_metro    0.02032    0.43294   0.047 0.962570    \n# num_dep5:dist_metro    0.10681    0.29026   0.368 0.712965    \n# num_dep6:dist_metro    0.05074    0.09829   0.516 0.605810    \n# num_dep7:dist_metro   -0.14110    0.14727  -0.958 0.338274    \n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n# \n# Residual standard error: 663.8 on 972 degrees of freedom\n# Multiple R-squared:  0.8323,  Adjusted R-squared:  0.8274 \n# F-statistic: 172.2 on 28 and 972 DF,  p-value: < 2.2e-16\n\nThe pre-test results are now changed, with possible spatial processes in both residuals and response being indicated:\n\nspdep::lm.morantest(LM_pr_reg, lw)\n# \n#   Global Moran I for regression residuals\n# \n# data:  \n# model: lm(formula = update(f_pr, . ~ num_dep/(0 + .)), data\n# = properties_in_dd)\n# weights: lw\n# \n# Moran I statistic standard deviate = 18.521, p-value <\n# 2.2e-16\n# alternative hypothesis: greater\n# sample estimates:\n# Observed Moran I      Expectation         Variance \n#     0.3198250101    -0.0139390950     0.0003247394\n\n\nspdep::lm.LMtests(LM_pr_reg, lw, test=c(\"RLMerr\", \"RLMlag\"))\n# \n#   Lagrange multiplier diagnostics for spatial dependence\n# \n# data:  \n# model: lm(formula = update(f_pr, . ~ num_dep/(0 + .)), data\n# = properties_in_dd)\n# weights: lw\n# \n# RLMerr = 40.873, df = 1, p-value = 1.625e-10\n# \n# \n#   Lagrange multiplier diagnostics for spatial dependence\n# \n# data:  \n# model: lm(formula = update(f_pr, . ~ num_dep/(0 + .)), data\n# = properties_in_dd)\n# weights: lw\n# \n# RLMlag = 19.055, df = 1, p-value = 1.27e-05"
  },
  {
    "objectID": "17.html#ex-17.3",
    "href": "17.html#ex-17.3",
    "title": "17  Spatial econometrics models",
    "section": "17.3 ex 17.3",
    "text": "17.3 ex 17.3\nFitting models initially by maximum likelihood (GMM may also be used), we pre-compute the eigenvalues:\n\neigs <- eigenw(lw)\n\nThe strong residual autocorrelation is picked up by the spatial coefficient, but unfortunately the Hausman test shows strong mis-specification:\n\nSEM_pr <- errorsarlm(f_pr, data=properties_in_dd, listw=lw, Durbin=FALSE,\n    control=list(pre_eig=eigs))\nsummary(SEM_pr, Hausman=TRUE)\n# \n# Call:\n# errorsarlm(formula = f_pr, data = properties_in_dd, listw = lw, \n#     Durbin = FALSE, control = list(pre_eig = eigs))\n# \n# Residuals:\n#       Min        1Q    Median        3Q       Max \n# -3214.224  -337.719   -75.561   196.920  5411.793 \n# \n# Type: error \n# Coefficients: (asymptotic standard errors) \n#               Estimate Std. Error  z value Pr(>|z|)\n# (Intercept) 1999.09195  120.70305  16.5621  < 2e-16\n# size           3.21092    0.35840   8.9591  < 2e-16\n# age          -23.64759    1.06426 -22.2199  < 2e-16\n# dist_metro    -0.27302    0.14694  -1.8581  0.06315\n# \n# Lambda: 0.6927, LR test value: 459.76, p-value: < 2.22e-16\n# Asymptotic standard error: 0.024179\n#     z-value: 28.649, p-value: < 2.22e-16\n# Wald statistic: 820.74, p-value: < 2.22e-16\n# \n# Log likelihood: -7861.931 for error model\n# ML residual variance (sigma squared): 354240, (sigma: 595.18)\n# Number of observations: 1000 \n# Number of parameters estimated: 6 \n# AIC: 15736, (AIC for lm: 16194)\n# Hausman test: -246.29, df: 4, p-value: < 2.22e-16\n\nThe Hausman test compares the OLS and SEM coefficient estimates and their standard errors, assessing whether their distributions overlap sufficiently to suggest the absence of major mis-specification:\n\n(LM_coefs <- coef(summary(LM_pr)))\n#                 Estimate  Std. Error    t value     Pr(>|t|)\n# (Intercept) 1702.6169273 78.59752134  21.662476 1.432984e-85\n# size           5.1868075  0.46509907  11.152049 2.679763e-27\n# age          -18.1193826  1.33737467 -13.548472 1.662323e-38\n# dist_metro    -0.3244594  0.07115232  -4.560068 5.749564e-06\n\n\n(SEM_coefs <- coef(summary(SEM_pr)))\n#                 Estimate  Std. Error    z value   Pr(>|z|)\n# (Intercept) 1999.0919482 120.7030490  16.562067 0.00000000\n# size           3.2109208   0.3583989   8.959070 0.00000000\n# age          -23.6475916   1.0642550 -22.219854 0.00000000\n# dist_metro    -0.2730234   0.1469362  -1.858108 0.06315363\n\nThe tables are harder to read than the figure, which shows that the coefficient estimates do differ a lot for two variables, somewhat for the intercept, and little for one variable, but where the ML standard error estimate under usual assumptions crosses zero:\n\nopar <- par(mfrow=c(2,2))\nplot(1, type=\"n\", xlim=c(1400, 2500), ylim=c(0, 0.006), xlab=rownames(LM_coefs)[1], ylab=\"\")\ncurve(dnorm(x, mean=LM_coefs[1,1], sd=LM_coefs[1,2]), add=TRUE)\nabline(v=LM_coefs[1,1])\nabline(v=SEM_coefs[1,1], lwd=2, col=\"orange\")\ncurve(dnorm(x, mean=SEM_coefs[1,1], sd=SEM_coefs[1,2]), add=TRUE, col=\"orange\", lwd=2)\nlegend(\"topright\", legend=c(\"LM\", \"SEM\"), col=c(\"black\", \"orange\"), lwd=1:2, bty=\"n\")\nplot(1, type=\"n\", xlim=c(1.5, 7), ylim=c(0, 1.1), xlab=rownames(LM_coefs)[2], ylab=\"\")\ncurve(dnorm(x, mean=LM_coefs[2,1], sd=LM_coefs[2,2]), add=TRUE)\nabline(v=LM_coefs[2,1])\nabline(v=SEM_coefs[2,1], lwd=2, col=\"orange\")\ncurve(dnorm(x, mean=SEM_coefs[2,1], sd=SEM_coefs[2,2]), add=TRUE, col=\"orange\", lwd=2)\nplot(1, type=\"n\", xlim=c(-28, -13), ylim=c(0, 0.4), xlab=rownames(LM_coefs)[3], ylab=\"\")\ncurve(dnorm(x, mean=LM_coefs[3,1], sd=LM_coefs[3,2]), add=TRUE)\nabline(v=LM_coefs[3,1])\nabline(v=SEM_coefs[3,1], lwd=2, col=\"orange\")\ncurve(dnorm(x, mean=SEM_coefs[3,1], sd=SEM_coefs[3,2]), add=TRUE, col=\"orange\", lwd=2)\nplot(1, type=\"n\", xlim=c(-0.9, 0.3), ylim=c(0, 6), xlab=rownames(LM_coefs)[4], ylab=\"\")\ncurve(dnorm(x, mean=LM_coefs[4,1], sd=LM_coefs[4,2]), add=TRUE)\nabline(v=LM_coefs[4,1])\nabline(v=SEM_coefs[4,1], lwd=2, col=\"orange\")\ncurve(dnorm(x, mean=SEM_coefs[4,1], sd=SEM_coefs[4,2]), add=TRUE, col=\"orange\", lwd=2)\n\n\n\npar(opar)\n\nThe Hausman test also suggests mis-specification for the SEM model augmented with the municipality department level variables:\n\nSEM_pr_md <- errorsarlm(f_pr_md, data=properties_in_dd, listw=lw, Durbin=FALSE,\n    control=list(pre_eig=eigs))\nsummary(SEM_pr_md, Hausman=TRUE)\n# \n# Call:\n# errorsarlm(formula = f_pr_md, data = properties_in_dd, listw = lw, \n#     Durbin = FALSE, control = list(pre_eig = eigs))\n# \n# Residuals:\n#       Min        1Q    Median        3Q       Max \n# -3421.923  -320.793   -65.301   224.785  5452.101 \n# \n# Type: error \n# Coefficients: (asymptotic standard errors) \n#               Estimate Std. Error  z value  Pr(>|z|)\n# (Intercept) 1945.79329  333.00264   5.8432 5.121e-09\n# size           3.05839    0.35711   8.5643 < 2.2e-16\n# age          -23.45168    1.06301 -22.0616 < 2.2e-16\n# dist_metro    -0.14969    0.13214  -1.1328    0.2573\n# foreigners    -7.66310   42.72513  -0.1794    0.8577\n# prgreensp     38.40538   31.36427   1.2245    0.2208\n# popdens     -211.52189  190.49686  -1.1104    0.2668\n# museums      -33.84713   39.33249  -0.8605    0.3895\n# airbnb         0.59183    0.60062   0.9854    0.3244\n# \n# Lambda: 0.62414, LR test value: 335.94, p-value: < 2.22e-16\n# Asymptotic standard error: 0.028289\n#     z-value: 22.063, p-value: < 2.22e-16\n# Wald statistic: 486.78, p-value: < 2.22e-16\n# \n# Log likelihood: -7836.138 for error model\n# ML residual variance (sigma squared): 345330, (sigma: 587.65)\n# Number of observations: 1000 \n# Number of parameters estimated: 11 \n# AIC: 15694, (AIC for lm: 16028)\n# Hausman test: -1348.4, df: 9, p-value: < 2.22e-16\n\nExtending to the SDEM models, and reporting impacts:\n\nSDEM_pr <- errorsarlm(f_pr, data=properties_in_dd, listw=lw, Durbin=TRUE,\n    control=list(pre_eig=eigs))\nsummary(impacts(SDEM_pr), short=TRUE, zstats=TRUE)\n# Impact measures (SDEM, estimable, n):\n#                 Direct   Indirect       Total\n# size         3.4445403  2.0126062   5.4571465\n# age        -21.9222897 10.9931416 -10.9291481\n# dist_metro   0.2000411 -0.5494199  -0.3493789\n# ========================================================\n# Standard errors:\n#               Direct  Indirect     Total\n# size       0.3983375 1.0053504 1.2310344\n# age        1.1425638 2.5191829 3.1162004\n# dist_metro 0.2734082 0.3104821 0.1551601\n# ========================================================\n# Z-values:\n#                 Direct  Indirect     Total\n# size         8.6472912  2.001895  4.432976\n# age        -19.1869278  4.363773 -3.507203\n# dist_metro   0.7316571 -1.769570 -2.251731\n# \n# p-values:\n#            Direct  Indirect   Total     \n# size       < 2e-16 0.045296   9.2941e-06\n# age        < 2e-16 1.2784e-05 0.00045284\n# dist_metro 0.46438 0.076799   0.02433930\n\nwe have Hausman test results still indicating strong mis-specification:\n\nHausman.test(SDEM_pr)\n# \n#   Spatial Hausman test (asymptotic)\n# \n# data:  NULL\n# Hausman test = 53.765, df = 7, p-value = 2.618e-09\n\nThe same applies to the properties variables augmented with the municipality department level variables:\n\nSDEM_pr_md <- errorsarlm(f_pr_md, data=properties_in_dd, listw=lw, Durbin=TRUE,\n    control=list(pre_eig=eigs))\nsummary(impacts(SDEM_pr_md), short=TRUE, zstats=TRUE)\n# Impact measures (SDEM, estimable, n):\n#                  Direct     Indirect       Total\n# size          3.2902830    1.6915614   4.9818444\n# age         -22.5877018    7.0789015 -15.5088003\n# dist_metro    0.2706259   -0.4643729  -0.1937470\n# foreigners  107.6194775 -146.8457499 -39.2262724\n# prgreensp    35.3436547  -12.5353756  22.8082791\n# popdens    -456.7261822  400.0782320 -56.6479501\n# museums      51.9819769  -93.3777531 -41.3957763\n# airbnb       -0.8658814    1.7162902   0.8504087\n# ========================================================\n# Standard errors:\n#                 Direct    Indirect       Total\n# size         0.3900357   0.9742178   1.1837952\n# age          1.1250931   2.5043048   3.0554700\n# dist_metro   0.2747535   0.3097415   0.1459340\n# foreigners  87.2520037  94.9604414  45.6522669\n# prgreensp   74.3147379  79.3194509  33.1282716\n# popdens    403.5523880 439.7109668 210.7386140\n# museums     85.7152271  93.9719231  43.7578249\n# airbnb       1.1452242   1.2862740   0.6730806\n# ========================================================\n# Z-values:\n#                 Direct   Indirect      Total\n# size         8.4358509  1.7363278  4.2083669\n# age        -20.0762964  2.8266933 -5.0757495\n# dist_metro   0.9849770 -1.4992275 -1.3276345\n# foreigners   1.2334327 -1.5463887 -0.8592404\n# prgreensp    0.4755942 -0.1580366  0.6884838\n# popdens     -1.1317643  0.9098664 -0.2688067\n# museums      0.6064497 -0.9936772 -0.9460200\n# airbnb      -0.7560803  1.3343115  1.2634574\n# \n# p-values:\n#            Direct  Indirect  Total     \n# size       < 2e-16 0.0825059 2.5722e-05\n# age        < 2e-16 0.0047031 3.8597e-07\n# dist_metro 0.32464 0.1338146 0.18430   \n# foreigners 0.21741 0.1220107 0.39021   \n# prgreensp  0.63436 0.8744280 0.49115   \n# popdens    0.25773 0.3628930 0.78808   \n# museums    0.54422 0.3203801 0.34414   \n# airbnb     0.44960 0.1821018 0.20642\n\n\nHausman.test(SDEM_pr_md)\n# \n#   Spatial Hausman test (asymptotic)\n# \n# data:  NULL\n# Hausman test = 229.24, df = 17, p-value < 2.2e-16\n\nReaching out the SLX models does not help, because although - as with the SDEM models - the indirect impacts (coefficients on lagged \\(X\\) variables) are large, so including lagged \\(X\\) variables especially at the properties level seems sensible, there is serious residual autocorrelation, and now the pre-test strategy points to a missing spatial process in the response:\n\nSLX_pr <- lmSLX(f_pr, data=properties_in_dd, listw=lw, Durbin=TRUE)\nsummary(impacts(SLX_pr), short=TRUE, zstats=TRUE)\n# Impact measures (SlX, estimable, n-k):\n#                 Direct   Indirect     Total\n# size         3.9424014  6.4922295 10.434631\n# age        -22.6778266 13.6377582 -9.040068\n# dist_metro   0.5710406 -0.8727816 -0.301741\n# ========================================================\n# Standard errors:\n#               Direct  Indirect      Total\n# size       0.4600099 0.8896271 0.90358915\n# age        1.3642232 2.1630220 2.15389898\n# dist_metro 0.3650804 0.3766962 0.07031201\n# ========================================================\n# Z-values:\n#                Direct  Indirect     Total\n# size         8.570254  7.297698 11.547982\n# age        -16.623252  6.304956 -4.197072\n# dist_metro   1.564150 -2.316938 -4.291458\n# \n# p-values:\n#            Direct  Indirect   Total     \n# size       < 2e-16 2.9265e-13 < 2.22e-16\n# age        < 2e-16 2.8828e-10 2.7039e-05\n# dist_metro 0.11778 0.020507   1.7750e-05\n\n\nspdep::lm.morantest(SLX_pr, lw)\n# \n#   Global Moran I for regression residuals\n# \n# data:  \n# model: lm(formula = formula(paste(\"y ~ \",\n# paste(colnames(x)[-1], collapse = \"+\"))), data =\n# as.data.frame(x), weights = weights)\n# weights: lw\n# \n# Moran I statistic standard deviate = 24.275, p-value <\n# 2.2e-16\n# alternative hypothesis: greater\n# sample estimates:\n# Observed Moran I      Expectation         Variance \n#     0.4505572652    -0.0029076149     0.0003489564\n\n\nspdep::lm.LMtests(SLX_pr, lw, test=c(\"RLMerr\", \"RLMlag\"))\n# \n#   Lagrange multiplier diagnostics for spatial dependence\n# \n# data:  \n# model: lm(formula = formula(paste(\"y ~ \",\n# paste(colnames(x)[-1], collapse = \"+\"))), data =\n# as.data.frame(x), weights = weights)\n# weights: lw\n# \n# RLMerr = 3.9929, df = 1, p-value = 0.04569\n# \n# \n#   Lagrange multiplier diagnostics for spatial dependence\n# \n# data:  \n# model: lm(formula = formula(paste(\"y ~ \",\n# paste(colnames(x)[-1], collapse = \"+\"))), data =\n# as.data.frame(x), weights = weights)\n# weights: lw\n# \n# RLMlag = 54.185, df = 1, p-value = 1.825e-13\n\n\nSLX_pr_md <- lmSLX(f_pr_md, data=properties_in_dd, listw=lw, Durbin=TRUE)\nsummary(impacts(SLX_pr_md), short=TRUE, zstats=TRUE)\n# Impact measures (SlX, estimable, n-k):\n#                  Direct     Indirect       Total\n# size          3.6899279    5.1853598   8.8752878\n# age         -22.4862582    7.7433691 -14.7428891\n# dist_metro    0.4883417   -0.6216376  -0.1332958\n# foreigners  111.8693211 -144.2912677 -32.4219466\n# prgreensp    -7.9374540   33.7296361  25.7921821\n# popdens    -175.9679694   76.3367323 -99.6312371\n# museums     132.9274926 -150.5428303 -17.6153377\n# airbnb       -1.6203107    2.0992718   0.4789610\n# ========================================================\n# Standard errors:\n#                 Direct    Indirect        Total\n# size         0.4374870   0.8640075   0.88540694\n# age          1.2999436   2.1706052   2.20258646\n# dist_metro   0.3469593   0.3606514   0.07353124\n# foreigners 113.3256387 115.6423334  22.69480121\n# prgreensp   98.1127344  99.5966256  16.36663957\n# popdens    521.4661878 532.2960996 105.63203070\n# museums    115.4990904 118.1243717  21.60621836\n# airbnb       1.5181784   1.5638933   0.33577968\n# ========================================================\n# Z-values:\n#                  Direct   Indirect      Total\n# size         8.43437158  6.0015219 10.0239645\n# age        -17.29787276  3.5673779 -6.6934440\n# dist_metro   1.40748996 -1.7236520 -1.8127784\n# foreigners   0.98714927 -1.2477374 -1.4286068\n# prgreensp   -0.08090136  0.3386624  1.5758997\n# popdens     -0.33744847  0.1434103 -0.9431915\n# museums      1.15089645 -1.2744434 -0.8152902\n# airbnb      -1.06727292  1.3423369  1.4264145\n# \n# p-values:\n#            Direct  Indirect   Total     \n# size       < 2e-16 1.9548e-09 < 2.22e-16\n# age        < 2e-16 0.00036057 2.1798e-11\n# dist_metro 0.15928 0.08477068 0.069866  \n# foreigners 0.32357 0.21212723 0.153117  \n# prgreensp  0.93552 0.73486404 0.115049  \n# popdens    0.73578 0.88596617 0.345583  \n# museums    0.24977 0.20250631 0.414906  \n# airbnb     0.28585 0.17948677 0.153749\n\n\nspdep::lm.morantest(SLX_pr_md, lw)\n# \n#   Global Moran I for regression residuals\n# \n# data:  \n# model: lm(formula = formula(paste(\"y ~ \",\n# paste(colnames(x)[-1], collapse = \"+\"))), data =\n# as.data.frame(x), weights = weights)\n# weights: lw\n# \n# Moran I statistic standard deviate = 22.047, p-value <\n# 2.2e-16\n# alternative hypothesis: greater\n# sample estimates:\n# Observed Moran I      Expectation         Variance \n#     0.3978202243    -0.0070160067     0.0003371825\n\n\nspdep::lm.LMtests(SLX_pr_md, lw, test=c(\"RLMerr\", \"RLMlag\"))\n# \n#   Lagrange multiplier diagnostics for spatial dependence\n# \n# data:  \n# model: lm(formula = formula(paste(\"y ~ \",\n# paste(colnames(x)[-1], collapse = \"+\"))), data =\n# as.data.frame(x), weights = weights)\n# weights: lw\n# \n# RLMerr = 15.011, df = 1, p-value = 0.0001069\n# \n# \n#   Lagrange multiplier diagnostics for spatial dependence\n# \n# data:  \n# model: lm(formula = formula(paste(\"y ~ \",\n# paste(colnames(x)[-1], collapse = \"+\"))), data =\n# as.data.frame(x), weights = weights)\n# weights: lw\n# \n# RLMlag = 56.758, df = 1, p-value = 4.929e-14\n\nSo on balance, the pre-test strategy has not worked out too well; it is unclear what is missing in the model."
  },
  {
    "objectID": "17.html#ex-17.4",
    "href": "17.html#ex-17.4",
    "title": "17  Spatial econometrics models",
    "section": "17.4 ex 17.4",
    "text": "17.4 ex 17.4\nTurning to estimating the general nested model first, followed by excluding the Durbin (spatially lagged \\(X\\)) variables, a likelihood ratio test shows that the spatially lagged \\(X\\) variables should be retained in the model:\n\nGNM_pr <- sacsarlm(f_pr, data=properties_in_dd, listw=lw, Durbin=TRUE,\n    control=list(pre_eig1=eigs, pre_eig2=eigs))\n\n\nSARAR_pr <- sacsarlm(f_pr, data=properties_in_dd, listw=lw, \n    control=list(pre_eig1=eigs, pre_eig2=eigs))\n\n\nlmtest::lrtest(SARAR_pr, GNM_pr)\n# Likelihood ratio test\n# \n# Model 1: prpsqm ~ size + age + dist_metro\n# Model 2: prpsqm ~ size + age + dist_metro\n#   #Df  LogLik Df  Chisq Pr(>Chisq)    \n# 1   7 -7814.6                         \n# 2  10 -7783.9  3 61.303  3.096e-13 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nAgain using a likelihood ratio test, the GNM model outperforms the SDEM model:\n\nlmtest::lrtest(SDEM_pr, GNM_pr)\n# Likelihood ratio test\n# \n# Model 1: prpsqm ~ size + age + dist_metro\n# Model 2: prpsqm ~ size + age + dist_metro\n#   #Df  LogLik Df Chisq Pr(>Chisq)    \n# 1   9 -7850.3                        \n# 2  10 -7783.9  1 132.7  < 2.2e-16 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nSDM_pr <- lagsarlm(f_pr, data=properties_in_dd, listw=lw, Durbin=TRUE,\n    control=list(pre_eig=eigs))\n\nas is also the case with the SDM model:\n\nlmtest::lrtest(SDM_pr, GNM_pr)\n# Likelihood ratio test\n# \n# Model 1: prpsqm ~ size + age + dist_metro\n# Model 2: prpsqm ~ size + age + dist_metro\n#   #Df  LogLik Df  Chisq Pr(>Chisq)    \n# 1   9 -7842.4                         \n# 2  10 -7783.9  1 117.06  < 2.2e-16 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nand the SLX model:\n\nlmtest::lrtest(SLX_pr, GNM_pr)\n# Warning in modelUpdate(objects[[i - 1]], objects[[i]]): original\n# model was of class \"SlX\", updated model is of class \"Sarlm\"\n# Likelihood ratio test\n# \n# Model 1: y ~ size + age + dist_metro + lag.size + lag.age + lag.dist_metro\n# Model 2: prpsqm ~ size + age + dist_metro\n#   #Df  LogLik Df  Chisq Pr(>Chisq)    \n# 1   8 -8040.1                         \n# 2  10 -7783.9  2 512.46  < 2.2e-16 ***\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nIs the inclusion of the municipality department level variables in the GNM model justified?\n\nGNM_pr_md <- sacsarlm(f_pr_md, data=properties_in_dd, listw=lw, Durbin=TRUE,\n    control=list(pre_eig1=eigs, pre_eig2=eigs))\n\nNo, not really:\n\nlmtest::lrtest(GNM_pr, GNM_pr_md)\n# Likelihood ratio test\n# \n# Model 1: prpsqm ~ size + age + dist_metro\n# Model 2: prpsqm ~ size + age + dist_metro + foreigners + prgreensp + popdens + \n#     museums + airbnb\n#   #Df  LogLik Df  Chisq Pr(>Chisq)  \n# 1  10 -7783.9                       \n# 2  20 -7773.4 10 20.956     0.0214 *\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nIf we drop the municipality department level variables from the Durbin term, we lose fewer degrees of freedom, so preferring the model including the municipality department level variables:\n\nGNM_pr_md1 <- sacsarlm(f_pr_md, data=properties_in_dd, listw=lw, \n    Durbin= ~ size + age + dist_metro,\n    control=list(pre_eig1=eigs, pre_eig2=eigs))\n\n\nlmtest::lrtest(GNM_pr, GNM_pr_md1)\n# Likelihood ratio test\n# \n# Model 1: prpsqm ~ size + age + dist_metro\n# Model 2: prpsqm ~ size + age + dist_metro + foreigners + prgreensp + popdens + \n#     museums + airbnb\n#   #Df  LogLik Df  Chisq Pr(>Chisq)   \n# 1  10 -7783.9                        \n# 2  15 -7775.5  5 16.794   0.004908 **\n# ---\n# Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nUnfortunately, impacts are depressing here:\n\ntrs <- trW(as(lw, \"CsparseMatrix\"))\ni_GNM_pr_md1 <- impacts(GNM_pr_md1, tr=trs, R=2000)\nsummary(i_GNM_pr_md1, short=TRUE, zstats=TRUE)\n# Impact measures (sacmixed, trace):\n#                  Direct    Indirect        Total\n# size         3.23740995   7.2985861   10.5359961\n# age        -23.03565197  10.9935858  -12.0420662\n# dist_metro   0.26484655  -0.4603147   -0.1954681\n# foreigners  -5.71030959 -23.1783890  -28.8886986\n# prgreensp    5.63045876  22.8542711   28.4847299\n# popdens    -20.87711879 -84.7411115 -105.6182302\n# museums     -4.03610471 -16.3827204  -20.4188251\n# airbnb       0.09458806   0.3839369    0.4785250\n# ========================================================\n# Simulation results ( variance matrix):\n# ========================================================\n# Simulated standard errors\n#                Direct    Indirect       Total\n# size        0.3597102   2.6310560   2.7559362\n# age         1.0627804   6.2919596   6.6519078\n# dist_metro  0.2183679   0.3124920   0.1958554\n# foreigners 11.9421349  48.9399352  60.8353381\n# prgreensp   8.4609139  34.8699213  43.2892966\n# popdens    54.8183313 225.7364135 280.3497429\n# museums    11.3149068  46.5345195  57.8069226\n# airbnb      0.1757385   0.7199914   0.8949945\n# \n# Simulated z-values:\n#                 Direct   Indirect      Total\n# size         9.0112151  2.7755549  3.8259472\n# age        -21.6409225  1.7572086 -1.7954642\n# dist_metro   1.2007739 -1.4363610 -0.9529521\n# foreigners  -0.4611294 -0.4500726 -0.4525888\n# prgreensp    0.6802075  0.6718716  0.6741455\n# popdens     -0.4000896 -0.3987091 -0.3992706\n# museums     -0.3303950 -0.3205705 -0.3227292\n# airbnb       0.5180658  0.5062130  0.5089563\n# \n# Simulated p-values:\n#            Direct  Indirect  Total     \n# size       < 2e-16 0.0055108 0.00013027\n# age        < 2e-16 0.0788823 0.07257977\n# dist_metro 0.22984 0.1508997 0.34061436\n# foreigners 0.64471 0.6526581 0.65084483\n# prgreensp  0.49637 0.5016655 0.50021880\n# popdens    0.68909 0.6901076 0.68969383\n# museums    0.74110 0.7485359 0.74690031\n# airbnb     0.60441 0.6127072 0.61078284\n\nThe values and standard errors of the spatial coefficients suggest numerical problems in finding an optimum where the two coefficients are equally strong but with opposing signs:\n\nc(\"response\"=GNM_pr_md1$rho, \"response se\"=GNM_pr_md1$rho.se, \"residual\"=GNM_pr_md1$lambda, \"residual se\"=GNM_pr_md1$lambda.se)\n#    response.rho     response se residual.lambda     residual se \n#      0.85971901      0.01547632     -0.86679315      0.04377824\n\nIf we fall back on the properties level only GNM, total impacts are only significant in conventional terms for size:\n\ni_GNM_pr <- impacts(GNM_pr, tr=trs, R=2000)\nsummary(i_GNM_pr, short=TRUE, zstats=TRUE)\n# Impact measures (sacmixed, trace):\n#                 Direct   Indirect     Total\n# size         3.3625438  9.2475199 12.610064\n# age        -22.7517497 16.8353080 -5.916442\n# dist_metro   0.2927223 -0.6523253 -0.359603\n# ========================================================\n# Simulation results ( variance matrix):\n# ========================================================\n# Simulated standard errors\n#               Direct  Indirect     Total\n# size       0.3714191 2.8896431 3.0392315\n# age        1.0486097 6.7046236 7.0748598\n# dist_metro 0.2236412 0.3200885 0.2014382\n# \n# Simulated z-values:\n#                Direct  Indirect      Total\n# size         9.073913  3.216071  4.1666854\n# age        -21.739784  2.457904 -0.8929116\n# dist_metro   1.323381 -2.055910 -1.7976272\n# \n# Simulated p-values:\n#            Direct  Indirect  Total     \n# size       < 2e-16 0.0012996 3.0906e-05\n# age        < 2e-16 0.0139750 0.371905  \n# dist_metro 0.18571 0.0397911 0.072236\n\nThe same problem occurs without the municipality department level variables; the impacts are being driven by the large spatial coefficient on the lagged response:\n\nc(\"response\"=GNM_pr$rho, \"response se\"=GNM_pr$rho.se, \"residual\"=GNM_pr$lambda, \"residual se\"=GNM_pr$lambda.se)\n#    response.rho     response se residual.lambda     residual se \n#      0.88013836      0.01319648     -0.89698401      0.03914224"
  },
  {
    "objectID": "17.html#ex-17.5",
    "href": "17.html#ex-17.5",
    "title": "17  Spatial econometrics models",
    "section": "17.5 ex 17.5",
    "text": "17.5 ex 17.5\nWe cannot say that the spatial econometrics approach has reached a clear conclusion. When including the upper level variables, we introduce a lot of spatial autocorrelation at the lower level. It is arguable that the MRF random effect at the upper level and including only the properties level variables gets at least as far as the most complex spatial econometrics models. It is fairly clear that mapping the actual green space and museums, and measuring distance from each property to the attractions would remove the scale problem for those variables. Disaggregation of the foreigners, airbnb and population density variables would be highly desirable. With improvements to the properties level data set, including more variables describing the properties themselves, much of the mis-specification should be removed."
  }
]