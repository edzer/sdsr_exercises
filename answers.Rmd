---
title: "Answers to exercises of _Spatial Data Science_"
author: "Edzer Pebesma"
output:
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    toc_depth: 1
---

# Chapter 1: Intro

## ex 1.1. List five differences between raster and vector data.

- raster data have values for pixels, vector data for points, lines of polygons
- spatial locations of raster pixels are constrained to a grid,
  vector data coordinates can have arbitrary locations (only limited
  by floating point representation of coordinates)
- raster data lend themselves well to represent spatially
  continuously observed variables (such as imagery) or spatially
  continuously varying variables (such as elevation or temperature);
  vector data lend themselves well to represent spatially discrete
  features such as houses and roads, or administrative regions.
- raster data cover their spatial extent completely: every point
  is part of a single pixel; vector data may contain holes, or have
  intersecting geometries where points belong to multiple polygons.
- the operations on raster data are often simple mathematical
  (raster algebra) operations that include spatial operations; such 
  simple operations are usually not available for vector data.
- raster data has trivial topology: it is clear which 4 or 8 pixels
  are the neighbours of every pixel; for vector data spatial there
  are more types of relationships, and these relationships are more
  complicated to detect.

The answer "Raster data is continuous data while vector data is
discrete data." is not complete: a raster of land use type represens
a discrete (type) variable, a polygon map with population density
represents a continuous variable. The difference lies in _spatially_
continuous variables like elevation or temperature which are
more easily represented by raster data, and _spatially_ discrete
features, such as houses and roads, which are easier represented
by vector data.

## ex 1.2. In addition to those listed below figure 1.1, list five further graphical components that are often found on a map.

* scale bar
* data source
* well defined title, subtitle
* orientation indicator, north arrow
* further reference elements: seas, land mass, rivers

## ex 1.3. Why the numeric information shown in figure 1.4 misleading (or meaningless):

The values shown in figure 1.4 are population _total_ associated
with their respective counties.  Without the county boundaries the
meaning disappears: raster pixels do not contain population totals
per pixel, population totals over larger regions or populations
densities can no longer be derived based on this raster map alone.

## ex 1.4. Under which conditions would you expect strong differences when doing geometrical operations on $S^2$, compared to doing them on $R^2$

* when computing distances between two points at large distance from each other
* when determining what the shortest line is between two points,
  in particular near to the poles, or when the antimeridian crosses
  this line

# Chapter 2: Coordinates

## ex 2.1. list three geographic measures that do not have a natural zero origin

* longitude: the zero meridian is arbitrary, 100 years ago there were many other zero meridians fashionable
* latitude: the equator may feel like a natural zero, but one could
  equally use the North Pole as zero, or choose entirely different
  origins and orientation for longitude and latitude.
* altitude (measured w.r.t. mean sea level, geoid, or ellispoid)

## ex 2.2 - 2.4

(thanks to Jonas Hurst)

Convert the (x, y) point s (10, 2), (-10, -1), (10, -2) and (0, 10) to polar cooridnates

```{r}
cart2polar = function(x, y){
  r = sqrt(x*x + y*y)  # compute r (distance from origin)
  phi = atan2(y, x)  # compute phi (angle between point and positive x axis in rad)
  phi_deg = phi * 180 / pi  #  compute angle in deg
  result = c(r, phi_deg)
  return(result)
}

cart2polar(10, 2)
cart2polar(-10, -1)
cart2polar(10, -2)
cart2polar(0, 10)
```

## Convert from Polar to Cartesian

Convert the polar (r, phi) points (10, 45°), (0, 100°) and (5, 259°) to Cartesian coordinates

```{r}
deg2rad = function(angle_degree) {
  angle_degree * pi / 180
}

polar2cart = function(r, phi_deg){
  # phi must be in degrees
  phi_rad = deg2rad(phi_deg)  # convert phi in degrees to radians
  x = r * cos(phi_rad)
  y = r * sin(phi_rad)
  c(x, y) # return value
}

polar2cart(10, 45)
polar2cart(0, 100)
polar2cart(5, 259)
```

assuming the Earth is a sphere with a radius of 6371 km, compute for (lambda, phi) points the great circle distance between (10, 10) and (11, 10), between (10, 80)    >and (11, 80), between (10, 10) and (10, 11) and between (10, 80) and (10, 81).

```{r}
distOnSphere = function(l1, phi1, l2, phi2, radius) {
  l1_rad = deg2rad(l1)
  l2_rad = deg2rad(l2)
  phi1_rad = deg2rad(phi1)
  phi2_rad = deg2rad(phi2)

  theta = acos(
    sin(phi1_rad) * sin(phi2_rad) +
    cos(phi1_rad) * cos(phi2_rad) * cos(abs(l1_rad - l2_rad))
  )
  radius * theta # return value
}

radius = 3671
distOnSphere(10, 10, 11, 10, radius)
distOnSphere(10, 80, 11, 80, radius)
distOnSphere(10, 10, 10, 11, radius)
distOnSphere(10, 80, 10, 81, radius)
```
Unit of all results are kilometers.

# Chapter 3: Geometries

(thanks to Jannis Fröhlking)

## ex 3.1 Give two examples of geometries in 2-D (flat) space that are not simple feature geometries, and create a plot of them.

```{r}
library(sf)
x1 <- st_linestring(rbind(c(0,0),c(2,2),c(0,2),c(2,0)))
x2 <- st_polygon(list(rbind(c(3,0),c(5,2),c(3,2),c(5,0),c(3,0))))
plot(c(x1,x2), col = 2:3)
st_is_simple(x1)
st_is_simple(x2)
```

## ex 3.2 Recompute the coordinates 10.542, 0.01, 45321.789 using precision values 1, 1e3, 1e6, and 1e-2.
```{r 2) precicsion}
for(i in c(1,1e3,1e6,1e-2)) 
  print(round(i * c(10.542, 0.01, 45321.789))/i)
```
## ex 3.3 Describe a practical problem for which an n-ary intersection would be needed.

* for a long-term set of polygons with fire extents, find the polygons that underwent
  0, 1, 2, 3, ... fires
* for a set of extents of n individual plant species, find polygons with 0, 1, ..., n species, or
  find the polygon(s) that contain a particular subset of plant species.

## ex 3.4 How can you create a Voronoi diagram (figure 3.3) that has closed polygons for every point?

Voronoi diagrams have "open polygons", areas that extend into
infinity, for boundary points. These cannot be represented by
simple feature geometries. `st_voronoi` chooses a default (square)
polygon to limit the extent, which can be enlarged. Alternatively,
the extent can be limited using `st_intersection` on its result:

```{r}
library(sf)
par(mfrow = c(2,2))
set.seed(133331)
mp = st_multipoint(matrix(runif(20), 10))
plot(st_voronoi(mp), col = NA, border = 'black')
plot(mp, add = TRUE)
title("default extent")
e2 = st_polygon(list(rbind(c(-5,-5), c(5, -5), c(5,5), c(-5, 5), c(-5,-5))))
plot(st_voronoi(mp, envelope = e2), col = NA, border = 'black')
plot(mp, add = TRUE)
title("enlarged envelope")
e3 = st_polygon(list(rbind(c(0,0), c(1, 0), c(1, 1), c(0, 1), c(0, 0))))
v = st_voronoi(mp) %>% st_collection_extract() # pulls POLYGONs out of GC
plot(st_intersection(v, e3), col = NA, border = 'black', axes=TRUE)
plot(mp, add = TRUE)
title("smaller, intersected envelope")
```

## ex 3.5 Give the unary measure dimension for geometries POINT Z (0 1 1), LINESTRING Z (0 0 1,1 1 2), and POLYGON Z ((0 0 0,1 0 0,1 1 0,0 0 0))

```{r unary measure dimension}
st_dimension(st_point(c(0,1,1)))
st_dimension(st_linestring(rbind(c(0,1,1),c(1,1,2))))
st_dimension(st_polygon(list(rbind(c(0,0,0),c(1,0,0),c(1,1,0),c(0,0,0)))))
```

(these are all zero-dimensional geometries because they are _points_, irrespective the number of dimensions they're defined in)

## ex 3.6 Give the DE-9IM relation between LINESTRING(0 0,1 0) and LINESTRING(0.5 0,0.5 1); explain the individual characters.
```{r DE-9IM relation}
line_1 = st_linestring(rbind(c(0,0),c(1,0)))
line_2 = st_linestring(rbind(c(.5,0),c(.5,1)))
plot(line_1,col = "green")
plot(line_2,col = "red", add = TRUE)
st_relate(line_1, line_2)
```
The DE-9IM relation is F01FF0102

- F   Intersection of green lines interior and red lines interior is empty
- 0   Intersection of green lines interior and red lines boundary results in one point in the middle of the green line
- 1   Intersection of green lines interior and red lines exterior results in a line covering most parts of the green line
- F   Intersection of green lines boundary and red lines interior is empty
- F   Intersection of green lines boundary and red lines boundary is empty
- 0   Intersection of green lines boundary and red lines exterior results in the two boundary points of the green line
- 1   Intersection of green lines exterior and red lines interior results in a line covering most parts of the red line
- 0   Intersection of green lines exterior and red lines boundary results in the upper boundary point of the red line
- 2   Intersection of green lines exterior and red lines results in a polygonal geometry covering everything except the two lines

(the boundary of a LINESTRING is formed by its two end points)

## ex 3.7 Can a set of simple feature polygons form a coverage? If so, under which constraints?
Yes, but I would say that the set may just contain one polygon, because simple features provide no way of assigning points on the boundary of two adjacent polygons to a single polygon.

## ex 3.8 For the nc counties in the dataset that comes with R package sf, find the points touched by four counties.
```{r}
# read data
nc <- st_read(system.file("shape/nc.shp", package="sf"))
# get intersections
(nc_geom = st_geometry(nc))
nc_ints = st_intersection(nc_geom)
plot(nc_ints, main = "All intersections")
# Function to check class of intersection objects
get_points = function(x){
  if(class(x)[2]=="POINT")  return(x)
}
# get points
points = lapply(nc_ints, get_points)
points[sapply(points,is.null)] <- NULL
sf_points = st_sfc(points)
st_crs(sf_points) = st_crs(nc)
# get points with four neighbouring geometries (=states)
touch = st_touches(sf_points, nc_geom)
four_n = sapply(touch, function(y) which(length(y)==4))
names(four_n) = seq_along(four_n)
point_no = array(as.numeric(names(unlist(four_n))))
result = st_sfc(points[point_no])
plot(nc_geom, main = "Points touched by four counties")
plot(result, add = TRUE, col = "red", pch = 10, cex = 2)
```

A more compact way might be to search for points where counties touch
another county _only_ in a point, which can be found using `st_relate`
using a `pattern`:

```{r}
(pts = nc %>% st_relate(pattern = "****0****"))
nc %>% st_relate(pattern = "****0****") %>% lengths() %>% sum()
```
which is, as expected, four times the number of points shown in the plot above.

How can we find these points? See [here](https://edzer.github.io/UseR2017/#higher-level-operations-summarise-interpolate-aggregate-st_join):

```{r}
nc = st_geometry(nc)
s2 = sf_use_s2(FALSE) # use GEOM geometry
pts = st_intersection(nc, nc)
pts = pts[st_dimension(pts) == 0]
plot(st_geometry(nc))
plot(st_geometry(pts), add = TRUE, col = "red", pch = 10, cex = 2)
sf_use_s2(s2) # set back
```

## ex 3.9 How would figure 3.6 look like if delta for the y-coordinate was positive?
Only cells that were fully crossed by the red line would be grey:

```{r}
library(stars)
ls = st_sf(a = 2, st_sfc(st_linestring(rbind(c(0.1, 0), c(1, .9)))))
grd = st_as_stars(st_bbox(ls), nx = 10, ny = 10, xlim = c(0, 1.0), ylim = c(0, 1),
   values = -1)
attr(grd, "dimensions")$y$delta = .1
attr(grd, "dimensions")$y$offset = 0 
r = st_rasterize(ls, grd, options = "ALL_TOUCHED=TRUE")
r[r == -1] = NA
plot(st_geometry(st_as_sf(grd)), border = 'orange', col = NA, 
     reset = FALSE, key.pos=NULL)
plot(r, axes = TRUE, add = TRUE, breaks = "equal") # ALL_TOUCHED=FALSE;
plot(ls, add = TRUE, col = "red", lwd = 2)
```

The reason is that in this case, _lower_ left corners of grid cells are part
of the cell, rather than _upper_ left corners.

# Chapter 4: Spherical geometry

## ex 4.1 Straight GeoJSON lines

How does the GeoJSON format define “straight” lines between
ellipsoidal coordinates (section 3.1.1)? Using this definition of
straight, how would `LINESTRING(0 85,180 85)` look like in a polar
projection? How could this geometry be modified to have it cross
the North Pole?

GeoJSON defines straight lines between pairs of ellipsoidal
coordinates as the straight line in _Cartesian_ space formed by
longitude and latitude. This means e.g. that all parallels are
straight lines.

Using this definition of straight, how would LINESTRING(0 85,180 85) look like in a polar projection? 

Like a half circle:
```{r}
library(sf)
l <- st_as_sfc("LINESTRING(0 85,180 85)") %>%
	st_segmentize(1) %>%
	st_set_crs('EPSG:4326')
plot(st_transform(l, 'EPSG:3995'), col = 'red', lwd = 2,
	 graticule = TRUE, axes = TRUE, reset = FALSE)
```

How could this geometry be modified to have it cross the North Pole?

One would have to let it pass through (0 90) and (180, 90):
```{r}
library(sf)
l <- st_as_sfc("LINESTRING(0 85,0 90,180 90,180 85)") %>%
	st_segmentize(1) %>%
	st_set_crs('EPSG:4326')
plot(st_transform(l, 'EPSG:3995'), col = 'red', lwd = 2,
	 graticule = TRUE, axes = TRUE, reset = FALSE)
```


## ex 4.2 For a typical polygon on $S^2$, how can you find out ring direction?

Ring direction (clock-wise CW, counter clock-wise CCW) is unambiguous
on $R^2$ but not on $S^2$: on $S^2$ every polygon divides the
sphere's surface in two parts. When the inside of the polygon is
taken as the area to the left when traversing the polygons's points
then for a small polygon, then ring direction is CCW if the area
of the polygon is smaller than half of the area of the sphere. For
polygons dividing the sphere in two equal parts (great circles such
as the equator or meridians) ring direction is ambiguous.

## ex 4.3 Are there advantages of using bounding caps over using bounding boxes? If so, list them.

Bounding caps may be more compact (have a smaller area compared
to the bounding box corresponding to the same geometries), they
need fewer parameters, and they are invariant under rotation of
(the origins of) longitude and latitude. 

For areas covering one of the poles, a bounding box will always need
to have a longitude range that spans from -180 to 180, irrespective
whether the geometry is centered around the pole.


## ex 4.4 Why is, for small areas, the orthographic projection centered at the area a good approximation of the geometry as handled on $S^2$

Because that is the closest approximation of the geometry on $R^2$.

## ex 4.5 Fiji

For `rnaturalearth::ne_countries(country = "Fiji",
returnclass="sf")`, check whether the geometry is valid on $R^2$,
on an orthographic projection centered on the country, and on $S^2$.
How can the geometry be made valid on `S^2`? Plot the resulting
geometry back on $R^2$.
Compare the centroid of the country, as computed on $R^2$ and on
$S^2$, and the distance between the two.

Valid on $R^2$:
```{r}
fi = rnaturalearth::ne_countries(country = "Fiji", returnclass="sf") %>%
		st_geometry()
s2 = sf_use_s2(FALSE)
st_is_valid(fi)
```

Valid on orthographic projection:

```{r}
ortho = "+proj=ortho +lon_0=178.6 +lat_0=-17.3"
st_transform(fi, ortho) %>% st_is_valid()
plot(st_transform(fi, ortho), border = 'red')
```

The red line following the antimeridian makes the geometry invalid in
this projection, and also on $S^2$:

```{r}
sf_use_s2(TRUE)
st_is_valid(fi)
```

Make valid on $S^2$, and plot:
```{r}
fi.s2 = st_make_valid(fi)
st_is_valid(fi.s2)
plot(st_transform(fi.s2, ortho), border = 'red')
title("valid")
```

where we see that the line at the antimeridian has disappeared. This
makes plotting in $R^2$ look terrible, with lines spanning the globe:
```{r}
plot(fi.s2, axes = TRUE)
```

Compare the centroid of the country, as computed on $R^2$ and on
$S^2$, and the distance between the two.
```{r}
sf_use_s2(FALSE)
(c1 = st_centroid(fi))
sf_use_s2(TRUE)
(c2 = st_centroid(fi.s2))
st_distance(c1, c2)
sf_use_s2(s2)
```

# Chapter 5: Attributes

## ex 5.1. type of `State`

The appropriate value would be `constant`: there is no identity
relationship of `State` to one of the counties in `nc`, and the
value of `State` is constant through each county in the state
(every point in every county in the state has this value for
`State`).

## ex 5.2. type of `State` for the entire state

Now, the unioned geometry _is_ that of the state, and we can assign
`identity`: there is only one state of North Carolina, an this
geometry is its geometry.

## ex 5.3.  the `AREA` variable

<!--
Use st_area to add a variable with name area to nc. Compare the area and AREA variables in the nc dataset. What are the units of AREA? Are the two linearly related? If there are discrepancies, what could be the cause?
-->

The `nc` dataset is rather old, and did not come with an extensive
report how, in detail, certain variables such as `AREA` were derived,
so some detective work is needed here. How did people do this,
more than three decades ago?

We can now compute `area` by
```{r}
library(sf)
nc = read_sf(system.file("gpkg/nc.gpkg", package="sf"))
nc$AREA[1:10]
s2 = sf_use_s2(FALSE) # use spherical geometry:
nc$area = a_sph = st_area(nc)
nc$area[1:10]
sf_use_s2(TRUE) # use ellipsoidal geometry:
nc$area = a_ell = st_area(nc)
nc$area[1:10]
sf_use_s2(s2) # set back to original
cor(a_ell, a_sph)
```
and this gives the area, in square metres, computed using either
ellipsoidal or spherical geometry. We see that these are not
identical, but nearly perfectly linearly correlated.

A first hypothesis might be a constant factor between the `area`
and `AREA` variables. For this, we could try a power of 10:
```{r}
nc$area2 = units::drop_units(nc$area / 1e10)
cor(nc$AREA, nc$area2)
summary(lm(area2 ~ AREA, nc))
plot(area2 ~ AREA, nc)
abline(0, 1)
```

and we see a pretty good, close to 1:1 correspondence! But the
factor 1e10 is strange: it does not convert square metres into a
usual unit for area, neither for metric nor for imperial units.

Also, there are deviations from the 1:1 regression line.  Could these
be explained by the rounding of `AREA` to three digits? If rounding
to three digits was the only cause of spread around the regression
line, we would expect a residual standard error similar to the standard
deviation of a uniform distribution with width .001, which is
```{r}
sqrt(0.001^2/12)
```
but the one obtained int he regression is three times larger. Also,
the units of `AREA` would be 1e10 $m^2$, or 1e4 $km^2$, which is
odd and could ring some bells: one degree latitude corresponds
roughly to 111 km, so one "square degree" at the equator corresponds
roughly to $1.11^2 \times 10^4$, and at 35 degrees North roughly to
```{r}
111 ^ 2 * cos(35 / 180 * pi)
```
which closely corresponds to the regression slope found above.

We can compute "square degree" area by using the $R^2$ area routines,
e.g. obtained when we set the CRS to `NA`:

```{r}
nc2 = nc
st_crs(nc2) = NA
nc2$area = st_area(nc2) # "square degrees"
plot(area ~ AREA, nc2)
abline(0,1)
cor(nc2$area, nc2$AREA)
summary(lm(area ~ AREA, nc2))
```
We now get a much better fit, a near perfect correlation, and a
regression standard error that corresponds exactly to what one
would expect after rounding `AREA` to three digits.

A further "red flag" against the constant (1e10) conversion
hypothesis is the spatial pattern of the regression residuals
obtained by the first approach:
```{r}
nc$resid = residuals(lm(area2 ~ AREA, nc))
plot(nc["resid"])
```

these residuals clearly show a North-South trend, corresponding
to the effect that the Earth's curvature has been ignored during
the computation of `AREA` (ellipsoidal coordinates were treated
as if they were Cartesian). "Square degrees" become smaller when
going north.

The "unit" of the `AREA` variable is hence "square degree". This
is a meaningless unit for area on the sphere, because a unit square
degree does not have a constant area. 

## ex 5.4 type of `area`

"area" is of type `aggregate`: it is a property of a polygon as a
whole, not of each individual point in the polygon. It is extensive:
if we cut a polygon in two parts, the total area is distributed
over the parts.

## ex 5.5 area-weighted interpolation

From the on-line version of the book we get the code that created the plot:
```{r}
g = st_make_grid(st_bbox(st_as_sfc("LINESTRING(0 0,1 1)")), n = c(2,2))
par(mar = rep(0,4))
plot(g)
plot(g[1] * diag(c(3/4, 1)) + c(0.25, 0.125), add = TRUE, lty = 2)
text(c(.2, .8, .2, .8), c(.2, .2, .8, .8), c(1,2,4,8), col = 'red')
```

A question is how we can make `g` into an `sf` object with the right
attribute values associated with the right geometries. We try values
`1:4`:
```
sf = st_sf(x = 1:4, geom = g)
plot(sf)
```
and see the order of the geometries: row-wise, bottom row first, so
```{r}
sf = st_sf(x = c(1,2,4,8), geom = g)
plot(sf)
```
gives us the source object. We create target geometries by
```{r}
dashed = g[1] * diag(c(3/4, 1)) + c(0.25, 0.125)
box = st_union(g)
c(dashed, box)
```
and can call `st_interpolate_aw` to compute the area-weighted interpolations:
```{r}
st_interpolate_aw(sf, c(dashed, box), extensive = TRUE)
st_interpolate_aw(sf, c(dashed, box), extensive = FALSE)
```
This generates a warning, which we can get rid of by setting the `agr` to
`constant`:
```{r}
st_agr(sf) = "constant"
st_interpolate_aw(sf, c(dashed, box), FALSE)
```


# Chapter 6: Data Cubes

## ex 6.1. 

Why is it difficult to represent trajectories, sequences of $(x,y,t)$
obtained by tracking moving objects, by data cubes as described in
this chapter?

* rounding $(x,y,t)$ to the discrete set of dimension values in a data
  cube may cause loss of information
* if the dimensions all have a high resolution, data loss is limited but the
  data cube will be very sparse; this will only be effective if a system
  capable of storing sparse data cubes is used (e.g. SciDB, TileDB)

## ex 6.2. 

In a socio-economic vector data cube with variables population,
life expectancy, and gross domestic product ordered by dimensions
country and year, which variables have block support for the spatial
dimension, and which have block support for the temporal dimension?

* population has spatial block support (total over an area), typically
not temporal block support (but the population e.g. on a particular
day of the year)
* life expectancy is calculated over the total population of the country,
  and as such has spatial block support; it has temporal block support as
  the number of deaths over a particular period are counted, it is not clear
  whether this always corresponds to a single year or a longer period.
* GDP has both spatial and temporal block support: it is a total over an area
  and a time period.


## ex 6.3. 

The Sentinel-2 satellites collect images in 12 spectral bands;
list advantages and disadvantages to represent them as (i) different
data cubes, (ii) a data cube with 12 attributes, one for each band,
and (iii) a single attribute data cube with a spectral dimension.

* as (i): it would be easy to cope with the differences in cell sizes;
* as (ii): one would have to cope with differences in cell sizes (10, 20, 60m), and
  it would not be easy to consider the spectral reflectance curve of individual pixels
* as (iii): as (ii) but it would be easier to consider (analyse, classify, reduce)
  spectral reflectance curves, as they are now organized in a dimension

## ex 6.4. 

Explain why a curvilinear raster as shown in figure 1.5 can be
considered a special case of a data cube.

* Curvilinear grids do not have a simple relationship between
dimension index (row/col, i/j) to coordinate values (lon/lat, x/y):
one needs both row and col to find the coordinate pair, and from a
coordinate pair a rather complex look-up to find the corresponding
row and column.

## ex 6.5. 

Explain how the following problems can be solved with data cube
operations `filter`, `apply`, `reduce` and/or `aggregate`, and
in which order. Also mention for each which function is applied,
and what the dimensionality of the resulting data cube is (if any):

## ex 6.5.1 

from hourly $PM_{10}$ measurements for a set of air quality
monitoring stations, compute per station the amount of days per
year that the average daily $PM_{10}$ value exceeds 50 $\mu g/m^3$

* convert measured hourly values into daily averages: aggregate (from hourly to daily, function: mean)
* convert daily averages into TRUE/FALSE whether the daily average exceeds 50: apply (function: larger-than)
* compute the number of days: reduce time (function: sum)

This gives a one-dimensional data cube, with dimension "station"

## ex 6.5.2 

for a sequence of aerial images of an oil spill, find the time at
which the oil spill had its largest extent, and the corresponding
extent

* for each image, classify pixels into oil/no oil: apply (function: classify)
* for each image, compute size (extent) of oil spill: reduce space (function: sum)
* for the extent time series, find time of maximum: reduce time (function: which.max, then look up time)

This gives a zero-dimensional data cube (a scalar).

## ex 6.5.3

from a 10-year period with global daily sea surface temperature
(SST) raster maps, find the area with the 10% largest and 10%
smallest temporal trends in SST values.

* from daily SST to trend values per pixel: reduce time (function: trend function, `lm`)
* from trend raster, find 10- and 90-percentile: reduce space (function: quantile)
* using percentiles, threshold the trend raster: apply (function: less than / more than)

This gives a two-dimensional data cube (or raster layer: the reclassified trend raster).


# Chapter 7: sf, stars

## ex. 7.1

 Find the names of the `nc` counties that intersect `LINESTRING(-84 35,-78 35)`; use `[` for this, and use `st_join()` for this.

```{r}
(file = system.file("gpkg/nc.gpkg", package="sf"))
nc = st_read(file)
line = st_as_sfc("LINESTRING(-84 35,-78 35)", crs = st_crs(nc))
nc[line,]$NAME
st_join(st_sf(line), nc)$NAME # left join: `line` should be first argument
```

## ex. 7.2

Repeat this after setting `sf_use_s2(FALSE)`, and _compute_ the difference (hint: use `setdiff()`), and color the counties of the difference using color '#00880088'.

```{r}
# save names first:
sf_use_s2(TRUE)
names_with_s2 = nc[line,]$NAME
sf_use_s2(FALSE)
nc[line,]$NAME
(diff = setdiff(names_with_s2, nc[line,]$NAME))
plot(st_geometry(nc))
plot(st_geometry(nc)[nc$NAME %in% diff], col = "#00880088", add = TRUE)
```

## ex. 7.3

Plot the two different lines in a single plot; note that R will plot a straight line always straight in the projection currently used; `st_segmentize` can be used to add points on straight line, or on a great circle for ellipsoidal coordinates.

```{r}
plot(st_geometry(nc))
plot(st_geometry(nc)[nc$NAME %in% diff], col = "#00880088", add = TRUE)
plot(line, add = TRUE)
plot(st_segmentize(line, units::set_units(10, km)), add = TRUE, col = 'red')
```

To show that the red line is curved, but only curved in plate carree,
and not e.g. in an orthographic projection centered at this region,
we can also plot it in an orthographic projection:

```{r}
l.gc = st_segmentize(line, units::set_units(10, km))
l.pc = st_segmentize(st_set_crs(line, NA), 0.1) %>% st_set_crs(st_crs(l.gc))
o = st_crs("+proj=ortho +lon_0=-80 +lat_0=35")
plot(st_transform(st_geometry(nc), o), axes = TRUE)
plot(st_transform(st_geometry(nc), o)[nc$NAME %in% diff],
	 col = "#00880088", add = TRUE)
plot(st_transform(l.gc, o), col = 'red', add = TRUE)
plot(st_transform(l.pc, o), col = 'black', add = TRUE)
plot(st_transform(line, o), col = 'green', add = TRUE)
```

The fact that the _unsegmented_ line `line` is straight (R plotted
it as straight, it contains only the two endpoints) and that it
covers the red line supports that in this plot, the great circle
line (red) _is_ plotted straight, and the "straight in plate carree"
line is not.

## ex. 7.4

NDVI, normalized differenced vegetation index, is computed as `(NIR-R)/(NIR+R)`, with NIR the near infrared and R the red band.  Read the `L7_ETMs.tif` file into object `x`, and distribute the band dimensions over attributes by `split(x, "band")`. Then, add attribute NDVI to this object by using an expression that uses the NIR (band 4) and R (band 3) attributes directly.


```{r}
library(stars)
(x = read_stars(system.file("tif/L7_ETMs.tif", package = "stars")))
(x.spl = split(x))
x.spl$NDVI = (x.spl$X4 - x.spl$X3)/(x.spl$X4 + x.spl$X3)
plot(x.spl["NDVI"])
```

## ex. 7.5

Compute NDVI for the `L7_ETMs.tif` image by reducing the band dimension, using `st_apply` and an a function `ndvi = function(x) { (x[4]-x[3])/(x[4]+x[3]) }`. Plot the result, and write the result to a GeoTIFF. 

```{r}
ndvi_fn = function(x) { (x[4]-x[3])/(x[4]+x[3]) }
ndvi = st_apply(x, 1:2, ndvi_fn)
plot(ndvi)
write_stars(ndvi, "ndvi.tif")
```

an alternative function is 

```{r}
ndvi_fn = function(x1,x2,x3,x4,x5,x6) { (x4-x3)/(x4+x3) }
ndvi2 = st_apply(x, 1:2, ndvi_fn)
all.equal(ndvi, ndvi2)
```

This latter function can be much faster, as it is called for chunks
of data rather than for individual pixels.

## ex. 7.6

Use `st_transform` to transform the `stars` object read from `L7_ETMs.tif` to `EPSG:4326`. Print the object. Is this a regular grid? Plot the first band using arguments `axes=TRUE` and explain why this takes such a long time.

```{r eval=FALSE}
(x_t = st_transform(x, 'EPSG:4326'))
plot(x_t[,,,1], axes = TRUE)
```

the report says it is a curvilinear grid. Plotting takes so long because for
curvilinear grids, each cell is converted to a small polygon and then plotted.

## ex. 7.7

Use `st_warp` to warp the `L7_ETMs.tif` object to `EPSG:4326`, and plot the resulting object with `axes=TRUE`. Why is the plot created much faster than after `st_transform`?

```{r}
x_w = st_warp(x, crs = 'EPSG:4326')
plot(x_w[,,,1], reset = FALSE)
plot(st_as_sfc(st_bbox(x_w)), col = NA, border = 'red', add = TRUE)
```

Plotting is faster now because we created a new regular grid. Note
that the grid border does not align perfectly with the square
formed by the bounding box (using straight lines in an equidistant
rectangular projection): white grid cells indicate the misalignment
due to warping/transforming.

## ex. 7.8

Using a vector representation of the raster `L7_ETMs`, plot the
intersection with a circular area around `POINT(293716 9113692)`
with radius 75 m,  and compute the area-weighted mean pixel values
for this circle.  Compare the area-weighted values with those
obtained by `aggregate` using the vector data, and by `aggregate`
using the raster data, using `exact=FALSE` (default) and
`exact=FALSE`.  Explain the differences.

```{r}
l7 = st_as_sf(x)
st_agr(l7) = "constant"
a = st_as_sfc("POINT(293716 9113692)", crs = st_crs(l7)) %>%
	st_buffer(units::set_units(74, m))
plot(st_intersection(l7, a))
(aw = st_interpolate_aw(l7, a, mean, extensive = FALSE))
(ag_vector  = aggregate(l7, a, mean))
(ag_rasterF = st_as_sf(aggregate(x, a, mean)))
(ag_rasterT = st_as_sf(aggregate(x, a, mean, exact = TRUE)))
rbind(st_drop_geometry(aw),
	  st_drop_geometry(ag_vector), 
	  st_drop_geometry(ag_rasterF), 
	  st_drop_geometry(ag_rasterT))
```

Area-weighted interpolation computes the area-weighted mean of the
areas shown in the plot; `aggregate` on the vector values computes
the _unweighted_ mean over all polygonized pixels that intersect
with the circle (black lines); `aggregate` on the raster values only
averages (unweighted) the cells with pixel _centers_ intersecting
with the circle (light red):

```{r}
plot(st_geometry(l7)[a])
plot(a, add = TRUE, col = NA, border = 'red')
plot(st_as_sf(L7_ETMs[a])[1], add = TRUE, col = '#ff000066')
plot(st_as_sf(L7_ETMs[a], as_points = TRUE)[1], add = TRUE, pch = 3, col = 1)
```

# Chapter 8: large datasets

## ex 8.1

For the S2 image (above), find out in which order the bands are
using `st_get_dimension_values()`, and try to find out (e.g. by internet
search) which spectral bands / colors they correspond to.

```{r}
f = "sentinel/S2A_MSIL1C_20180220T105051_N0206_R051_T32ULE_20180221T134037.zip"
  granule = system.file(file = f, package = "starsdata")
file.size(granule)
base_name = strsplit(basename(granule), ".zip")[[1]]
s2 = paste0("SENTINEL2_L1C:/vsizip/", granule, "/", base_name, 
	".SAFE/MTD_MSIL1C.xml:10m:EPSG_32632")
library(stars)
(p = read_stars(s2, proxy = TRUE))
st_get_dimension_values(p, "band")
```

## ex 8.2

Compute NDVI for the S2 image, using `st_apply` and an an appropriate 
`ndvi` function.  Plot the result to screen, and then write the
result to a GeoTIFF. Explain the difference in runtime between
plotting and writing.

```{r}
ndvi_fn = function(r, g, b, nir) (nir-r)/(nir+r)
ndvi = st_apply(p, 1:2, ndvi_fn)
plot(ndvi)
```

Alternatively, one could use
```{r}
ndvi_fn = function(r, g, b, nir) (nir-r)/(nir+r)
```

but that is much less efficient. Write to a tiff:
```{r}
system.time(write_stars(ndvi, "ndvi.tif"))
```

The runtime difference is caused by the fact that `plot` downsamples,
so computes a very small fraction of the available pixels, where
`write_stars` computes all pixels, and then writes them.

## ex 8.3

Plot an RGB composite of the S2 image, using the `rgb` argument
to `plot()`, and then by using `st_rgb()` first.

```{r}
plot(p, rgb = 1:3)
# plot(st_rgb(p[,,,1:3], maxColorValue=13600)) # FIXME: fails
```

## ex 8.4

select five random points from the bounding box of `S2`, and extract
the band values at these points. What is the class of the object returned?
Convert the object returned to an `sf` object.

```{r}
pts =  p %>% st_bbox() %>% st_as_sfc() %>% st_sample(5)
(p5 = st_extract(p, pts))
class(p5)
st_as_sf(p5)
```

## ex 8.5

For the 10 km radius circle around `POINT(390000  5940000)`, compute
the mean pixel values of the S2 image when downsampling the images
with factor 30, and on the original resolution. Compute the relative
difference between the results.

```{r}
b = st_buffer(st_sfc(st_point(c(390000, 5940000)), crs = st_crs(p)), 
	units::set_units(10, km))
plot(p[,,,1], reset = FALSE, axes = TRUE)
plot(b, col = NA, border = 'green', add = TRUE)
p1 = st_as_stars(p, downsample = 30)
a1 = aggregate(p1, b, mean)
```

For the full resolution, this takes a while:

```{r eval=TRUE}
system.time(a2 <- aggregate(p, b, mean))
```

Relative differences: we will work on the array of the stars objects:

```{r}
(a1[[1]] - a2[[1]])/((a1[[1]]+a2[[1]])/2)
```

Alternatively one could convert `a1` and `a2` to a `data.frame`, using
`as.data.frame`, and work on the third column of the data frames.

## ex 8.6 

Use `hist` to compute the histogram on the downsampled S2 image.
Also do this for each of the bands. Use `ggplot2` to compute a
single plot with all four histograms in facets.

```{r}
hist(p1)
hist(p1[,,,1])
hist(p1[,,,2])
hist(p1[,,,3])
hist(p1[,,,4])
library(ggplot2)
ggplot(as.data.frame(p1), aes(x = MTD_MSIL1C.xml.10m.EPSG_32632)) +
		geom_histogram() + facet_wrap(~band)
```

## ex 8.7

Use `st_crop` to crop the S2 image to the area covered by the 10 km circle.
Plot the results. Explore the effect of setting argument `crop = FALSE`

```{r}
plot(st_crop(p, b))
plot(st_crop(p, b, crop = FALSE))
```

## ex 8.8

With the downsampled image, compute the logical layer where all four
bands have pixel values higher than 1000. Use a raster algebra expression
on the four bands (use `split` first), or use `st_apply` for this.

```{r}
p_spl = split(p1)
p_spl$high = p_spl$B4 > 1000 & p_spl$B3 > 1000 & p_spl$B2 > 1000 & p_spl$B8 > 1000
plot(p_spl["high"])
```

alternative, using `st_apply` on the band dimension

```{r}
p2 = st_apply(p1, 1:2, function(x) all(x > 1000))
plot(p2)
```

# Chapter 12: Spatial Interpolation

## ex 12.1

_Create a plot like the one in figure 12.13 that has the inverse distance interpolated map of figure 12.2 added on left side._

load the .Rmd of this chapter in rstudio, and run it all the way up to the chunk where figure 12.13 is created
(note that this requires that the population density csv file is present).

```{r}
load("ch12.RData")
```

The chunk preceding the one that creates the plot can be modified
as follows, to add inverse distance interpolations to the kriging
and residual kriging:

```{r}
library(stars)
library(gstat)
kr = krige(NO2~sqrt(pop_dens), no2.sf, grd["pop_dens"], vr.m)
k$kr1 = k$var1.pred
k$kr2 = kr$var1.pred
i = idw(NO2~1, no2.sf, grd["values"])
k$kr0 = i$var1.pred
st_redimension(k[c("kr0", "kr1", "kr2")], 
	along = list(what = c("idw", "kriging", "residual kriging"))) %>%
	setNames("NO2") -> km
```

Next, the plot can be created identically, as the `what` dimension now
contains the inverse distance "layer":

```{r}
library(ggplot2)
g + geom_stars(data = km, aes(fill = NO2, x = x, y = y)) + 
    geom_sf(data = st_cast(de, "MULTILINESTRING")) + 
    geom_sf(data = no2.sf) + facet_wrap(~what) +
    coord_sf(lims_method = "geometry_bbox")
```

## ex 12.2

_Create a scatter plot of the map values of the idw and kriging map, and a scatter plot of map values of idw and residual kriging._

For this we can e.g. convert the object $k$ to a `data.frame`:

```{r}
as.data.frame(k) |> head() # NA indicates cells outside AOI
k.df = as.data.frame(k)
plot(kr0 ~ kr1, k.df, xlab = "kriging", ylab = "idw")
plot(kr0 ~ kr2, k.df, xlab = "residual kriging", ylab = "idw")
```

## ex 12.3

_Carry out a block kriging by setting the `block` argument in
`krige()`, and do this for block sizes of 10 km (the grid cell size),
50 km and 200 km. Compare the resulting maps of estimates for these
three blocks sizes with those obtained by point kriging, and
do the same thing for all associated kriging standard errors._

For the point and block kriging values: 
```{r}
b0 =   krige(NO2~1, no2.sf, grd["values"], v.m) # points kriging
b10 =  krige(NO2~1, no2.sf, grd["values"], v.m, block = c(1e4, 1e4))
b50 =  krige(NO2~1, no2.sf, grd["values"], v.m, block = c(5e4, 5e4))
b200 = krige(NO2~1, no2.sf, grd["values"], v.m, block = c(2e5, 2e5))
b10$points = b0$var1.pred
b10$b10 = b10$var1.pred
b10$b50 = b50$var1.pred
b10$b200 = b200$var1.pred
st_redimension(b10[c("points", "b10", "b50", "b200")], 
	along = list(what = c("points", "blocks: 10 km", 
						  "blocks: 50 km", "blocks: 200 km"))) %>%
	setNames("NO2") -> b
g + geom_stars(data = b, aes(fill = NO2, x = x, y = y)) + 
    geom_sf(data = st_cast(de, "MULTILINESTRING")) + 
    geom_sf(data = no2.sf) + facet_wrap(~what) +
    coord_sf(lims_method = "geometry_bbox")
```

For the standard errors:
```{r}
b10$pointse = sqrt(b0$var1.var)
b10$b10se = sqrt(b10$var1.var)
b10$b50se = sqrt(b50$var1.var)
b10$b200se = sqrt(b200$var1.var)
st_redimension(b10[c("pointse", "b10se", "b50se", "b200se")], 
	along = list(what = c("points", "blocks: 10 km", 
						  "blocks: 50 km", "blocks: 200 km"))) %>%
	setNames("NO2_SE") -> b
g + geom_stars(data = b, aes(fill = NO2_SE, x = x, y = y)) + 
    geom_sf(data = st_cast(de, "MULTILINESTRING")) + 
    geom_sf(data = no2.sf) + facet_wrap(~what) +
    coord_sf(lims_method = "geometry_bbox")

```

## ex 12.4

_Based on the residual kriging results obtained above, compute
maps of the lower and upper boundary of a 95% confidence interval,
when assuming that the kriging error is normally distributed,
and show them in a plot with a single (joint) legend_

```{r}
b10$lower = b10$points - 1.96 * b10$pointse
b10$upper = b10$points + 1.96 * b10$pointse
st_redimension(b10[c("lower", "upper")], 
	along = list(what = c("lower boundary C.I.", "upper boundary C.I."))) %>%
	setNames("NO2_95CI") -> b
g + geom_stars(data = b, aes(fill = NO2_95CI, x = x, y = y)) + 
    geom_sf(data = st_cast(de, "MULTILINESTRING")) + 
    geom_sf(data = no2.sf) + facet_wrap(~what) +
    coord_sf(lims_method = "geometry_bbox")
```	

## ex 12.5
_Compute and show the map with the probability that NO2 point values
exceeds the level of 15 ppm, assuming a normal distribution._

For this we use `pnorm()`, which gives the cumulative area under the
curve from minus infinity up to a given value; one minus that value
gives the probability of exceeding it.

```{r}
b10$`p(NO2 > 15)` = 1 - pnorm(15, b10$points, b10$pointse)
g + geom_stars(data = b10, aes(fill = `p(NO2 > 15)`, x = x, y = y)) + 
    geom_sf(data = st_cast(de, "MULTILINESTRING")) + 
    geom_sf(data = no2.sf, col = 'orange') +
    coord_sf(lims_method = "geometry_bbox")
```

# Chapter 13: Multivariate and Spatiotemporal Geostatistics

## ex 13.1 

Which fraction of the stations is removed in section \@ref(preparing) when the criterion applied that a station must be 75% complete?

```{r}
load("ch13.RData")
sel = apply(aq, 2, function(x) mean(is.na(x)) < 0.25)
1 - mean(sel)
```
meaning, 1.7 percent of the stations were removed in this step. We can use `mean` becasue the logical values `TRUE` and `FALSE` map to 1 and 0, respectively, when treated as numeric.

## ex 13.2

From the hourly time series in `no2.st`, compute daily mean concentrations using `aggregate`, and compute the spatiotemporal variogram of this. How does it compare to the variogram of hourly values?

```{r}
no2.d = aggregate(no2.st, "1 day", mean, na.rm = TRUE)
v.d = variogramST(NO2~1, no2.d)
```

## ex 13.3

Carry out a spatiotemporal interpolation for daily mean values for the days corresponding to those shown in \@ref(fig:plotspatiotemporalpredictions), and compare the results.

```{r}
prodSumModel <- vgmST("productSum",
	space = vgm(50, "Exp", 200, 0),
	time = vgm(20, "Sph", 40, 0),
	k = 2)
StAni = estiStAni(v.d, c(0,20000))
(fitProdSumModel <- fit.StVariogram(v.d, prodSumModel, fit.method = 7,
	stAni = StAni, method = "L-BFGS-B",
	control = list(parscale = c(1,10,1,1,0.1,1,10)),
	lower = rep(0.0001, 7)))
plot(v.d, fitProdSumModel, wireframe = FALSE, all = TRUE, scales = list(arrows=FALSE), zlim = c(0,50))
```
```{r}

```

## ex 13.4

Following the example in the demo scripts pointed at in section \@ref(cokriging), carry out a cokriging on the daily mean station data for the four days shown in \@ref(fig:plotspatiotemporalpredictions).  What are the differences of this approach to spatiotemporal kriging?


# Chapter 14: Proximity and Areal Data

## ex 14.1

If dimensionality (point/line/polygon) varies in the data set, geometries must be reduced to the lowest dimension present (usually points). If all the observations are polygonal (polygon or multipolygon), contiguities (shared boundaries) are a sparse and robust neighbour representation (`spdep::poly2nb()`). Polygons may also be reduced to points by taking for example centroids, but neighbours found by triangulating points may not be the same as contiguity neighbours for the polygons being represented by these centroids (`spdep::tri2nb()`). If the geometries are multipoint, they must also be reduced to a single point. If the geometries have point rather than areal support, for example real estate transaction data, k-nearest neighbour (`spdep::knn2nb(spdep::knearneigh())`), graph-based (`spdep::graph2nb()` applied to the output of `spdep::soi.graph()`, `spdep::relativeneigh()` or `spdep::gabrielneigh()`) and distance-based methods (spdep::dnearneigh()`) may be used.

## ex 14.2

Graph-based functions for creating neighbour objects (`spdep::soi.graph()`, `spdep::relativeneigh()` and `spdep::gabrielneigh()`) may not be used if the support of the observations is not that of points on the plane. All other functions may be used with both planar and spherical/elliptical geometries, but the neighbours generated may differ if a non-planar data set is treated as planar.

## ex 14.3

A chessboard is an $8 \times 8$ grid:

```{r}
xy <- data.frame(expand.grid(1:8, 1:8), col=rep(c(rep(c("black", "white"), 4), rep(c("white", "black"), 4)), 4))
library(stars)
library(sf)
(xy %>% st_as_stars() %>% st_as_sf() -> grd)
```

```{r}
library(spdep)
(rook <- poly2nb(grd, queen=FALSE))
```
The `rook` neighbours also form a grid, where the neighbours share a grid edge:

```{r}
plot(st_geometry(grd), col=grd$col) 
plot(rook, xy, add=TRUE, col="grey")
```


```{r}
(queen <- poly2nb(grd, queen=TRUE))
```
The `queen` neighbours add neighbours sharing only one corner point:
```{r}
plot(st_geometry(grd), col=grd$col) 
plot(queen, xy, add=TRUE, col="grey")
```

and the difference yields neighbours sharing not more than one boundary point:

```{r}
plot(st_geometry(grd), col=grd$col) 
plot(diffnb(queen, rook), xy, add=TRUE, col="grey")
```

## ex 14.4

We can access cardinalities using `card()`, and tabulate their frequencies for the chessboard rook case:

```{r}
((rook %>% card() -> rc) %>% table() -> t)
```
Taking the counts found, we can construct the weights corresponding to those neighbour counts:

```{r}
1/rev(as.numeric(names(t)))
```
Plotting the row-standardized weights, we see that they up-weight the neighbours of observations with few neighbours, and down-weight the neighbours of observations with more neighbours:

```{r}
grd$rc <- as.factor(1/rc)
plot(grd[, "rc"], main="rook row-standardized weights", key.width = lcm(2.5))
```

We can also use the cardinality frequency table to find counts of neighbours with (increasing) weights:

```{r}
unname(rev(t))*rev(as.numeric(names(t)))
```
This can be confirmed by tabulating the frequencies of weights yielded by `nb2listw()`:

```{r}
table(unlist(nb2listw(rook, style="W")$weights))
```
Repeating for the `queen` case again shows how row-standardization can engender edge effects:

```{r}
((queen %>% card() -> rc) %>% table() -> t)
```
```{r}
1/rev(as.numeric(names(t)))
```
```{r}
grd$rc <- as.factor(1/rc)
plot(grd[, "rc"], main = "rook row-standardised weights", key.width = lcm(2.5))
```

```{r}
unname(rev(t))*rev(as.numeric(names(t)))
```


```{r}
table(unlist(nb2listw(queen, style="W")$weights))
```

# Chapter 15: Measures of spatial autocorrelation


## ex 15.1

Re-using the objects from exercise 14.3, we have:

```{r}
(grd$col %>% factor() -> COL) %>% table()
```
In the `rook` case, no `black:black` or `white:white` neighbours are found, differing greatly from the expected values, which are based on  non-free sampling from the proportions of colours in the data. Highly significant spatial autocorrelation is detected:

```{r}
(jc_r <- joincount.multi(COL, listw=nb2listw(rook, style="B")))
```
In the `queen` neighbour case, no spatial autocorrelation is found, despite a chessboard looking spatially structured:

```{r}
joincount.multi(COL, listw=nb2listw(queen, style="B"))
```
This is because we have chosen to see all eight neighbour grid cells as neighbours (away from the edges of the board), so the two categories occur equally often as neighbour values, as expected.


## ex 15.2

First, create an uncorrelated variable, and confirm that it is uncorrelated:

```{r}
set.seed(1)
x <- rnorm(nrow(grd))
moran.test(x, nb2listw(queen, style="W"), randomisation=FALSE, alternative="two.sided")
```
Next inject patterning into the variable by adding a linear trend:

```{r}
x_t <- x + (0.15 * xy$Var1)
moran.test(x_t, nb2listw(queen, style="W"), randomisation=FALSE, alternative="two.sided")
```
Test again having taken the residuals from a linear model removing the injected trend:

```{r}
lm.morantest(lm(x_t ~ xy$Var1), nb2listw(queen, style="W"), alternative="two.sided")
```
This is important to understand because the spatial patterning in a variable of interest, and picked up by a global measure of spatial autocorrelation, may be driven by an omitted variable. If we cannot add that variable, a latent variable or mixed effects model may be a good choice.

## ex 15.3

Following the discovery that, irrespective of the number of draws, permutation conditional standard deviates (omitting the value at $i$) are very close to analytical conditional standard deviates for $I_i$ and $G_i$, it seems that the use of conditional permutation is no longer necessary, unless one wishes to check for robustness against distributional assumptions. It will be interesting to see whether the same holds for local Geary's $C_i$ and other local measures.

## ex 15.4

False discovery rate adjustment is required when conducting repeated tests on the same data set. Usually, local measures of spatial autocorrelation are calculated for all the observations in a data set, and so constitute repeated tests. When repeated tests are conducted, the usual reading of confidence intervals and probability values must be adjusted to take the repeated use of the data into account. 

## ex 15.5

If we start with the standard local Moran's $I_i$ for the random values with a slight 1D trend, upgraded to analytical conditional standard deviates, but with only the standard intercept-only mean model, we have a starting point; a fair number of the values exceed 2:

```{r}
locm <- localmoran(x_t, nb2listw(queen, style="W"), conditional=TRUE,
    alternative="two.sided")
```

```{r}
plot(density(locm[, 4]))
abline(v=c(-2, 2))
```


```{r}
grd$locm_sd <- locm[, 4]
plot(grd[, "locm_sd"]) 
```

```{r}
sum(p.adjust(locm[, 5], method="none") < 0.05)
```

If we apply false discovery rate adjustment, we have just one significant measure:

```{r}
sum(p.adjust(locm[, 5], method="fdr") < 0.05)
```


In the first Saddlepoint approximation also for the random values with a slight 1D trend, the distribution of standard deviates shifts leftward, with both positive and negative values beyond `abs(2)`:

```{r}
lm_null <- lm(x_t ~ 1)
locm_null <- summary(localmoran.sad(lm_null, nb=queen, style="W", 
    alternative="two.sided"))
```

```{r}
plot(density(locm_null[, "Saddlepoint"]))
abline(v=c(-2, 2))
```


```{r}
grd$locm_null_sd <- locm_null[, "Saddlepoint"]
plot(grd[, "locm_null_sd"]) 
```

```{r}
sum(p.adjust(locm_null[, "Pr. (Sad)"], method="none") < 0.05)
```

If we apply false discovery rate adjustment, we also have just one significant measure:

```{r}
sum(p.adjust(locm_null[, "Pr. (Sad)"], method="fdr") < 0.05)
```


Once we analyse a model including the 1D trend, most of the distribution of standard deviate values is between -2 and 2:

```{r}
lm_trend <- lm(x_t ~ xy$Var1)
locm_tr <- summary(localmoran.sad(lm_trend, nb=queen, style="W", 
    alternative="two.sided"))
```


```{r}
plot(density(locm_tr[, "Saddlepoint"]))
abline(v=c(-2, 2))
```

```{r}
grd$locm_tr_sd <- locm_tr[, "Saddlepoint"]
plot(grd[, "locm_tr_sd"]) 
```

```{r}
sum(p.adjust(locm_tr[, "Pr. (Sad)"], method="none") < 0.05)
```

If we apply false discovery rate adjustment, we now have no significant measures, as expected:

```{r}
sum(p.adjust(locm_tr[, "Pr. (Sad)"], method="fdr") < 0.05)
```

`localmoran.sad()` or `localmoran.exact()` provide both richer mean models, and estimates of the standard deviates built on the underlying spatial relationships for each observation, rather than analytical or permutation assumptions for the whole data set. This is achieved at the cost of longer compute times and larger memory use, especially when the `Omega=` argument to `localmoran.sad()` or `localmoran.exact.alt()` is used, because this is a dense $n \times n$ matrix.


# Chapter 16: Spatial Regression


## ex 16.1

The **HSAR** package includes an upper level polygon support municipality department data set, and a lower level property data set. Both are `"sf"` objects, in the same projected CRS.  

```{r}
library(sf)
library(HSAR)
data(depmunic)
data(properties)
depmunic$popdens <- depmunic$population/ (10000*depmunic$area)
depmunic$foreigners <- 100 * depmunic$pop_rest/ depmunic$population
depmunic$prgreensp <- depmunic$greensp/ (10000*depmunic$area)
```

In the vignette, two upper-level variables are added to the six already present, and we change the green space variable scaling to avoid numerical issues in calculating coefficient standard errors.

```{r}
summary(depmunic)
```
The properties data set has only four variables, but with price per square metre already added: 

```{r}
summary(properties)
```
The values of the variables in `depmunic` get copied to each of the properties falling within the boundaries of the municipality departments:

```{r}
properties_in_dd <- st_join(properties, depmunic, join = st_within)
```


## ex 16.2

For polygon support, we prefer contiguous neighbours:

```{r}
(mun_nb <- spdep::poly2nb(depmunic, row.names=as.character(depmunic$num_dep)))
```

Global spatial autocorrelation is marginally detected for the green space variable:

```{r}
spdep::moran.test(depmunic$prgreensp, nb2listw(mun_nb))
```
Unlike the vignette, which uses distance neighbours up to 1300 m and creates a very dense representation, we choose `k=4` k-nearest neighbours, then convert to symmetry (note that some point locations are duplicated, preventing the use of spatial indexing):

```{r}
(pr_nb_k4s <- spdep::knn2nb(spdep::knearneigh(properties, k=4), sym=TRUE, row.names=properties$id))
```
Copying out has led to the introduction of very powerful positive spatial autocorrelation in this and other variables copied out:

```{r}
spdep::moran.test(properties_in_dd$prgreensp, nb2listw(pr_nb_k4s))
```


## ex 16.3

The vignette proposes the full property level and municipal department level set of variables straight away. Here we choose the property level ones first, and update for the copied out municipal department level ones next:

```{r}
f_pr <- prpsqm ~ size + age + dist_metro
f_pr_md <- update(f_pr, . ~ . + foreigners + prgreensp + popdens + museums + airbnb)
```

Adding in the copied out upper level variables appears to account for more of the variability of the response than leaving them out:

```{r}
library(mgcv)
pr_base <- gam(f_pr, data=properties_in_dd)
pr_2lev <- gam(f_pr_md, data=properties_in_dd)
anova(pr_base, pr_2lev, test="Chisq")
```
```{r}
summary(pr_base)
```

```{r}
summary(pr_2lev)
```
Adding an upper level IID random effect to the base formula also improves the fit of the model substantially:

```{r}
pr_base_iid <- gam(update(f_pr, . ~ . + s(num_dep, bs="re")), data=properties_in_dd)
anova(pr_base, pr_base_iid, test="Chisq")
```

```{r}
summary(pr_base_iid)
```

This improvement is much more moderate when both the upper level variables and IID random effect are present:

```{r}
pr_2lev_iid <- gam(update(f_pr_md, . ~ . + s(num_dep, bs="re")), data=properties_in_dd)
anova(pr_2lev, pr_2lev_iid, test="Chisq")
```

```{r}
summary(pr_2lev_iid)
```

## ex 16.4

The `"mrf"` smooth term needs ID keys set so that the neighbour object is correctly matched to the observations. Once these are provided, the properties level model with a municipality department level MRF smooth may be fit:

```{r}
names(mun_nb) <- attr(mun_nb, "region.id")
properties_in_dd$num_dep <- factor(properties_in_dd$num_dep)
pr_base_mrf <- gam(update(f_pr, . ~ . + s(num_dep, bs="mrf", xt=list(nb=mun_nb))),
    data=properties_in_dd)
summary(pr_base_mrf)
```
Repeating for the extended model with upper level variables present, we see that no more response variability is accounted for than in the lower level variables only MRF RE model, and none of the upper level variables are significant at conventional levels.

```{r}
pr_2lev_mrf <- gam(update(f_pr_md, . ~ . + s(num_dep, bs="mrf", xt=list(nb=mun_nb))),
    data=properties_in_dd)
summary(pr_2lev_mrf)
```
It also seems that the model without upper level variables outperforms that with them included:
```{r}
anova(pr_base_mrf, pr_2lev_mrf, test="Chisq")
```
and the MRF RE outperforms the IID RE:

```{r}
anova(pr_base_mrf, pr_base_iid, test="Chisq")
```

# Chapter 17: Spatial econometrics models


## ex 17.1

First create a spatial weights object from the `k=4` symmetrized neighbour object:

```{r}
library(spatialreg)
lw <- nb2listw(pr_nb_k4s)
```

Fit a linear model to the lower-level data; all the included variables seem worth retaining:

```{r}
LM_pr <- lm(f_pr, data=properties_in_dd)
summary(LM_pr)
```
However, there is strong residual autocorrelation:

```{r}
lm.morantest(LM_pr, lw)
```
Robust Lagrange multiplier tests suggest that the fitted model should include a spatial autoregressive process in the residuals, but not in the response:

```{r}
lm.LMtests(LM_pr, lw, test=c("RLMerr", "RLMlag"))
```
Adding in the copied out municipality department level variables, we see that they do not seem to be worth retaining (unless there are good reasons for doing so); they do however improve model fit:

```{r}
LM_pr_md <- lm(f_pr_md, data=properties_in_dd)
summary(LM_pr_md)
```
The pre-test results are similar to those for the properties-only variables:

```{r}
lm.morantest(LM_pr_md, lw)
```
and the LM tests continue to indicate an omitted spatial process in the residual rather than the response:

```{r}
lm.LMtests(LM_pr_md, lw, test=c("RLMerr", "RLMlag"))
```

## ex 17.2

We may update the formula for the properties-only model to include municipality department "fixed effects", dummy variables:

```{r}
LM_pr_fx <- lm(update(f_pr, . ~ . + num_dep), data=properties_in_dd)
summary(LM_pr_fx)
```
The pre-test output is similar to that for the models considered above:
```{r}
lm.morantest(LM_pr_fx, lw)
```
```{r}
lm.LMtests(LM_pr_fx, lw, test=c("RLMerr", "RLMlag"))
```
We may fit a regimes model, where separate regression coefficients are calculated for interactions between the municipality department dummies and the included variables; `size` and `dist_metro` only retian influence for municipality departments 1 and 2:

```{r}
LM_pr_reg <- lm(update(f_pr, . ~ num_dep/(0 + .)), data=properties_in_dd)
summary(LM_pr_reg)
```
The pre-test results are now changed, with possible spatial processes in both residuals and response being indicated:
```{r}
lm.morantest(LM_pr_reg, lw)
```

```{r}
lm.LMtests(LM_pr_reg, lw, test=c("RLMerr", "RLMlag"))
```

## ex 17.3

Fitting models initially by maximum likelihood (GMM may also be used), we pre-compute the eigenvalues:

```{r, cache=TRUE}
eigs <- eigenw(lw)
```

The strong residual autocorrelation is picked up by the spatial coefficient, but unfortunately the Hausman test shows strong mis-specification:

```{r}
SEM_pr <- errorsarlm(f_pr, data=properties_in_dd, listw=lw, Durbin=FALSE,
    control=list(pre_eig=eigs))
summary(SEM_pr, Hausman=TRUE)
```

The Hausman test compares the OLS and SEM coefficient estimates and their standard errors, assessing whether their distributions overlap sufficiently to suggest the absence of major mis-specification:

```{r}
(LM_coefs <- coef(summary(LM_pr)))
```

```{r}
(SEM_coefs <- coef(summary(SEM_pr)))
```
The tables are harder to read than the figure, which shows that the coefficient estimates do differ a lot for two variables, somewhat for the intercept, and little for one variable, but where the ML standard error estimate under usual assumptions crosses zero:

```{r}
opar <- par(mfrow=c(2,2))
plot(1, type="n", xlim=c(1400, 2500), ylim=c(0, 0.006), xlab=rownames(LM_coefs)[1], ylab="")
curve(dnorm(x, mean=LM_coefs[1,1], sd=LM_coefs[1,2]), add=TRUE)
abline(v=LM_coefs[1,1])
abline(v=SEM_coefs[1,1], lwd=2, col="orange")
curve(dnorm(x, mean=SEM_coefs[1,1], sd=SEM_coefs[1,2]), add=TRUE, col="orange", lwd=2)
legend("topright", legend=c("LM", "SEM"), col=c("black", "orange"), lwd=1:2, bty="n")
plot(1, type="n", xlim=c(1.5, 7), ylim=c(0, 1.1), xlab=rownames(LM_coefs)[2], ylab="")
curve(dnorm(x, mean=LM_coefs[2,1], sd=LM_coefs[2,2]), add=TRUE)
abline(v=LM_coefs[2,1])
abline(v=SEM_coefs[2,1], lwd=2, col="orange")
curve(dnorm(x, mean=SEM_coefs[2,1], sd=SEM_coefs[2,2]), add=TRUE, col="orange", lwd=2)
plot(1, type="n", xlim=c(-28, -13), ylim=c(0, 0.4), xlab=rownames(LM_coefs)[3], ylab="")
curve(dnorm(x, mean=LM_coefs[3,1], sd=LM_coefs[3,2]), add=TRUE)
abline(v=LM_coefs[3,1])
abline(v=SEM_coefs[3,1], lwd=2, col="orange")
curve(dnorm(x, mean=SEM_coefs[3,1], sd=SEM_coefs[3,2]), add=TRUE, col="orange", lwd=2)
plot(1, type="n", xlim=c(-0.9, 0.3), ylim=c(0, 6), xlab=rownames(LM_coefs)[4], ylab="")
curve(dnorm(x, mean=LM_coefs[4,1], sd=LM_coefs[4,2]), add=TRUE)
abline(v=LM_coefs[4,1])
abline(v=SEM_coefs[4,1], lwd=2, col="orange")
curve(dnorm(x, mean=SEM_coefs[4,1], sd=SEM_coefs[4,2]), add=TRUE, col="orange", lwd=2)
par(opar)
```
The Hausman test also suggests mis-specification for the SEM model augmented with the municipality department level variables:

```{r}
SEM_pr_md <- errorsarlm(f_pr_md, data=properties_in_dd, listw=lw, Durbin=FALSE,
    control=list(pre_eig=eigs))
summary(SEM_pr_md, Hausman=TRUE)
```
Extending to the SDEM models, and reporting impacts:

```{r, cache=TRUE}
SDEM_pr <- errorsarlm(f_pr, data=properties_in_dd, listw=lw, Durbin=TRUE,
    control=list(pre_eig=eigs))
summary(impacts(SDEM_pr), short=TRUE, zstats=TRUE)
```
we have Hausman test results still indicating strong mis-specification:

```{r}
Hausman.test(SDEM_pr)
```
The same applies to the properties variables augmented with the municipality department level variables:

```{r, cache=TRUE}
SDEM_pr_md <- errorsarlm(f_pr_md, data=properties_in_dd, listw=lw, Durbin=TRUE,
    control=list(pre_eig=eigs))
summary(impacts(SDEM_pr_md), short=TRUE, zstats=TRUE)
```

```{r}
Hausman.test(SDEM_pr_md)
```
Reaching out the SLX models does not help, because although - as with the SDEM models - the indirect impacts (coefficients on lagged $X$ variables) are large, so including lagged $X$ variables especially at the properties level seems sensible, there is serious residual autocorrelation, and now the pre-test strategy points to a missing spatial process in the response:

```{r}
SLX_pr <- lmSLX(f_pr, data=properties_in_dd, listw=lw, Durbin=TRUE)
summary(impacts(SLX_pr), short=TRUE, zstats=TRUE)
```

```{r}
lm.morantest(SLX_pr, lw)
```

```{r}
lm.LMtests(SLX_pr, lw, test=c("RLMerr", "RLMlag"))
```

```{r}
SLX_pr_md <- lmSLX(f_pr_md, data=properties_in_dd, listw=lw, Durbin=TRUE)
summary(impacts(SLX_pr_md), short=TRUE, zstats=TRUE)
```

```{r}
lm.morantest(SLX_pr_md, lw)
```


```{r}
lm.LMtests(SLX_pr_md, lw, test=c("RLMerr", "RLMlag"))
```

So on balance, the pre-test strategy has not worked out too well; it is unclear what is missing in the model.

## ex 17.4

Turning to estimating the general nested model first, followed by excluding the Durbin (spatially lagged $X$) variables, a likelihood ratio test shows that the spatially lagged $X$ variables should be retained in the model:

```{r, cache=TRUE}
GNM_pr <- sacsarlm(f_pr, data=properties_in_dd, listw=lw, Durbin=TRUE,
    control=list(pre_eig1=eigs, pre_eig2=eigs))
```

```{r, cache=TRUE}
SARAR_pr <- sacsarlm(f_pr, data=properties_in_dd, listw=lw, 
    control=list(pre_eig1=eigs, pre_eig2=eigs))
```

```{r}
lmtest::lrtest(SARAR_pr, GNM_pr)
```
Again using a likelihood ratio test, the GNM model outperforms the SDEM model:

```{r}
lmtest::lrtest(SDEM_pr, GNM_pr)
```

```{r, cache=TRUE}
SDM_pr <- lagsarlm(f_pr, data=properties_in_dd, listw=lw, Durbin=TRUE,
    control=list(pre_eig=eigs))
```

as is also the case with the SDM model:

```{r}
lmtest::lrtest(SDM_pr, GNM_pr)
```
and the SLX model:

```{r}
lmtest::lrtest(SLX_pr, GNM_pr)
```

Is the inclusion of the municipality department level variables in the GNM model justified?

```{r, cache=TRUE}
GNM_pr_md <- sacsarlm(f_pr_md, data=properties_in_dd, listw=lw, Durbin=TRUE,
    control=list(pre_eig1=eigs, pre_eig2=eigs))
```

No, not really:

```{r}
lmtest::lrtest(GNM_pr, GNM_pr_md)
```
If we drop the municipality department level variables from the Durbin term, we lose fewer degrees of freedom, so preferring the model including the municipality department level variables:

```{r, cache=TRUE}
GNM_pr_md1 <- sacsarlm(f_pr_md, data=properties_in_dd, listw=lw, 
    Durbin= ~ size + age + dist_metro,
    control=list(pre_eig1=eigs, pre_eig2=eigs))
```

```{r}
lmtest::lrtest(GNM_pr, GNM_pr_md1)
```
Unfortunately, impacts are depressing here:

```{r}
trs <- trW(as(lw, "CsparseMatrix"))
i_GNM_pr_md1 <- impacts(GNM_pr_md1, tr=trs, R=2000)
summary(i_GNM_pr_md1, short=TRUE, zstats=TRUE)
```
The values and standard errors of the spatial coefficients suggest numerical problems in finding an optimum where the two coefficients are equally strong but with opposing signs:

```{r}
c("response"=GNM_pr_md1$rho, "response se"=GNM_pr_md1$rho.se, "residual"=GNM_pr_md1$lambda, "residual se"=GNM_pr_md1$lambda.se)
```


If we fall back on the properties level only GNM, total impacts are only significant in conventional terms for `size`:

```{r}
i_GNM_pr <- impacts(GNM_pr, tr=trs, R=2000)
summary(i_GNM_pr, short=TRUE, zstats=TRUE)
```
The same problem occurs without the municipality department level variables; the impacts are being driven by the large spatial coefficient on the lagged response:

```{r}
c("response"=GNM_pr$rho, "response se"=GNM_pr$rho.se, "residual"=GNM_pr$lambda, "residual se"=GNM_pr$lambda.se)
```



## ex 17.5

We cannot say that the spatial econometrics approach has reached a clear conclusion. When including the upper level variables, we introduce a lot of spatial autocorrelation at the lower level. It is arguable that the MRF random effect at the upper level and including only the properties level variables gets at least as far as the most complex spatial econometrics models. It is fairly clear that mapping the actual green space and museums, and measuring distance from each property to the attractions would remove the scale problem for those variables. Disaggregation of the foreigners, airbnb and population density variables would be highly desirable. With improvements to the properties level data set, including more variables describing the properties themselves, much of the mis-specification should be removed. 
